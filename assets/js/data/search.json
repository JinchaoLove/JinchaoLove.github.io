{
      
        "0": {
          "doc": "Archives",
          "title": "Archives",
          "content": "\n",
          "url": "/blog/archives/",
          
          "relUrl": "/blog/archives/"
        }
      ,
        "1": {
          "doc": "Categories",
          "title": "Categories",
          "content": "\n",
          "url": "/blog/categories/",
          
          "relUrl": "/blog/categories/"
        }
      ,
        "2": {
          "doc": "Tags",
          "title": "Tags",
          "content": "\n",
          "url": "/blog/tags/",
          
          "relUrl": "/blog/tags/"
        }
      ,
        "3": {
          "doc": "About",
          "title": "Short Bio",
          "content": "\n\n<p>I am a final-year Ph.D. student at the <a href=\"https://www.se.cuhk.edu.hk/laboratories/human-computer-communications-laboratory/\">Human-Computer Communications Laboratory</a> (HCCL) in <a href=\"https://cuhk.edu.hk\">The Chinese University of Hong Kong</a>, advised by Prof. <a href=\"https://www.se.cuhk.edu.hk/people/academic-staff/prof-meng-mei-ling-helen/\">Helen Meng</a>.<br />\nBefore that, I obtained my B.S. with honors from <a href=\"https://www.nju.edu.cn\">Nanjing University</a> in 2019.<br />\nMy research interests encompass <strong>multimodal AI (speech, language, vision) for social good (e.g., healthcare)</strong>, such as:</p>\n\n<ul>\n  <li>Neurocognitive Disorder Recognition</li>\n  <li>Multimodal Emotion Recognition</li>\n  <li>Multimodal Large Language Models</li>\n</ul>\n\n<hr />\n\n",
          "url": "/#short-bio",
          
          "relUrl": "/#short-bio"
        },
        "4": {
          "doc": "About",
          "title": "News",
          "content": "\n\n<blockquote class=\"prompt-info\">\n  <p><b>I am actively seeking academic and industry opportunities, including Postdoctoral, Research Fellow, and Research Internship positions. Interested collaborators are welcome to connect!</b></p>\n</blockquote>\n\n<ul>\n  <li>2025.08: One paper accepted by ACM-MM 2025</li>\n  <li>2024.12: One paper submitted to journal (<a href=\"https://arxiv.org/pdf/2501.03727\">under review</a>)</li>\n  <li>2023.01: Two papers (<a href=\"https://arxiv.org/pdf/2303.08019\">1</a>, <a href=\"https://arxiv.org/pdf/2303.08027\">2</a>) accepted by ICASSP 2023</li>\n  <li>Fall 2022: Co-teach ‚ÄúConversational AI systems‚Äù (ASR part) with Prof. Meng and other nice colleagues in CUHK</li>\n  <li>2022.09: Winner of two tasks in the ‚Äú<a href=\"https://www.competitions.hume.ai/avb2022\">ACII Affective Vocal Bursts (A-VB)</a>‚Äù competition organized by <a href=\"https://hume.ai\">Hume AI</a></li>\n</ul>\n\n<hr />\n\n",
          "url": "/#news",
          
          "relUrl": "/#news"
        },
        "5": {
          "doc": "About",
          "title": "Publications",
          "content": "\n\n<p>ü§óThanks to all the collaborators for their great work! Check out my <a href=\"https://scholar.google.com/citations?hl=en&amp;user=SB7xjMoAAAAJ\">Google Scholar</a> for more information.</p>\n\n<p><em>* indicates equal contributions.</em></p>\n\n<div class=\"publications\" style=\"font-size: 1.01rem;\">\n  <ol class=\"bibliography\"><li><!-- _layouts/bib.html -->\n<div class=\"row\"><div class=\"col-sm-2\">\n      <div class=\"preview d-flex align-items-center\">\n\n\n<a href=\"/assets/papers/25/gap.png\" class=\"popup img-link\" title=\"papers/25/gap.png\">\n\n<figure>\n  <picture>\n    <!-- Auto scaling with imagemagick -->\n    \n      <source class=\"responsive-img-srcset\" srcset=\"\n          /assets/papers/25/gap-480.webp 480w,\n          /assets/papers/25/gap-800.webp 800w,\n          /assets/papers/25/gap-1280.webp 1280w,\n          \" sizes=\"200px\" type=\"image/webp\" loading=\"lazy\" data-proofer-ignore=\"\" />\n    \n\n    <!-- Fallback to the original file -->\n    <img src=\"/assets/papers/25/gap.png\" class=\"preview rounded\" width=\"100%\" height=\"auto\" alt=\"papers/25/gap.png\" data-zoomable=\"\" loading=\"lazy\" decoding=\"async\" onerror=\"this.onerror=null; $('.responsive-img-srcset').remove();\" />\n  </picture>\n</figure>\n\n  </a>\n\n</div>\n    </div><!-- Entry bib key -->\n  <div id=\"li2025generate\" class=\"col-sm-10\">\n    <!-- Title -->\n    <div class=\"title\">Generate, Align and Predict (GAP): Detecting Neurocognitive Disorders via Cross-modal Consistency in Narratives</div>\n    <!-- Author -->\n    <div class=\"author\">\n      \n\n      \n\n      \n      Junan\n            Li*,&nbsp;<span class=\"highlight-author\">Jinchao\n            Li*</span>,&nbsp;Simon\n            Wong,&nbsp;Xixin\n            Wu, Helen\n      Meng\n    </div>\n\n    <!-- Journal/Book title and date -->\n    \n    \n    <div class=\"periodical\">\n      <em>In ACM-MM</em>, 2025\n    </div>\n    <div class=\"periodical\">\n      \n    </div>\n\n    <!-- Links/Buttons -->\n    <div class=\"links\">\n        <button class=\"abstract btn btn-sm z-depth-0\" title=\"Click to show/hide abstract\">\n          ABS\n          <i class=\"fas fa-sort open\"></i>\n          <i class=\"fas fa-times hidden\"></i>\n        </button>\n        <button class=\"bibtex btn btn-sm z-depth-0\" title=\"Click to show/hide bibtex\">\n          BIB\n          <i class=\"fas fa-sort open\"></i>\n          <i class=\"fas fa-times hidden\"></i>\n        </button>\n    </div>\n    \n      \n      \n      \n\n    <!-- Hidden abstract block -->\n      <div class=\"abstract hidden\">\n        <p>The early detection of neurocognitive disorders (NCDs), such as Alzheimer‚Äôs disease, remains a critical global health challenge due to the limitations of conventional diagnostic tools with high costs and limited accessibility. Visual-stimulated narrative (VSN)-based approach offers a promising alternative by capturing narrative patterns related to holistic cognitive domains. While prior work has focused on unimodal microstructural features (e.g., pauses, lexical diversity), macrostructural impairments‚Äîsuch as disrupted coherence and cross-modal inconsistency between narratives and visual stimuli‚Äîremain understudied despite their clinical importance. To address this, we propose GAP (Generate, Align, and Predict), a novel multimodal framework leveraging advances in Multimodal Large Language Models (MLLMs), Dynamic Programming (DP), and Vision-Language Model (VLM) to evaluate dynamic semantic consistency in VSNs. Evaluated on the Cantonese CU-MARVEL-RABBIT dataset, GAP achieved state-of-the-art performance (F1=0.65, AUC=0.75), outperforming traditional acoustic, linguistic, and pattern-matching baselines. In addition, we conduct an in-depth analysis that reveals key factors that provide insights into cognitive assessment using VSNs.</p>\n      </div><!-- Hidden bibtex block -->\n      <div class=\"bibtex hidden\"><figure class=\"highlight\"><pre><code class=\"language-bibtex\" data-lang=\"bibtex\"><span class=\"nc\">@inproceedings</span><span class=\"p\">{</span><span class=\"nl\">li2025generate</span><span class=\"p\">,</span>\n  <span class=\"na\">title</span> <span class=\"p\">=</span> <span class=\"s\">{Generate, Align and Predict (GAP): Detecting Neurocognitive Disorders via Cross-modal Consistency in Narratives}</span><span class=\"p\">,</span>\n  <span class=\"na\">author</span> <span class=\"p\">=</span> <span class=\"s\">{Li*, Junan and Li*, Jinchao and Wong, Simon and Wu, Xixin and Meng, Helen}</span><span class=\"p\">,</span>\n  <span class=\"na\">booktitle</span> <span class=\"p\">=</span> <span class=\"s\">{ACM-MM}</span><span class=\"p\">,</span>\n  <span class=\"na\">year</span> <span class=\"p\">=</span> <span class=\"s\">{2025}</span><span class=\"p\">,</span>\n  <span class=\"na\">organization</span> <span class=\"p\">=</span> <span class=\"s\">{ACM}</span>\n<span class=\"p\">}</span></code></pre></figure></div>\n\n    </div>\n</div>\n</li>\n<li><!-- _layouts/bib.html -->\n<div class=\"row\"><div class=\"col-sm-2\">\n      <div class=\"preview d-flex align-items-center\">\n\n\n<a href=\"/assets/papers/24/titan.png\" class=\"popup img-link\" title=\"papers/24/titan.png\">\n\n<figure>\n  <picture>\n    <!-- Auto scaling with imagemagick -->\n    \n      <source class=\"responsive-img-srcset\" srcset=\"\n          /assets/papers/24/titan-480.webp 480w,\n          /assets/papers/24/titan-800.webp 800w,\n          /assets/papers/24/titan-1280.webp 1280w,\n          \" sizes=\"200px\" type=\"image/webp\" loading=\"lazy\" data-proofer-ignore=\"\" />\n    \n\n    <!-- Fallback to the original file -->\n    <img src=\"/assets/papers/24/titan.png\" class=\"preview rounded\" width=\"100%\" height=\"auto\" alt=\"papers/24/titan.png\" data-zoomable=\"\" loading=\"lazy\" decoding=\"async\" onerror=\"this.onerror=null; $('.responsive-img-srcset').remove();\" />\n  </picture>\n</figure>\n\n  </a>\n\n</div>\n    </div><!-- Entry bib key -->\n  <div id=\"li2025detecting\" class=\"col-sm-10\">\n    <!-- Title -->\n    <div class=\"title\">Detecting Neurocognitive Disorders through Analyses of Topic Evolution and Cross-modal Consistency in Visual-Stimulated Narratives</div>\n    <!-- Author -->\n    <div class=\"author\">\n      \n\n      \n\n      \n      <span class=\"highlight-author\">Jinchao\n            Li*</span>,&nbsp;Yuejiao\n            Wang*,&nbsp;Junan\n            Li*,&nbsp;Jiawen\n            Kang*,&nbsp;Bo\n            Zheng,&nbsp;Simon\n            Wong,\n          <span class=\"more-authors\" title=\"click to view 10 more authors\" onclick=\"\n                var element = $(this);\n                element.attr('title', '');\n                var more_authors_text = element.text() == '10 more authors' ? 'Brian Mak, Helene Fung, Jean Woo, Man-Wai Mak, Timothy Kwok, Vincent Mok, Xianmin Gong, Xixin Wu, Xunying Liu, Patrick Wong' : '10 more authors';\n                var cursorPosition = 0;\n                var textAdder = setInterval(function(){\n                  element.text(more_authors_text.substring(0, cursorPosition + 1));\n                  if (++cursorPosition == more_authors_text.length){\n                    clearInterval(textAdder);\n                  }\n              }, '10');\n            \">10 more authors</span>, Helen\n      Meng\n    </div>\n\n    <!-- Journal/Book title and date -->\n    \n    \n    <div class=\"periodical\">\n      <em>In JSTSP (under review)</em>, 2025\n    </div>\n    <div class=\"periodical\">\n      \n    </div>\n\n    <!-- Links/Buttons -->\n    <div class=\"links\">\n        <button class=\"abstract btn btn-sm z-depth-0\" title=\"Click to show/hide abstract\">\n          ABS\n          <i class=\"fas fa-sort open\"></i>\n          <i class=\"fas fa-times hidden\"></i>\n        </button>\n        <button class=\"bibtex btn btn-sm z-depth-0\" title=\"Click to show/hide bibtex\">\n          BIB\n          <i class=\"fas fa-sort open\"></i>\n          <i class=\"fas fa-times hidden\"></i>\n        </button>\n        <a href=\"https://arxiv.org/pdf/2501.03727\" class=\"btn btn-sm z-depth-0\" role=\"button\">PDF</a>\n    </div>\n    \n      \n      \n      \n\n    <!-- Hidden abstract block -->\n      <div class=\"abstract hidden\">\n        <p>Early detection of neurocognitive disorders (NCDs) is crucial for timely intervention and disease management. Given that language impairments manifest early in NCD progression, visual-stimulated narrative (VSN)-based analysis offers a promising avenue for NCD detection. Current VSN-based NCD detection methods primarily focus on linguistic microstructures (e.g., pauses, lexical diversity), which are potentially linked to bottom-up (stimulus-driven) cognitive processing. While these features illuminate basic language abilities, the higher-order linguistic macrostructures (e.g., thematic or logical development), which may reflect top-down (concept-driven) cognitive abilities, remain underexplored. These patterns are crucial for NCD detection yet challenging to quantify due to their abstract and complex nature. To bridge this gap, we propose two novel dynamic macrostructural approaches: (1) Dynamic Topic Model (DTM) to track topic evolution over time, and (2) Text-Image Temporal Alignment Network (TITAN) to measure cross-modal consistency between narrative and visual stimuli. Experimental results validated the efficiency of proposed approaches in NCD detection, with TITAN achieving superior performance both on the CU-MARVEL-RABBIT corpus (F1 = 0.7238) and the ADReSS corpus (F1 = 0.8889). The feature contribution analysis revealed that macrostructural features (e.g., topic variability, topic change rate, and topic consistency) constituted the most significant contributors in the model‚Äôs decision pathways, outperforming investigated microstructural features. These findings underscore the critical role of macrostructural patterns in understanding cognitive impairment mechanisms in NCDs.</p>\n      </div><!-- Hidden bibtex block -->\n      <div class=\"bibtex hidden\"><figure class=\"highlight\"><pre><code class=\"language-bibtex\" data-lang=\"bibtex\"><span class=\"nc\">@inproceedings</span><span class=\"p\">{</span><span class=\"nl\">li2025detecting</span><span class=\"p\">,</span>\n  <span class=\"na\">url</span> <span class=\"p\">=</span> <span class=\"s\">{https://arxiv.org/abs/2501.03727}</span><span class=\"p\">,</span>\n  <span class=\"na\">title</span> <span class=\"p\">=</span> <span class=\"s\">{Detecting Neurocognitive Disorders through Analyses of Topic Evolution and Cross-modal Consistency in Visual-Stimulated Narratives}</span><span class=\"p\">,</span>\n  <span class=\"na\">author</span> <span class=\"p\">=</span> <span class=\"s\">{Li*, Jinchao and Wang*, Yuejiao and Li*, Junan and Kang*, Jiawen and Zheng, Bo and Wong, Simon and Mak, Brian and Fung, Helene and Woo, Jean and Mak, Man-Wai and Kwok, Timothy and Mok, Vincent and Gong, Xianmin and Wu, Xixin and Liu, Xunying and Wong, Patrick and Meng, Helen}</span><span class=\"p\">,</span>\n  <span class=\"na\">booktitle</span> <span class=\"p\">=</span> <span class=\"s\">{JSTSP (under review)}</span><span class=\"p\">,</span>\n  <span class=\"na\">year</span> <span class=\"p\">=</span> <span class=\"s\">{2025}</span><span class=\"p\">,</span>\n  <span class=\"na\">organization</span> <span class=\"p\">=</span> <span class=\"s\">{IEEE}</span>\n<span class=\"p\">}</span></code></pre></figure></div>\n\n    </div>\n</div>\n</li>\n<li><!-- _layouts/bib.html -->\n<div class=\"row\"><div class=\"col-sm-2\">\n      <div class=\"preview d-flex align-items-center\">\n\n\n<a href=\"/assets/papers/23/emotion2vec.png\" class=\"popup img-link\" title=\"papers/23/emotion2vec.png\">\n\n<figure>\n  <picture>\n    <!-- Auto scaling with imagemagick -->\n    \n      <source class=\"responsive-img-srcset\" srcset=\"\n          /assets/papers/23/emotion2vec-480.webp 480w,\n          /assets/papers/23/emotion2vec-800.webp 800w,\n          /assets/papers/23/emotion2vec-1280.webp 1280w,\n          \" sizes=\"200px\" type=\"image/webp\" loading=\"lazy\" data-proofer-ignore=\"\" />\n    \n\n    <!-- Fallback to the original file -->\n    <img src=\"/assets/papers/23/emotion2vec.png\" class=\"preview rounded\" width=\"100%\" height=\"auto\" alt=\"papers/23/emotion2vec.png\" data-zoomable=\"\" loading=\"lazy\" decoding=\"async\" onerror=\"this.onerror=null; $('.responsive-img-srcset').remove();\" />\n  </picture>\n</figure>\n\n  </a>\n\n</div>\n    </div><!-- Entry bib key -->\n  <div id=\"ma2023emotion2vec\" class=\"col-sm-10\">\n    <!-- Title -->\n    <div class=\"title\">emotion2vec: Self-Supervised Pre-Training for Speech Emotion Representation</div>\n    <!-- Author -->\n    <div class=\"author\">\n      \n\n      \n\n      \n      Ziyang\n            Ma,&nbsp;Zhisheng\n            Zheng,&nbsp;Jiaxin\n            Ye,&nbsp;<span class=\"highlight-author\">Jinchao\n            Li</span>,&nbsp;Zhifu\n            Gao,&nbsp;Shiliang\n            Zhang, Xie\n      Chen\n    </div>\n\n    <!-- Journal/Book title and date -->\n    \n    \n    <div class=\"periodical\">\n      <em>In ACL</em>, 2024\n    </div>\n    <div class=\"periodical\">\n      \n    </div>\n\n    <!-- Links/Buttons -->\n    <div class=\"links\">\n        <button class=\"abstract btn btn-sm z-depth-0\" title=\"Click to show/hide abstract\">\n          ABS\n          <i class=\"fas fa-sort open\"></i>\n          <i class=\"fas fa-times hidden\"></i>\n        </button>\n        <button class=\"bibtex btn btn-sm z-depth-0\" title=\"Click to show/hide bibtex\">\n          BIB\n          <i class=\"fas fa-sort open\"></i>\n          <i class=\"fas fa-times hidden\"></i>\n        </button>\n        <a href=\"https://arxiv.org/pdf/2312.15185\" class=\"btn btn-sm z-depth-0\" role=\"button\">PDF</a>\n        <a href=\"https://www.modelscope.cn/models/iic/emotion2vec_base_finetuned/summary\" class=\"btn btn-sm z-depth-0\" role=\"button\">DEMO</a>\n        <a href=\"https://github.com/ddlBoJack/emotion2vec\" class=\"btn btn-sm z-depth-0\" role=\"button\">CODE</a>\n    </div>\n    \n      \n      \n      \n\n    <!-- Hidden abstract block -->\n      <div class=\"abstract hidden\">\n        <p>We propose emotion2vec, a universal speech emotion representation model. \n  emotion2vec is pre-trained on open-source unlabeled emotion data through self-supervised online distillation, combining utterance-level loss and frame-level loss during pre-training. \n  emotion2vec outperforms state-of-the-art pre-trained universal models and emotion specialist models by only training linear layers for the speech emotion recognition task on the mainstream IEMOCAP dataset. \n  In addition, emotion2vec shows consistent improvements among 10 different languages of speech emotion recognition datasets. \n  emotion2vec also shows excellent results on other emotion tasks, such as song emotion recognition, emotion prediction in conversation, and sentiment analysis. \n  Comparison experiments, ablation experiments, and visualization comprehensively demonstrate the universal capability of the proposed emotion2vec. \n  To the best of our knowledge, emotion2vec is the first universal representation model in various emotion-related tasks, filling a gap in the field.</p>\n      </div><!-- Hidden bibtex block -->\n      <div class=\"bibtex hidden\"><figure class=\"highlight\"><pre><code class=\"language-bibtex\" data-lang=\"bibtex\"><span class=\"nc\">@inproceedings</span><span class=\"p\">{</span><span class=\"nl\">ma2023emotion2vec</span><span class=\"p\">,</span>\n  <span class=\"na\">url</span> <span class=\"p\">=</span> <span class=\"s\">{https://arxiv.org/abs/2312.15185}</span><span class=\"p\">,</span>\n  <span class=\"na\">title</span> <span class=\"p\">=</span> <span class=\"s\">{emotion2vec: Self-Supervised Pre-Training for Speech Emotion Representation}</span><span class=\"p\">,</span>\n  <span class=\"na\">author</span> <span class=\"p\">=</span> <span class=\"s\">{Ma, Ziyang and Zheng, Zhisheng and Ye, Jiaxin and Li, Jinchao and Gao, Zhifu and Zhang, Shiliang and Chen, Xie}</span><span class=\"p\">,</span>\n  <span class=\"na\">booktitle</span> <span class=\"p\">=</span> <span class=\"s\">{ACL}</span><span class=\"p\">,</span>\n  <span class=\"na\">year</span> <span class=\"p\">=</span> <span class=\"s\">{2024}</span><span class=\"p\">,</span>\n  <span class=\"na\">organization</span> <span class=\"p\">=</span> <span class=\"s\">{ACL}</span>\n<span class=\"p\">}</span></code></pre></figure></div>\n\n    </div>\n</div>\n</li>\n<li><!-- _layouts/bib.html -->\n<div class=\"row\"><div class=\"col-sm-2\">\n      <div class=\"preview d-flex align-items-center\">\n\n\n<a href=\"/assets/papers/23/MultiAD_ICASSP23.png\" class=\"popup img-link\" title=\"papers/23/MultiAD_ICASSP23.png\">\n\n<figure>\n  <picture>\n    <!-- Auto scaling with imagemagick -->\n    \n      <source class=\"responsive-img-srcset\" srcset=\"\n          /assets/papers/23/MultiAD_ICASSP23-480.webp 480w,\n          /assets/papers/23/MultiAD_ICASSP23-800.webp 800w,\n          /assets/papers/23/MultiAD_ICASSP23-1280.webp 1280w,\n          \" sizes=\"200px\" type=\"image/webp\" loading=\"lazy\" data-proofer-ignore=\"\" />\n    \n\n    <!-- Fallback to the original file -->\n    <img src=\"/assets/papers/23/MultiAD_ICASSP23.png\" class=\"preview rounded\" width=\"100%\" height=\"auto\" alt=\"papers/23/MultiAD_ICASSP23.png\" data-zoomable=\"\" loading=\"lazy\" decoding=\"async\" onerror=\"this.onerror=null; $('.responsive-img-srcset').remove();\" />\n  </picture>\n</figure>\n\n  </a>\n\n</div>\n    </div><!-- Entry bib key -->\n  <div id=\"li2023leveraging\" class=\"col-sm-10\">\n    <!-- Title -->\n    <div class=\"title\">Leveraging Pretrained Representations With Task-Related Keywords for Alzheimer‚Äôs Disease Detection</div>\n    <!-- Author -->\n    <div class=\"author\">\n      \n\n      \n\n      \n      <span class=\"highlight-author\">Jinchao\n            Li</span>,&nbsp;Kaitao\n            Song,&nbsp;Junan\n            Li,&nbsp;Bo\n            Zheng,&nbsp;Dongsheng\n            Li,&nbsp;Xixin\n            Wu, Xunying\n          Liu, Helen\n      Meng\n    </div>\n\n    <!-- Journal/Book title and date -->\n    \n    \n    <div class=\"periodical\">\n      <em>In ICASSP</em>, 2023\n    </div>\n    <div class=\"periodical\">\n      \n    </div>\n\n    <!-- Links/Buttons -->\n    <div class=\"links\">\n        <button class=\"abstract btn btn-sm z-depth-0\" title=\"Click to show/hide abstract\">\n          ABS\n          <i class=\"fas fa-sort open\"></i>\n          <i class=\"fas fa-times hidden\"></i>\n        </button>\n        <button class=\"bibtex btn btn-sm z-depth-0\" title=\"Click to show/hide bibtex\">\n          BIB\n          <i class=\"fas fa-sort open\"></i>\n          <i class=\"fas fa-times hidden\"></i>\n        </button>\n        <a href=\"https://arxiv.org/pdf/2303.08019\" class=\"btn btn-sm z-depth-0\" role=\"button\">PDF</a>\n        <a href=\"/assets/papers/23/AD_poster.pdf\" class=\"btn btn-sm z-depth-0\" role=\"button\" target=\"_blank\" rel=\"noopener noreferrer\">POSTER</a>\n    </div>\n    \n      \n      \n      \n\n    <!-- Hidden abstract block -->\n      <div class=\"abstract hidden\">\n        <p>With the global population aging rapidly, Alzheimer‚Äôs disease (AD) is particularly prominent in older adults, which has an insidious onset and leads to a gradual, irreversible deterioration in cognitive domains (memory, communication, etc.). Speech-based AD detection opens up the possibility of widespread screening and timely disease intervention. Recent advances in pre-trained models motivate AD detection modeling to shift from low-level features to high-level representations. This paper presents several efficient methods to extract better AD-related cues from high-level acoustic and linguistic features. Based on these features, the paper also proposes a novel task-oriented approach by modeling the relationship between the participants‚Äô description and the cognitive task. Experiments are carried out on the ADReSS dataset in a binary classification setup, and models are evaluated on the unseen test set. Results and comparison with recent literature demonstrate the efficiency and superior performance of proposed acoustic, linguistic and task-oriented methods. The findings also show the importance of semantic and syntactic information, and feasibility of automation and generalization with the promising audio-only and task-oriented methods for the AD detection task.</p>\n      </div><!-- Hidden bibtex block -->\n      <div class=\"bibtex hidden\"><figure class=\"highlight\"><pre><code class=\"language-bibtex\" data-lang=\"bibtex\"><span class=\"nc\">@inproceedings</span><span class=\"p\">{</span><span class=\"nl\">li2023leveraging</span><span class=\"p\">,</span>\n  <span class=\"na\">url</span> <span class=\"p\">=</span> <span class=\"s\">{https://ieeexplore.ieee.org/document/10096205}</span><span class=\"p\">,</span>\n  <span class=\"na\">title</span> <span class=\"p\">=</span> <span class=\"s\">{Leveraging Pretrained Representations With Task-Related Keywords for Alzheimer‚Äôs Disease Detection}</span><span class=\"p\">,</span>\n  <span class=\"na\">author</span> <span class=\"p\">=</span> <span class=\"s\">{Li, Jinchao and Song, Kaitao and Li, Junan and Zheng, Bo and Li, Dongsheng and Wu, Xixin and Liu, Xunying and Meng, Helen}</span><span class=\"p\">,</span>\n  <span class=\"na\">booktitle</span> <span class=\"p\">=</span> <span class=\"s\">{ICASSP}</span><span class=\"p\">,</span>\n  <span class=\"na\">year</span> <span class=\"p\">=</span> <span class=\"s\">{2023}</span><span class=\"p\">,</span>\n  <span class=\"na\">organization</span> <span class=\"p\">=</span> <span class=\"s\">{IEEE}</span>\n<span class=\"p\">}</span></code></pre></figure></div>\n\n    </div>\n</div>\n</li>\n<li><!-- _layouts/bib.html -->\n<div class=\"row\"><div class=\"col-sm-2\">\n      <div class=\"preview d-flex align-items-center\">\n\n\n<a href=\"/assets/papers/23/AVB_ICASSP23.png\" class=\"popup img-link\" title=\"papers/23/AVB_ICASSP23.png\">\n\n<figure>\n  <picture>\n    <!-- Auto scaling with imagemagick -->\n    \n      <source class=\"responsive-img-srcset\" srcset=\"\n          /assets/papers/23/AVB_ICASSP23-480.webp 480w,\n          /assets/papers/23/AVB_ICASSP23-800.webp 800w,\n          /assets/papers/23/AVB_ICASSP23-1280.webp 1280w,\n          \" sizes=\"200px\" type=\"image/webp\" loading=\"lazy\" data-proofer-ignore=\"\" />\n    \n\n    <!-- Fallback to the original file -->\n    <img src=\"/assets/papers/23/AVB_ICASSP23.png\" class=\"preview rounded\" width=\"100%\" height=\"auto\" alt=\"papers/23/AVB_ICASSP23.png\" data-zoomable=\"\" loading=\"lazy\" decoding=\"async\" onerror=\"this.onerror=null; $('.responsive-img-srcset').remove();\" />\n  </picture>\n</figure>\n\n  </a>\n\n</div>\n    </div><!-- Entry bib key -->\n  <div id=\"li2023hierarchical\" class=\"col-sm-10\">\n    <!-- Title -->\n    <div class=\"title\">A Hierarchical Regression Chain Framework for Affective Vocal Burst Recognition</div>\n    <!-- Author -->\n    <div class=\"author\">\n      \n\n      \n\n      \n      <span class=\"highlight-author\">Jinchao\n            Li</span>,&nbsp;Xixin\n            Wu,&nbsp;Kaitao\n            Song,&nbsp;Dongsheng\n            Li,&nbsp;Xunying\n            Liu, Helen\n      Meng\n    </div>\n\n    <!-- Journal/Book title and date -->\n    \n    \n    <div class=\"periodical\">\n      <em>In ICASSP</em>, 2023\n    </div>\n    <div class=\"periodical\">\n      \n    </div>\n\n    <!-- Links/Buttons -->\n    <div class=\"links\">\n        <button class=\"abstract btn btn-sm z-depth-0\" title=\"Click to show/hide abstract\">\n          ABS\n          <i class=\"fas fa-sort open\"></i>\n          <i class=\"fas fa-times hidden\"></i>\n        </button>\n        <button class=\"bibtex btn btn-sm z-depth-0\" title=\"Click to show/hide bibtex\">\n          BIB\n          <i class=\"fas fa-sort open\"></i>\n          <i class=\"fas fa-times hidden\"></i>\n        </button>\n        <a href=\"https://arxiv.org/pdf/2303.08027\" class=\"btn btn-sm z-depth-0\" role=\"button\">PDF</a>\n        <a href=\"/assets/papers/23/AVB_poster.pdf\" class=\"btn btn-sm z-depth-0\" role=\"button\" target=\"_blank\" rel=\"noopener noreferrer\">POSTER</a>\n        <a href=\"https://github.com/JinchaoLove/AffectiveVocalBurstRecognition\" class=\"btn btn-sm z-depth-0\" role=\"button\">CODE</a>\n    </div>\n    \n      \n      \n      \n\n    <!-- Hidden abstract block -->\n      <div class=\"abstract hidden\">\n        <p>As a common way of emotion signaling via non-linguistic vocalizations, vocal burst (VB) plays an important role in daily social interaction. Understanding and modeling human vocal bursts are indispensable for developing robust and general artificial intelligence. Exploring computational approaches for understanding vocal bursts is attracting increasing research attention. In this work, we propose a hierarchical framework, based on chain regression models, for affective recognition from VBs, that explicitly considers multiple relationships: (i) between emotional states and diverse cultures; (ii) between low-dimensional (arousal &amp; valence) and high-dimensional (10 emotion classes) emotion spaces; and (iii) between various emotion classes within the high-dimensional space. To address the challenge of data sparsity, we also use self-supervised learning (SSL) representations with layer-wise and temporal aggregation modules. The proposed systems participated in the ACII Affective Vocal Burst (A-VB) Challenge 2022 and ranked first in the \"TWO‚Äù and \"CULTURE‚Äù tasks. Experimental results based on the ACII Challenge 2022 dataset demonstrate the superior performance of the proposed system and the effectiveness of considering multiple relationships using hierarchical regression chain models.</p>\n      </div><!-- Hidden bibtex block -->\n      <div class=\"bibtex hidden\"><figure class=\"highlight\"><pre><code class=\"language-bibtex\" data-lang=\"bibtex\"><span class=\"nc\">@inproceedings</span><span class=\"p\">{</span><span class=\"nl\">li2023hierarchical</span><span class=\"p\">,</span>\n  <span class=\"na\">url</span> <span class=\"p\">=</span> <span class=\"s\">{https://ieeexplore.ieee.org/document/10096395/}</span><span class=\"p\">,</span>\n  <span class=\"na\">title</span> <span class=\"p\">=</span> <span class=\"s\">{A Hierarchical Regression Chain Framework for Affective Vocal Burst Recognition}</span><span class=\"p\">,</span>\n  <span class=\"na\">author</span> <span class=\"p\">=</span> <span class=\"s\">{Li, Jinchao and Wu, Xixin and Song, Kaitao and Li, Dongsheng and Liu, Xunying and Meng, Helen}</span><span class=\"p\">,</span>\n  <span class=\"na\">booktitle</span> <span class=\"p\">=</span> <span class=\"s\">{ICASSP}</span><span class=\"p\">,</span>\n  <span class=\"na\">year</span> <span class=\"p\">=</span> <span class=\"s\">{2023}</span><span class=\"p\">,</span>\n  <span class=\"na\">organization</span> <span class=\"p\">=</span> <span class=\"s\">{IEEE}</span>\n<span class=\"p\">}</span></code></pre></figure></div>\n\n    </div>\n</div>\n</li>\n<li><!-- _layouts/bib.html -->\n<div class=\"row\"><div class=\"col-sm-2\">\n      <div class=\"preview d-flex align-items-center\">\n\n\n<a href=\"/assets/papers/22/MER.png\" class=\"popup img-link\" title=\"papers/22/MER.png\">\n\n<figure>\n  <picture>\n    <!-- Auto scaling with imagemagick -->\n    \n      <source class=\"responsive-img-srcset\" srcset=\"\n          /assets/papers/22/MER-480.webp 480w,\n          /assets/papers/22/MER-800.webp 800w,\n          /assets/papers/22/MER-1280.webp 1280w,\n          \" sizes=\"200px\" type=\"image/webp\" loading=\"lazy\" data-proofer-ignore=\"\" />\n    \n\n    <!-- Fallback to the original file -->\n    <img src=\"/assets/papers/22/MER.png\" class=\"preview rounded\" width=\"100%\" height=\"auto\" alt=\"papers/22/MER.png\" data-zoomable=\"\" loading=\"lazy\" decoding=\"async\" onerror=\"this.onerror=null; $('.responsive-img-srcset').remove();\" />\n  </picture>\n</figure>\n\n  </a>\n\n</div>\n    </div><!-- Entry bib key -->\n  <div id=\"li2022context\" class=\"col-sm-10\">\n    <!-- Title -->\n    <div class=\"title\">Context-Aware Multimodal Fusion for Emotion Recognition</div>\n    <!-- Author -->\n    <div class=\"author\">\n      \n\n      \n\n      \n      <span class=\"highlight-author\">Jinchao\n            Li</span>,&nbsp;Shuai\n            Wang,&nbsp;Yang\n            Chao,&nbsp;Xunying\n            Liu, Helen\n      Meng\n    </div>\n\n    <!-- Journal/Book title and date -->\n    \n    \n    <div class=\"periodical\">\n      <em>In INTERSPEECH</em>, 2022\n    </div>\n    <div class=\"periodical\">\n      \n    </div>\n\n    <!-- Links/Buttons -->\n    <div class=\"links\">\n        <button class=\"abstract btn btn-sm z-depth-0\" title=\"Click to show/hide abstract\">\n          ABS\n          <i class=\"fas fa-sort open\"></i>\n          <i class=\"fas fa-times hidden\"></i>\n        </button>\n        <button class=\"bibtex btn btn-sm z-depth-0\" title=\"Click to show/hide bibtex\">\n          BIB\n          <i class=\"fas fa-sort open\"></i>\n          <i class=\"fas fa-times hidden\"></i>\n        </button>\n        <a href=\"/assets/papers/22/MER_paper_IS22.pdf\" class=\"btn btn-sm z-depth-0\" role=\"button\" target=\"_blank\" rel=\"noopener noreferrer\">PDF</a>\n        <a href=\"/assets/papers/22/MER_poster_IS22.png\" class=\"btn btn-sm z-depth-0\" role=\"button\" target=\"_blank\" rel=\"noopener noreferrer\">POSTER</a>\n    </div>\n    \n      \n      \n      \n\n    <!-- Hidden abstract block -->\n      <div class=\"abstract hidden\">\n        <p>Automatic emotion recognition (AER) is an inherently complex multimodal task that aims to automatically determine the emotional state of a given expression. Recent works have witnessed the benefits of upstream pretrained models in both audio and textual modalities for the AER task. However, efforts are still needed to effectively integrate features across multiple modalities, devoting due considerations to granularity mismatch and asynchrony in time steps. In this work, we first validate the effectiveness of the upstream models in a unimodal setup and empirically find that partial fine-tuning of the pretrained model in the feature space can significantly boost performance. Moreover, we take the context of the current sentence to model a more accurate emotional state. Based on the unimodal setups, we further propose several multimodal fusion methods to combine high-level features from the audio and text modalities. Experiments are carried out on the IEMOCAP dataset in a 4-category classification problem and compared with state-of-the-art methods in recent literature. Results show that the proposed models gave a superior performance of up to 84.45% and 80.36% weighted accuracy scores respectively in Session 5 and 5-fold cross-validation settings.</p>\n      </div><!-- Hidden bibtex block -->\n      <div class=\"bibtex hidden\"><figure class=\"highlight\"><pre><code class=\"language-bibtex\" data-lang=\"bibtex\"><span class=\"nc\">@inproceedings</span><span class=\"p\">{</span><span class=\"nl\">li2022context</span><span class=\"p\">,</span>\n  <span class=\"na\">url</span> <span class=\"p\">=</span> <span class=\"s\">{https://www.isca-speech.org/archive/interspeech_2022/li22v_interspeech.html}</span><span class=\"p\">,</span>\n  <span class=\"na\">title</span> <span class=\"p\">=</span> <span class=\"s\">{Context-Aware Multimodal Fusion for Emotion Recognition}</span><span class=\"p\">,</span>\n  <span class=\"na\">author</span> <span class=\"p\">=</span> <span class=\"s\">{Li, Jinchao and Wang, Shuai and Chao, Yang and Liu, Xunying and Meng, Helen}</span><span class=\"p\">,</span>\n  <span class=\"na\">booktitle</span> <span class=\"p\">=</span> <span class=\"s\">{INTERSPEECH}</span><span class=\"p\">,</span>\n  <span class=\"na\">year</span> <span class=\"p\">=</span> <span class=\"s\">{2022}</span><span class=\"p\">,</span>\n  <span class=\"na\">organization</span> <span class=\"p\">=</span> <span class=\"s\">{IEEE}</span>\n<span class=\"p\">}</span></code></pre></figure></div>\n\n    </div>\n</div>\n</li>\n<li><!-- _layouts/bib.html -->\n<div class=\"row\"><div class=\"col-sm-2\">\n      <div class=\"preview d-flex align-items-center\">\n\n\n<a href=\"/assets/papers/21/Comp_AD.png\" class=\"popup img-link\" title=\"papers/21/Comp_AD.png\">\n\n<figure>\n  <picture>\n    <!-- Auto scaling with imagemagick -->\n    \n      <source class=\"responsive-img-srcset\" srcset=\"\n          /assets/papers/21/Comp_AD-480.webp 480w,\n          /assets/papers/21/Comp_AD-800.webp 800w,\n          /assets/papers/21/Comp_AD-1280.webp 1280w,\n          \" sizes=\"200px\" type=\"image/webp\" loading=\"lazy\" data-proofer-ignore=\"\" />\n    \n\n    <!-- Fallback to the original file -->\n    <img src=\"/assets/papers/21/Comp_AD.png\" class=\"preview rounded\" width=\"100%\" height=\"auto\" alt=\"papers/21/Comp_AD.png\" data-zoomable=\"\" loading=\"lazy\" decoding=\"async\" onerror=\"this.onerror=null; $('.responsive-img-srcset').remove();\" />\n  </picture>\n</figure>\n\n  </a>\n\n</div>\n    </div><!-- Entry bib key -->\n  <div id=\"li2021comparative\" class=\"col-sm-10\">\n    <!-- Title -->\n    <div class=\"title\">A Comparative Study of Acoustic and Linguistic Features Classification for Alzheimer‚Äôs Disease Detection</div>\n    <!-- Author -->\n    <div class=\"author\">\n      \n\n      \n\n      \n      <span class=\"highlight-author\">Jinchao\n            Li</span>,&nbsp;Jianwei\n            Yu,&nbsp;Zi\n            Ye,&nbsp;Simon\n            Wong,&nbsp;Manwai\n            Mak,&nbsp;Brian\n            Mak, Xunying\n          Liu, Helen\n      Meng\n    </div>\n\n    <!-- Journal/Book title and date -->\n    \n    \n    <div class=\"periodical\">\n      <em>In ICASSP</em>, 2021\n    </div>\n    <div class=\"periodical\">\n      \n    </div>\n\n    <!-- Links/Buttons -->\n    <div class=\"links\">\n        <button class=\"abstract btn btn-sm z-depth-0\" title=\"Click to show/hide abstract\">\n          ABS\n          <i class=\"fas fa-sort open\"></i>\n          <i class=\"fas fa-times hidden\"></i>\n        </button>\n        <button class=\"bibtex btn btn-sm z-depth-0\" title=\"Click to show/hide bibtex\">\n          BIB\n          <i class=\"fas fa-sort open\"></i>\n          <i class=\"fas fa-times hidden\"></i>\n        </button>\n        <a href=\"https://www1.se.cuhk.edu.hk/~hccl/publications/pub/ICASSP_jcli.pdf\" class=\"btn btn-sm z-depth-0\" role=\"button\">PDF</a>\n        <a href=\"/assets/papers/21/Comp_AD.pdf\" class=\"btn btn-sm z-depth-0\" role=\"button\" target=\"_blank\" rel=\"noopener noreferrer\">POSTER</a>\n        <a href=\"https://github.com/JinchaoLove/NCDdetection_ICASSP2021\" class=\"btn btn-sm z-depth-0\" role=\"button\">CODE</a>\n    </div>\n    \n      \n      \n      \n\n    <!-- Hidden abstract block -->\n      <div class=\"abstract hidden\">\n        <p>With the global population ageing rapidly, Alzheimer‚Äôs Disease (AD) is particularly prominent in older adults, which has an insidious onset followed by gradual, irreversible deterioration in cognitive domains (memory, communication, etc). Thus the detection of Alzheimer‚Äôs Disease is crucial for timely intervention to slow down disease progression. This paper presents a comparative study of different acoustic and linguistic features for the AD detection using various classifiers. Experimental results on ADReSS dataset reflect that the proposed models using ComParE, X-vector, Linguistics, TFIDF and BERT features are able to detect AD with high accuracy and sensitivity, and are comparable with the state-of-the-art results reported. While most previous work used manual transcripts, our results also indicate that similar or even better performance could be obtained using automatically recognized transcripts over manually collected ones. This work achieves accuracy scores at 0.67 for acoustic features and 0.88 for linguistic features on either manual or ASR transcripts on the ADReSS Challenge test set.</p>\n      </div><!-- Hidden bibtex block -->\n      <div class=\"bibtex hidden\"><figure class=\"highlight\"><pre><code class=\"language-bibtex\" data-lang=\"bibtex\"><span class=\"nc\">@inproceedings</span><span class=\"p\">{</span><span class=\"nl\">li2021comparative</span><span class=\"p\">,</span>\n  <span class=\"na\">url</span> <span class=\"p\">=</span> <span class=\"s\">{https://ieeexplore.ieee.org/document/9414147}</span><span class=\"p\">,</span>\n  <span class=\"na\">title</span> <span class=\"p\">=</span> <span class=\"s\">{A Comparative Study of Acoustic and Linguistic Features Classification for Alzheimer's Disease Detection}</span><span class=\"p\">,</span>\n  <span class=\"na\">author</span> <span class=\"p\">=</span> <span class=\"s\">{Li, Jinchao and Yu, Jianwei and Ye, Zi and Wong, Simon and Mak, Manwai and Mak, Brian and Liu, Xunying and Meng, Helen}</span><span class=\"p\">,</span>\n  <span class=\"na\">booktitle</span> <span class=\"p\">=</span> <span class=\"s\">{ICASSP}</span><span class=\"p\">,</span>\n  <span class=\"na\">year</span> <span class=\"p\">=</span> <span class=\"s\">{2021}</span><span class=\"p\">,</span>\n  <span class=\"na\">organization</span> <span class=\"p\">=</span> <span class=\"s\">{IEEE}</span>\n<span class=\"p\">}</span></code></pre></figure></div>\n\n    </div>\n</div>\n</li>\n<li><!-- _layouts/bib.html -->\n<div class=\"row\"><div class=\"col-sm-2\">\n      <div class=\"preview d-flex align-items-center\">\n\n\n<a href=\"/assets/papers/21/ASR_AD.png\" class=\"popup img-link\" title=\"papers/21/ASR_AD.png\">\n\n<figure>\n  <picture>\n    <!-- Auto scaling with imagemagick -->\n    \n      <source class=\"responsive-img-srcset\" srcset=\"\n          /assets/papers/21/ASR_AD-480.webp 480w,\n          /assets/papers/21/ASR_AD-800.webp 800w,\n          /assets/papers/21/ASR_AD-1280.webp 1280w,\n          \" sizes=\"200px\" type=\"image/webp\" loading=\"lazy\" data-proofer-ignore=\"\" />\n    \n\n    <!-- Fallback to the original file -->\n    <img src=\"/assets/papers/21/ASR_AD.png\" class=\"preview rounded\" width=\"100%\" height=\"auto\" alt=\"papers/21/ASR_AD.png\" data-zoomable=\"\" loading=\"lazy\" decoding=\"async\" onerror=\"this.onerror=null; $('.responsive-img-srcset').remove();\" />\n  </picture>\n</figure>\n\n  </a>\n\n</div>\n    </div><!-- Entry bib key -->\n  <div id=\"ye2021development\" class=\"col-sm-10\">\n    <!-- Title -->\n    <div class=\"title\">Development of the CUHK Elderly Speech Recognition System for Neurocognitive Disorder Detection Using the DementiaBank Corpus</div>\n    <!-- Author -->\n    <div class=\"author\">\n      \n\n      \n\n      \n      Zi\n            Ye,&nbsp;Shoukang\n            Hu,&nbsp;<span class=\"highlight-author\">Jinchao\n            Li</span>,&nbsp;Xurong\n            Xie,&nbsp;Mengzhe\n            Geng,&nbsp;Jianwei\n            Yu,\n          <span class=\"more-authors\" title=\"click to view 4 more authors\" onclick=\"\n                var element = $(this);\n                element.attr('title', '');\n                var more_authors_text = element.text() == '4 more authors' ? 'Junhao Xu, Boyang Xue, Shansong Liu, Xunying Liu' : '4 more authors';\n                var cursorPosition = 0;\n                var textAdder = setInterval(function(){\n                  element.text(more_authors_text.substring(0, cursorPosition + 1));\n                  if (++cursorPosition == more_authors_text.length){\n                    clearInterval(textAdder);\n                  }\n              }, '10');\n            \">4 more authors</span>, Helen\n      Meng\n    </div>\n\n    <!-- Journal/Book title and date -->\n    \n    \n    <div class=\"periodical\">\n      <em>In ICASSP</em>, 2021\n    </div>\n    <div class=\"periodical\">\n      \n    </div>\n\n    <!-- Links/Buttons -->\n    <div class=\"links\">\n        <button class=\"abstract btn btn-sm z-depth-0\" title=\"Click to show/hide abstract\">\n          ABS\n          <i class=\"fas fa-sort open\"></i>\n          <i class=\"fas fa-times hidden\"></i>\n        </button>\n        <button class=\"bibtex btn btn-sm z-depth-0\" title=\"Click to show/hide bibtex\">\n          BIB\n          <i class=\"fas fa-sort open\"></i>\n          <i class=\"fas fa-times hidden\"></i>\n        </button>\n        <a href=\"https://www1.se.cuhk.edu.hk/~hccl/publications/pub/zye_revised_v4.pdf\" class=\"btn btn-sm z-depth-0\" role=\"button\">PDF</a>\n        <a href=\"/assets/papers/21/ASR_AD.pdf\" class=\"btn btn-sm z-depth-0\" role=\"button\" target=\"_blank\" rel=\"noopener noreferrer\">POSTER</a>\n    </div>\n    \n      \n      \n      \n\n    <!-- Hidden abstract block -->\n      <div class=\"abstract hidden\">\n        <p>Early diagnosis of Neurocognitive Disorder (NCD) is crucial in facilitating preventive care and timely treatment to delay further progression. This paper presents the development of a state-of-the-art automatic speech recognition (ASR) system built on the Dementia-Bank Pitt corpus for automatic NCD detection. Speed perturbation based audio data augmentation expanded the limited elderly speech data by four times. Large quantities of out-of-domain, non-aged adult speech were exploited by cross-domain adapting a 1000-hour LibriSpeech corpus trained LF-MMI factored TDNN system to DementiaBank. The variability among elderly speakers was modeled using i-Vector and learning hidden unit contributions (LHUC) based speaker adaptive training. Robust Bayesian estimation of TDNN systems and LHUC transforms were used in both cross-domain and speaker adaptation. A Transformer language model was also built to improve the final system performance. A word error rate (WER) reduction of 11.72% absolute (26.11% relative) was obtained over the baseline i-Vector adapted LF-MMI TDNN system on the evaluation data of 48 elderly speakers. The best NCD detection accuracy of 88%, comparable to that using the ground truth speech transcripts, was obtained using the textual features extracted from the final ASR system outputs.</p>\n      </div><!-- Hidden bibtex block -->\n      <div class=\"bibtex hidden\"><figure class=\"highlight\"><pre><code class=\"language-bibtex\" data-lang=\"bibtex\"><span class=\"nc\">@inproceedings</span><span class=\"p\">{</span><span class=\"nl\">ye2021development</span><span class=\"p\">,</span>\n  <span class=\"na\">url</span> <span class=\"p\">=</span> <span class=\"s\">{https://ieeexplore.ieee.org/document/9413634}</span><span class=\"p\">,</span>\n  <span class=\"na\">title</span> <span class=\"p\">=</span> <span class=\"s\">{Development of the CUHK Elderly Speech Recognition System for Neurocognitive Disorder Detection Using the DementiaBank Corpus}</span><span class=\"p\">,</span>\n  <span class=\"na\">author</span> <span class=\"p\">=</span> <span class=\"s\">{Ye, Zi and Hu, Shoukang and Li, Jinchao and Xie, Xurong and Geng, Mengzhe and Yu, Jianwei and Xu, Junhao and Xue, Boyang and Liu, Shansong and Liu, Xunying and Meng, Helen}</span><span class=\"p\">,</span>\n  <span class=\"na\">booktitle</span> <span class=\"p\">=</span> <span class=\"s\">{ICASSP}</span><span class=\"p\">,</span>\n  <span class=\"na\">year</span> <span class=\"p\">=</span> <span class=\"s\">{2021}</span><span class=\"p\">,</span>\n  <span class=\"na\">organization</span> <span class=\"p\">=</span> <span class=\"s\">{IEEE}</span>\n<span class=\"p\">}</span></code></pre></figure></div>\n\n    </div>\n</div>\n</li></ol>\n</div>\n\n<hr />\n\n",
          "url": "/#publications",
          
          "relUrl": "/#publications"
        },
        "6": {
          "doc": "About",
          "title": "Experiences",
          "content": "\n\n<p><strong><em>PhD Project: Neurocognitive Disorder (NCD) Detection</em></strong></p>\n\n<p>‚ÄÇ Advised by Prof. <a href=\"https://www.se.cuhk.edu.hk/people/academic-staff/prof-meng-mei-ling-helen/\">Helen Meng</a> @<a href=\"https://www.se.cuhk.edu.hk/laboratories/human-computer-communications-laboratory/\">HCCL</a>, <a href=\"https://cuhk.edu.hk\">CUHK</a>, Jul. 2020 - Now</p>\n\n<ul>\n  <li>Speech and language based NCD detection: feature engineering, multimodal and multilevel fusion</li>\n  <li>Comparatively analyzed NCD-related acoustic and linguistic features</li>\n  <li>Combined narratives with visual stimuli to model macro-level topic evolution and cross-modal consistency</li>\n</ul>\n\n<p><strong><em>Speech-Empowered Large Language Model (LLM)</em></strong></p>\n\n<p>‚ÄÇ Advised by Dr. <a href=\"https://scholar.google.com/citations?user=uIUfGxYAAAAJ&amp;hl=zh-CN\">Ming Yan</a> and <a href=\"https://scholar.google.com/citations?user=bS8Ku4MAAAAJ&amp;hl=zh-CN\">Guohai Xu</a> @<a href=\"https://damo.alibaba.com\">Alibaba</a> DAMO Academy, Aug. 2023 - Nov. 2023</p>\n\n<ul>\n  <li>Processed large-scale audio and textual corpora</li>\n  <li>Empowered LLM with modularized speech ability for adaptive dialogue policy</li>\n</ul>\n\n<p><strong><em>Multimodal NCD Detection and Affective Computing</em></strong></p>\n\n<p>‚ÄÇ Advised by Dr. <a href=\"https://recmind.cn/\">Dongsheng Li</a> and <a href=\"https://www.microsoft.com/en-us/research/people/kaitaosong\">Kaitao Song</a> @<a href=\"https://www.msra.cn\">MSRA</a> Shanghai AI lab, Jun. 2022 - Oct. 2022</p>\n\n<ul>\n  <li>Task-related text-visual NCD detection</li>\n  <li>Hierarchical multi-output regression for affective vocal burst recognition</li>\n</ul>\n\n<p><strong><em>Emotion Recognition (ER)</em></strong>, <strong><em>Speech Enhancement (SE)</em></strong></p>\n\n<p>‚ÄÇ Advised by Dr. <a href=\"https://wsstriving.github.io\">Shuai Wang</a> @<a href=\"https://www.lightspeed-studios.com\">Tencent</a> Lightspeed &amp; Quantum Studios, Oct. 2021 - May 2022</p>\n\n<ul>\n  <li>Multimodal ER: context-aware multimodal fusion for the ER task</li>\n  <li>Real-time monaural SE: FullSubNet-based denoiser for ASR with fbank information</li>\n</ul>\n\n<p><strong><em>Source Counting (SC)</em></strong></p>\n\n<p>‚ÄÇ Advised by Prof. <a href=\"https://acoustics.nju.edu.cn/rydw/szgk/js/lj/index.html\">Jing Lu</a> @<a href=\"https://www.nju.edu.cn\">NJU</a> and Mr. <a href=\"https://www.linkedin.com/in/ÈïøÂÆù-Êú±-a9b778b6/\">Changbao Zhu</a> @<a href=\"https://en.horizon.cc\">Horizon Robotics</a>, Dec. 2018 - Apr. 2019</p>\n\n<ul>\n  <li>Binaural speech SC with similarity and correlation features in various acoustic scenarios</li>\n  <li>Honored the ‚ÄúExcellent Undergraduate Thesis‚Äù at NJU in 2019, and published a patent in 2021</li>\n</ul>\n\n<hr />\n\n",
          "url": "/#experiences",
          
          "relUrl": "/#experiences"
        },
        "7": {
          "doc": "About",
          "title": "Honors &amp; Awards",
          "content": "\n\n<ul>\n  <li>2022: Winner of two tasks in the <a href=\"https://www.competitions.hume.ai/avb2022\">ACII Affective Vocal Bursts (A-VB)</a> competition</li>\n  <li>2019: Excellent Undergraduate Thesis of Nanjing University</li>\n  <li>2018: Meritorious winner prize in <a href=\"https://www.comap.com/contests/mcm-icm\">American Mathematical Contest in Modeling</a></li>\n  <li>2017: Meritorious winner prize in <a href=\"https://en.mcm.edu.cn\">CUMCM</a>, ranked top 1.5% in China</li>\n  <li>2017: National Scholarship, awarded by the Ministry of Education in China<br />\n<!-- - 2016: First Prize, Elite Program Scholarship of Nanjing University --></li>\n</ul>\n\n<!-- ## Miscellaneous -->\n<hr />\n\n",
          "url": "/#honors--awards",
          
          "relUrl": "/#honors--awards"
        },
        "8": {
          "doc": "About",
          "title": "Academic Activities",
          "content": "\n<!-- **_Academic Activities_** -->\n\n<ul>\n  <li>Reviewer of TASLP, ICASSP, INTERSPEECH, COLING, etc.</li>\n  <li>Co-teach ‚ÄúConversational AI systems‚Äù (ASR part) with Prof. Meng and other nice colleagues in CUHK in Fall 2022</li>\n  <li>Every term 2 during 2019-2023 in CUHK: teaching assistant in ENGG1120 (Linear Algebra for Engineers)</li>\n</ul>\n\n<!-- **_Volunteer and Leadership_** -->\n\n<!-- - Associate organizing chair of [2023 International Doctoral Forum][phdforum23] -->\n<!-- - Worked as team leader in [American Mathematical Contest in Modeling][mcm], 2018. -->\n<!-- - Volunteered in psychological consulting with elderly people, folk-art teaching, etc. -->\n<!-- - Organized a rural education research in Jiangxi Province to investigate and call for more attention to rural children's growth and education. Honored the \"Top Ten Teams of Social Practice\" in NJU, 2016. -->\n\n<!-- [phdforum23]: https://phdforum.se.cuhk.edu.hk/2023/index.html -->\n<!-- [xunying]: https://www1.se.cuhk.edu.hk/~xyliu/ -->\n",
          "url": "/#academic-activities",
          
          "relUrl": "/#academic-activities"
        },
        "9": {
          "doc": "About",
          "title": "About",
          "content": "<div class=\"row\">\n  <div class=\"col-sm-3\">\n    <div class=\"preview d-flex align-items-center\">\n      \n      \n\n\n<a href=\"/assets/img/avatar/Jinchao.png\" class=\"popup img-link\" title=\"Jinchao Li\">\n\n<figure>\n  <picture>\n    <!-- Auto scaling with imagemagick -->\n    \n      <source class=\"responsive-img-srcset\" srcset=\"\n          /assets/img/avatar/Jinchao-480.webp 480w,\n          /assets/img/avatar/Jinchao-800.webp 800w,\n          /assets/img/avatar/Jinchao-1280.webp 1280w,\n          \" sizes=\"95vw\" type=\"image/webp\" loading=\"lazy\" data-proofer-ignore=\"\" />\n    \n\n    <!-- Fallback to the original file -->\n    <img src=\"/assets/img/avatar/Jinchao.png\" class=\"preview rounded\" width=\"200\" height=\"auto\" alt=\"Jinchao Li\" data-zoomable=\"\" loading=\"lazy\" decoding=\"async\" onerror=\"this.onerror=null; $('.responsive-img-srcset').remove();\" />\n  </picture>\n</figure>\n\n  </a>\n\n\n    </div>\n  </div>\n\n  <div class=\"col-sm-9\">\n    \n    <p>\n      <strong style=\"font-size: 1.5rem;\">Jinchao Li (ÊùéÈî¶Ë∂Ö)</strong><br />\n      Ph.D. Candidate <a href=\"/assets/CV_jinchaoli.pdf\" target=\"_blank\">[Resume]</a><br />\n      The Chinese University of Hong Kong<br />\n      Hong Kong, China<br />\n      Email: jinchaolovefy [at] gmail.com<br />\n      <div class=\"social\">\n        \n  \n\n  \n    <a href=\"javascript:location.href = 'mailto:' + ['jinchaolovefy','gmail.com'].join('@')\" data-bs-toggle=\"tooltip\" data-bs-placement=\"top\" aria-label=\"email\" data-bs-original-title=\"email\" link-attr-ignore=\"\" style=\"color: #0072c5;\">\n      <i class=\"fas fa-envelope\"></i>\n    </a>\n  \n\n  \n\n  \n    <a href=\"https://scholar.google.com/citations?hl=en&amp;user=SB7xjMoAAAAJ\" data-bs-toggle=\"tooltip\" data-bs-placement=\"top\" aria-label=\"google-scholar\" data-bs-original-title=\"google-scholar\" link-attr-ignore=\"\" target=\"_blank\" rel=\"noopener noreferrer\" style=\"color: #5c92f6;\">\n      <i class=\"fa-brands fa-google-scholar\"></i>\n    </a>\n  \n\n  \n\n  \n    <a href=\"https://github.com/JinchaoLove\" data-bs-toggle=\"tooltip\" data-bs-placement=\"top\" aria-label=\"github\" data-bs-original-title=\"github\" link-attr-ignore=\"\" target=\"_blank\" rel=\"noopener noreferrer\">\n      <i class=\"fab fa-github\"></i>\n    </a>\n  \n\n  \n\n  \n    <a href=\"/feed.xml\" data-bs-toggle=\"tooltip\" data-bs-placement=\"top\" aria-label=\"rss\" data-bs-original-title=\"rss\" link-attr-ignore=\"\" style=\"color: #ee802f;\">\n      <i class=\"fas fa-rss\"></i>\n    </a>\n  \n\n\n      </div>\n    </p>\n  </div>\n</div>\n\n",
          "url": "/",
          
          "relUrl": "/"
        }
      ,
        "10": {
          "doc": "Publications",
          "title": "2025",
          "content": "\n<ol class=\"bibliography\"><li><!-- _layouts/bib.html -->\n<div class=\"row\"><div class=\"col-sm-2\">\n      <div class=\"preview d-flex align-items-center\">\n\n\n<a href=\"/assets/papers/25/gap.png\" class=\"popup img-link\" title=\"papers/25/gap.png\">\n\n<figure>\n  <picture>\n    <!-- Auto scaling with imagemagick -->\n    \n      <source class=\"responsive-img-srcset\" srcset=\"\n          /assets/papers/25/gap-480.webp 480w,\n          /assets/papers/25/gap-800.webp 800w,\n          /assets/papers/25/gap-1280.webp 1280w,\n          \" sizes=\"200px\" type=\"image/webp\" loading=\"lazy\" data-proofer-ignore=\"\" />\n    \n\n    <!-- Fallback to the original file -->\n    <img src=\"/assets/papers/25/gap.png\" class=\"preview rounded\" width=\"100%\" height=\"auto\" alt=\"papers/25/gap.png\" data-zoomable=\"\" loading=\"lazy\" decoding=\"async\" onerror=\"this.onerror=null; $('.responsive-img-srcset').remove();\" />\n  </picture>\n</figure>\n\n  </a>\n\n</div>\n    </div><!-- Entry bib key -->\n  <div id=\"li2025generate\" class=\"col-sm-10\">\n    <!-- Title -->\n    <div class=\"title\">Generate, Align and Predict (GAP): Detecting Neurocognitive Disorders via Cross-modal Consistency in Narratives</div>\n    <!-- Author -->\n    <div class=\"author\">\n      \n\n      \n\n      \n      Junan\n            Li*,&nbsp;<span class=\"highlight-author\">Jinchao\n            Li*</span>,&nbsp;Simon\n            Wong,&nbsp;Xixin\n            Wu, Helen\n      Meng\n    </div>\n\n    <!-- Journal/Book title and date -->\n    \n    \n    <div class=\"periodical\">\n      <em>In ACM-MM</em>, 2025\n    </div>\n    <div class=\"periodical\">\n      \n    </div>\n\n    <!-- Links/Buttons -->\n    <div class=\"links\">\n        <button class=\"abstract btn btn-sm z-depth-0\" title=\"Click to show/hide abstract\">\n          ABS\n          <i class=\"fas fa-sort open\"></i>\n          <i class=\"fas fa-times hidden\"></i>\n        </button>\n        <button class=\"bibtex btn btn-sm z-depth-0\" title=\"Click to show/hide bibtex\">\n          BIB\n          <i class=\"fas fa-sort open\"></i>\n          <i class=\"fas fa-times hidden\"></i>\n        </button>\n    </div>\n    \n      \n      \n      \n\n    <!-- Hidden abstract block -->\n      <div class=\"abstract hidden\">\n        <p>The early detection of neurocognitive disorders (NCDs), such as Alzheimer‚Äôs disease, remains a critical global health challenge due to the limitations of conventional diagnostic tools with high costs and limited accessibility. Visual-stimulated narrative (VSN)-based approach offers a promising alternative by capturing narrative patterns related to holistic cognitive domains. While prior work has focused on unimodal microstructural features (e.g., pauses, lexical diversity), macrostructural impairments‚Äîsuch as disrupted coherence and cross-modal inconsistency between narratives and visual stimuli‚Äîremain understudied despite their clinical importance. To address this, we propose GAP (Generate, Align, and Predict), a novel multimodal framework leveraging advances in Multimodal Large Language Models (MLLMs), Dynamic Programming (DP), and Vision-Language Model (VLM) to evaluate dynamic semantic consistency in VSNs. Evaluated on the Cantonese CU-MARVEL-RABBIT dataset, GAP achieved state-of-the-art performance (F1=0.65, AUC=0.75), outperforming traditional acoustic, linguistic, and pattern-matching baselines. In addition, we conduct an in-depth analysis that reveals key factors that provide insights into cognitive assessment using VSNs.</p>\n      </div><!-- Hidden bibtex block -->\n      <div class=\"bibtex hidden\"><figure class=\"highlight\"><pre><code class=\"language-bibtex\" data-lang=\"bibtex\"><span class=\"nc\">@inproceedings</span><span class=\"p\">{</span><span class=\"nl\">li2025generate</span><span class=\"p\">,</span>\n  <span class=\"na\">title</span> <span class=\"p\">=</span> <span class=\"s\">{Generate, Align and Predict (GAP): Detecting Neurocognitive Disorders via Cross-modal Consistency in Narratives}</span><span class=\"p\">,</span>\n  <span class=\"na\">author</span> <span class=\"p\">=</span> <span class=\"s\">{Li*, Junan and Li*, Jinchao and Wong, Simon and Wu, Xixin and Meng, Helen}</span><span class=\"p\">,</span>\n  <span class=\"na\">booktitle</span> <span class=\"p\">=</span> <span class=\"s\">{ACM-MM}</span><span class=\"p\">,</span>\n  <span class=\"na\">year</span> <span class=\"p\">=</span> <span class=\"s\">{2025}</span><span class=\"p\">,</span>\n  <span class=\"na\">organization</span> <span class=\"p\">=</span> <span class=\"s\">{ACM}</span>\n<span class=\"p\">}</span></code></pre></figure></div>\n\n    </div>\n</div>\n</li>\n<li><!-- _layouts/bib.html -->\n<div class=\"row\"><div class=\"col-sm-2\">\n      <div class=\"preview d-flex align-items-center\">\n\n\n<a href=\"/assets/papers/24/titan.png\" class=\"popup img-link\" title=\"papers/24/titan.png\">\n\n<figure>\n  <picture>\n    <!-- Auto scaling with imagemagick -->\n    \n      <source class=\"responsive-img-srcset\" srcset=\"\n          /assets/papers/24/titan-480.webp 480w,\n          /assets/papers/24/titan-800.webp 800w,\n          /assets/papers/24/titan-1280.webp 1280w,\n          \" sizes=\"200px\" type=\"image/webp\" loading=\"lazy\" data-proofer-ignore=\"\" />\n    \n\n    <!-- Fallback to the original file -->\n    <img src=\"/assets/papers/24/titan.png\" class=\"preview rounded\" width=\"100%\" height=\"auto\" alt=\"papers/24/titan.png\" data-zoomable=\"\" loading=\"lazy\" decoding=\"async\" onerror=\"this.onerror=null; $('.responsive-img-srcset').remove();\" />\n  </picture>\n</figure>\n\n  </a>\n\n</div>\n    </div><!-- Entry bib key -->\n  <div id=\"li2025detecting\" class=\"col-sm-10\">\n    <!-- Title -->\n    <div class=\"title\">Detecting Neurocognitive Disorders through Analyses of Topic Evolution and Cross-modal Consistency in Visual-Stimulated Narratives</div>\n    <!-- Author -->\n    <div class=\"author\">\n      \n\n      \n\n      \n      <span class=\"highlight-author\">Jinchao\n            Li*</span>,&nbsp;Yuejiao\n            Wang*,&nbsp;Junan\n            Li*,&nbsp;Jiawen\n            Kang*,&nbsp;Bo\n            Zheng,&nbsp;Simon\n            Wong,\n          <span class=\"more-authors\" title=\"click to view 10 more authors\" onclick=\"\n                var element = $(this);\n                element.attr('title', '');\n                var more_authors_text = element.text() == '10 more authors' ? 'Brian Mak, Helene Fung, Jean Woo, Man-Wai Mak, Timothy Kwok, Vincent Mok, Xianmin Gong, Xixin Wu, Xunying Liu, Patrick Wong' : '10 more authors';\n                var cursorPosition = 0;\n                var textAdder = setInterval(function(){\n                  element.text(more_authors_text.substring(0, cursorPosition + 1));\n                  if (++cursorPosition == more_authors_text.length){\n                    clearInterval(textAdder);\n                  }\n              }, '10');\n            \">10 more authors</span>, Helen\n      Meng\n    </div>\n\n    <!-- Journal/Book title and date -->\n    \n    \n    <div class=\"periodical\">\n      <em>In JSTSP (under review)</em>, 2025\n    </div>\n    <div class=\"periodical\">\n      \n    </div>\n\n    <!-- Links/Buttons -->\n    <div class=\"links\">\n        <button class=\"abstract btn btn-sm z-depth-0\" title=\"Click to show/hide abstract\">\n          ABS\n          <i class=\"fas fa-sort open\"></i>\n          <i class=\"fas fa-times hidden\"></i>\n        </button>\n        <button class=\"bibtex btn btn-sm z-depth-0\" title=\"Click to show/hide bibtex\">\n          BIB\n          <i class=\"fas fa-sort open\"></i>\n          <i class=\"fas fa-times hidden\"></i>\n        </button>\n        <a href=\"https://arxiv.org/pdf/2501.03727\" class=\"btn btn-sm z-depth-0\" role=\"button\">PDF</a>\n    </div>\n    \n      \n      \n      \n\n    <!-- Hidden abstract block -->\n      <div class=\"abstract hidden\">\n        <p>Early detection of neurocognitive disorders (NCDs) is crucial for timely intervention and disease management. Given that language impairments manifest early in NCD progression, visual-stimulated narrative (VSN)-based analysis offers a promising avenue for NCD detection. Current VSN-based NCD detection methods primarily focus on linguistic microstructures (e.g., pauses, lexical diversity), which are potentially linked to bottom-up (stimulus-driven) cognitive processing. While these features illuminate basic language abilities, the higher-order linguistic macrostructures (e.g., thematic or logical development), which may reflect top-down (concept-driven) cognitive abilities, remain underexplored. These patterns are crucial for NCD detection yet challenging to quantify due to their abstract and complex nature. To bridge this gap, we propose two novel dynamic macrostructural approaches: (1) Dynamic Topic Model (DTM) to track topic evolution over time, and (2) Text-Image Temporal Alignment Network (TITAN) to measure cross-modal consistency between narrative and visual stimuli. Experimental results validated the efficiency of proposed approaches in NCD detection, with TITAN achieving superior performance both on the CU-MARVEL-RABBIT corpus (F1 = 0.7238) and the ADReSS corpus (F1 = 0.8889). The feature contribution analysis revealed that macrostructural features (e.g., topic variability, topic change rate, and topic consistency) constituted the most significant contributors in the model‚Äôs decision pathways, outperforming investigated microstructural features. These findings underscore the critical role of macrostructural patterns in understanding cognitive impairment mechanisms in NCDs.</p>\n      </div><!-- Hidden bibtex block -->\n      <div class=\"bibtex hidden\"><figure class=\"highlight\"><pre><code class=\"language-bibtex\" data-lang=\"bibtex\"><span class=\"nc\">@inproceedings</span><span class=\"p\">{</span><span class=\"nl\">li2025detecting</span><span class=\"p\">,</span>\n  <span class=\"na\">url</span> <span class=\"p\">=</span> <span class=\"s\">{https://arxiv.org/abs/2501.03727}</span><span class=\"p\">,</span>\n  <span class=\"na\">title</span> <span class=\"p\">=</span> <span class=\"s\">{Detecting Neurocognitive Disorders through Analyses of Topic Evolution and Cross-modal Consistency in Visual-Stimulated Narratives}</span><span class=\"p\">,</span>\n  <span class=\"na\">author</span> <span class=\"p\">=</span> <span class=\"s\">{Li*, Jinchao and Wang*, Yuejiao and Li*, Junan and Kang*, Jiawen and Zheng, Bo and Wong, Simon and Mak, Brian and Fung, Helene and Woo, Jean and Mak, Man-Wai and Kwok, Timothy and Mok, Vincent and Gong, Xianmin and Wu, Xixin and Liu, Xunying and Wong, Patrick and Meng, Helen}</span><span class=\"p\">,</span>\n  <span class=\"na\">booktitle</span> <span class=\"p\">=</span> <span class=\"s\">{JSTSP (under review)}</span><span class=\"p\">,</span>\n  <span class=\"na\">year</span> <span class=\"p\">=</span> <span class=\"s\">{2025}</span><span class=\"p\">,</span>\n  <span class=\"na\">organization</span> <span class=\"p\">=</span> <span class=\"s\">{IEEE}</span>\n<span class=\"p\">}</span></code></pre></figure></div>\n\n    </div>\n</div>\n</li></ol>\n",
          "url": "/publications/",
          
          "relUrl": "/publications/"
        },
        "11": {
          "doc": "Publications",
          "title": "2024",
          "content": "\n<ol class=\"bibliography\"><li><!-- _layouts/bib.html -->\n<div class=\"row\"><div class=\"col-sm-2\">\n      <div class=\"preview d-flex align-items-center\">\n\n\n<a href=\"/assets/papers/23/emotion2vec.png\" class=\"popup img-link\" title=\"papers/23/emotion2vec.png\">\n\n<figure>\n  <picture>\n    <!-- Auto scaling with imagemagick -->\n    \n      <source class=\"responsive-img-srcset\" srcset=\"\n          /assets/papers/23/emotion2vec-480.webp 480w,\n          /assets/papers/23/emotion2vec-800.webp 800w,\n          /assets/papers/23/emotion2vec-1280.webp 1280w,\n          \" sizes=\"200px\" type=\"image/webp\" loading=\"lazy\" data-proofer-ignore=\"\" />\n    \n\n    <!-- Fallback to the original file -->\n    <img src=\"/assets/papers/23/emotion2vec.png\" class=\"preview rounded\" width=\"100%\" height=\"auto\" alt=\"papers/23/emotion2vec.png\" data-zoomable=\"\" loading=\"lazy\" decoding=\"async\" onerror=\"this.onerror=null; $('.responsive-img-srcset').remove();\" />\n  </picture>\n</figure>\n\n  </a>\n\n</div>\n    </div><!-- Entry bib key -->\n  <div id=\"ma2023emotion2vec\" class=\"col-sm-10\">\n    <!-- Title -->\n    <div class=\"title\">emotion2vec: Self-Supervised Pre-Training for Speech Emotion Representation</div>\n    <!-- Author -->\n    <div class=\"author\">\n      \n\n      \n\n      \n      Ziyang\n            Ma,&nbsp;Zhisheng\n            Zheng,&nbsp;Jiaxin\n            Ye,&nbsp;<span class=\"highlight-author\">Jinchao\n            Li</span>,&nbsp;Zhifu\n            Gao,&nbsp;Shiliang\n            Zhang, Xie\n      Chen\n    </div>\n\n    <!-- Journal/Book title and date -->\n    \n    \n    <div class=\"periodical\">\n      <em>In ACL</em>, 2024\n    </div>\n    <div class=\"periodical\">\n      \n    </div>\n\n    <!-- Links/Buttons -->\n    <div class=\"links\">\n        <button class=\"abstract btn btn-sm z-depth-0\" title=\"Click to show/hide abstract\">\n          ABS\n          <i class=\"fas fa-sort open\"></i>\n          <i class=\"fas fa-times hidden\"></i>\n        </button>\n        <button class=\"bibtex btn btn-sm z-depth-0\" title=\"Click to show/hide bibtex\">\n          BIB\n          <i class=\"fas fa-sort open\"></i>\n          <i class=\"fas fa-times hidden\"></i>\n        </button>\n        <a href=\"https://arxiv.org/pdf/2312.15185\" class=\"btn btn-sm z-depth-0\" role=\"button\">PDF</a>\n        <a href=\"https://www.modelscope.cn/models/iic/emotion2vec_base_finetuned/summary\" class=\"btn btn-sm z-depth-0\" role=\"button\">DEMO</a>\n        <a href=\"https://github.com/ddlBoJack/emotion2vec\" class=\"btn btn-sm z-depth-0\" role=\"button\">CODE</a>\n    </div>\n    \n      \n      \n      \n\n    <!-- Hidden abstract block -->\n      <div class=\"abstract hidden\">\n        <p>We propose emotion2vec, a universal speech emotion representation model. \n  emotion2vec is pre-trained on open-source unlabeled emotion data through self-supervised online distillation, combining utterance-level loss and frame-level loss during pre-training. \n  emotion2vec outperforms state-of-the-art pre-trained universal models and emotion specialist models by only training linear layers for the speech emotion recognition task on the mainstream IEMOCAP dataset. \n  In addition, emotion2vec shows consistent improvements among 10 different languages of speech emotion recognition datasets. \n  emotion2vec also shows excellent results on other emotion tasks, such as song emotion recognition, emotion prediction in conversation, and sentiment analysis. \n  Comparison experiments, ablation experiments, and visualization comprehensively demonstrate the universal capability of the proposed emotion2vec. \n  To the best of our knowledge, emotion2vec is the first universal representation model in various emotion-related tasks, filling a gap in the field.</p>\n      </div><!-- Hidden bibtex block -->\n      <div class=\"bibtex hidden\"><figure class=\"highlight\"><pre><code class=\"language-bibtex\" data-lang=\"bibtex\"><span class=\"nc\">@inproceedings</span><span class=\"p\">{</span><span class=\"nl\">ma2023emotion2vec</span><span class=\"p\">,</span>\n  <span class=\"na\">url</span> <span class=\"p\">=</span> <span class=\"s\">{https://arxiv.org/abs/2312.15185}</span><span class=\"p\">,</span>\n  <span class=\"na\">title</span> <span class=\"p\">=</span> <span class=\"s\">{emotion2vec: Self-Supervised Pre-Training for Speech Emotion Representation}</span><span class=\"p\">,</span>\n  <span class=\"na\">author</span> <span class=\"p\">=</span> <span class=\"s\">{Ma, Ziyang and Zheng, Zhisheng and Ye, Jiaxin and Li, Jinchao and Gao, Zhifu and Zhang, Shiliang and Chen, Xie}</span><span class=\"p\">,</span>\n  <span class=\"na\">booktitle</span> <span class=\"p\">=</span> <span class=\"s\">{ACL}</span><span class=\"p\">,</span>\n  <span class=\"na\">year</span> <span class=\"p\">=</span> <span class=\"s\">{2024}</span><span class=\"p\">,</span>\n  <span class=\"na\">organization</span> <span class=\"p\">=</span> <span class=\"s\">{ACL}</span>\n<span class=\"p\">}</span></code></pre></figure></div>\n\n    </div>\n</div>\n</li></ol>\n",
          "url": "/publications/",
          
          "relUrl": "/publications/"
        },
        "12": {
          "doc": "Publications",
          "title": "2023",
          "content": "\n<ol class=\"bibliography\"><li><!-- _layouts/bib.html -->\n<div class=\"row\"><div class=\"col-sm-2\">\n      <div class=\"preview d-flex align-items-center\">\n\n\n<a href=\"/assets/papers/23/MultiAD_ICASSP23.png\" class=\"popup img-link\" title=\"papers/23/MultiAD_ICASSP23.png\">\n\n<figure>\n  <picture>\n    <!-- Auto scaling with imagemagick -->\n    \n      <source class=\"responsive-img-srcset\" srcset=\"\n          /assets/papers/23/MultiAD_ICASSP23-480.webp 480w,\n          /assets/papers/23/MultiAD_ICASSP23-800.webp 800w,\n          /assets/papers/23/MultiAD_ICASSP23-1280.webp 1280w,\n          \" sizes=\"200px\" type=\"image/webp\" loading=\"lazy\" data-proofer-ignore=\"\" />\n    \n\n    <!-- Fallback to the original file -->\n    <img src=\"/assets/papers/23/MultiAD_ICASSP23.png\" class=\"preview rounded\" width=\"100%\" height=\"auto\" alt=\"papers/23/MultiAD_ICASSP23.png\" data-zoomable=\"\" loading=\"lazy\" decoding=\"async\" onerror=\"this.onerror=null; $('.responsive-img-srcset').remove();\" />\n  </picture>\n</figure>\n\n  </a>\n\n</div>\n    </div><!-- Entry bib key -->\n  <div id=\"li2023leveraging\" class=\"col-sm-10\">\n    <!-- Title -->\n    <div class=\"title\">Leveraging Pretrained Representations With Task-Related Keywords for Alzheimer‚Äôs Disease Detection</div>\n    <!-- Author -->\n    <div class=\"author\">\n      \n\n      \n\n      \n      <span class=\"highlight-author\">Jinchao\n            Li</span>,&nbsp;Kaitao\n            Song,&nbsp;Junan\n            Li,&nbsp;Bo\n            Zheng,&nbsp;Dongsheng\n            Li,&nbsp;Xixin\n            Wu, Xunying\n          Liu, Helen\n      Meng\n    </div>\n\n    <!-- Journal/Book title and date -->\n    \n    \n    <div class=\"periodical\">\n      <em>In ICASSP</em>, 2023\n    </div>\n    <div class=\"periodical\">\n      \n    </div>\n\n    <!-- Links/Buttons -->\n    <div class=\"links\">\n        <button class=\"abstract btn btn-sm z-depth-0\" title=\"Click to show/hide abstract\">\n          ABS\n          <i class=\"fas fa-sort open\"></i>\n          <i class=\"fas fa-times hidden\"></i>\n        </button>\n        <button class=\"bibtex btn btn-sm z-depth-0\" title=\"Click to show/hide bibtex\">\n          BIB\n          <i class=\"fas fa-sort open\"></i>\n          <i class=\"fas fa-times hidden\"></i>\n        </button>\n        <a href=\"https://arxiv.org/pdf/2303.08019\" class=\"btn btn-sm z-depth-0\" role=\"button\">PDF</a>\n        <a href=\"/assets/papers/23/AD_poster.pdf\" class=\"btn btn-sm z-depth-0\" role=\"button\" target=\"_blank\" rel=\"noopener noreferrer\">POSTER</a>\n    </div>\n    \n      \n      \n      \n\n    <!-- Hidden abstract block -->\n      <div class=\"abstract hidden\">\n        <p>With the global population aging rapidly, Alzheimer‚Äôs disease (AD) is particularly prominent in older adults, which has an insidious onset and leads to a gradual, irreversible deterioration in cognitive domains (memory, communication, etc.). Speech-based AD detection opens up the possibility of widespread screening and timely disease intervention. Recent advances in pre-trained models motivate AD detection modeling to shift from low-level features to high-level representations. This paper presents several efficient methods to extract better AD-related cues from high-level acoustic and linguistic features. Based on these features, the paper also proposes a novel task-oriented approach by modeling the relationship between the participants‚Äô description and the cognitive task. Experiments are carried out on the ADReSS dataset in a binary classification setup, and models are evaluated on the unseen test set. Results and comparison with recent literature demonstrate the efficiency and superior performance of proposed acoustic, linguistic and task-oriented methods. The findings also show the importance of semantic and syntactic information, and feasibility of automation and generalization with the promising audio-only and task-oriented methods for the AD detection task.</p>\n      </div><!-- Hidden bibtex block -->\n      <div class=\"bibtex hidden\"><figure class=\"highlight\"><pre><code class=\"language-bibtex\" data-lang=\"bibtex\"><span class=\"nc\">@inproceedings</span><span class=\"p\">{</span><span class=\"nl\">li2023leveraging</span><span class=\"p\">,</span>\n  <span class=\"na\">url</span> <span class=\"p\">=</span> <span class=\"s\">{https://ieeexplore.ieee.org/document/10096205}</span><span class=\"p\">,</span>\n  <span class=\"na\">title</span> <span class=\"p\">=</span> <span class=\"s\">{Leveraging Pretrained Representations With Task-Related Keywords for Alzheimer‚Äôs Disease Detection}</span><span class=\"p\">,</span>\n  <span class=\"na\">author</span> <span class=\"p\">=</span> <span class=\"s\">{Li, Jinchao and Song, Kaitao and Li, Junan and Zheng, Bo and Li, Dongsheng and Wu, Xixin and Liu, Xunying and Meng, Helen}</span><span class=\"p\">,</span>\n  <span class=\"na\">booktitle</span> <span class=\"p\">=</span> <span class=\"s\">{ICASSP}</span><span class=\"p\">,</span>\n  <span class=\"na\">year</span> <span class=\"p\">=</span> <span class=\"s\">{2023}</span><span class=\"p\">,</span>\n  <span class=\"na\">organization</span> <span class=\"p\">=</span> <span class=\"s\">{IEEE}</span>\n<span class=\"p\">}</span></code></pre></figure></div>\n\n    </div>\n</div>\n</li>\n<li><!-- _layouts/bib.html -->\n<div class=\"row\"><div class=\"col-sm-2\">\n      <div class=\"preview d-flex align-items-center\">\n\n\n<a href=\"/assets/papers/23/AVB_ICASSP23.png\" class=\"popup img-link\" title=\"papers/23/AVB_ICASSP23.png\">\n\n<figure>\n  <picture>\n    <!-- Auto scaling with imagemagick -->\n    \n      <source class=\"responsive-img-srcset\" srcset=\"\n          /assets/papers/23/AVB_ICASSP23-480.webp 480w,\n          /assets/papers/23/AVB_ICASSP23-800.webp 800w,\n          /assets/papers/23/AVB_ICASSP23-1280.webp 1280w,\n          \" sizes=\"200px\" type=\"image/webp\" loading=\"lazy\" data-proofer-ignore=\"\" />\n    \n\n    <!-- Fallback to the original file -->\n    <img src=\"/assets/papers/23/AVB_ICASSP23.png\" class=\"preview rounded\" width=\"100%\" height=\"auto\" alt=\"papers/23/AVB_ICASSP23.png\" data-zoomable=\"\" loading=\"lazy\" decoding=\"async\" onerror=\"this.onerror=null; $('.responsive-img-srcset').remove();\" />\n  </picture>\n</figure>\n\n  </a>\n\n</div>\n    </div><!-- Entry bib key -->\n  <div id=\"li2023hierarchical\" class=\"col-sm-10\">\n    <!-- Title -->\n    <div class=\"title\">A Hierarchical Regression Chain Framework for Affective Vocal Burst Recognition</div>\n    <!-- Author -->\n    <div class=\"author\">\n      \n\n      \n\n      \n      <span class=\"highlight-author\">Jinchao\n            Li</span>,&nbsp;Xixin\n            Wu,&nbsp;Kaitao\n            Song,&nbsp;Dongsheng\n            Li,&nbsp;Xunying\n            Liu, Helen\n      Meng\n    </div>\n\n    <!-- Journal/Book title and date -->\n    \n    \n    <div class=\"periodical\">\n      <em>In ICASSP</em>, 2023\n    </div>\n    <div class=\"periodical\">\n      \n    </div>\n\n    <!-- Links/Buttons -->\n    <div class=\"links\">\n        <button class=\"abstract btn btn-sm z-depth-0\" title=\"Click to show/hide abstract\">\n          ABS\n          <i class=\"fas fa-sort open\"></i>\n          <i class=\"fas fa-times hidden\"></i>\n        </button>\n        <button class=\"bibtex btn btn-sm z-depth-0\" title=\"Click to show/hide bibtex\">\n          BIB\n          <i class=\"fas fa-sort open\"></i>\n          <i class=\"fas fa-times hidden\"></i>\n        </button>\n        <a href=\"https://arxiv.org/pdf/2303.08027\" class=\"btn btn-sm z-depth-0\" role=\"button\">PDF</a>\n        <a href=\"/assets/papers/23/AVB_poster.pdf\" class=\"btn btn-sm z-depth-0\" role=\"button\" target=\"_blank\" rel=\"noopener noreferrer\">POSTER</a>\n        <a href=\"https://github.com/JinchaoLove/AffectiveVocalBurstRecognition\" class=\"btn btn-sm z-depth-0\" role=\"button\">CODE</a>\n    </div>\n    \n      \n      \n      \n\n    <!-- Hidden abstract block -->\n      <div class=\"abstract hidden\">\n        <p>As a common way of emotion signaling via non-linguistic vocalizations, vocal burst (VB) plays an important role in daily social interaction. Understanding and modeling human vocal bursts are indispensable for developing robust and general artificial intelligence. Exploring computational approaches for understanding vocal bursts is attracting increasing research attention. In this work, we propose a hierarchical framework, based on chain regression models, for affective recognition from VBs, that explicitly considers multiple relationships: (i) between emotional states and diverse cultures; (ii) between low-dimensional (arousal &amp; valence) and high-dimensional (10 emotion classes) emotion spaces; and (iii) between various emotion classes within the high-dimensional space. To address the challenge of data sparsity, we also use self-supervised learning (SSL) representations with layer-wise and temporal aggregation modules. The proposed systems participated in the ACII Affective Vocal Burst (A-VB) Challenge 2022 and ranked first in the \"TWO‚Äù and \"CULTURE‚Äù tasks. Experimental results based on the ACII Challenge 2022 dataset demonstrate the superior performance of the proposed system and the effectiveness of considering multiple relationships using hierarchical regression chain models.</p>\n      </div><!-- Hidden bibtex block -->\n      <div class=\"bibtex hidden\"><figure class=\"highlight\"><pre><code class=\"language-bibtex\" data-lang=\"bibtex\"><span class=\"nc\">@inproceedings</span><span class=\"p\">{</span><span class=\"nl\">li2023hierarchical</span><span class=\"p\">,</span>\n  <span class=\"na\">url</span> <span class=\"p\">=</span> <span class=\"s\">{https://ieeexplore.ieee.org/document/10096395/}</span><span class=\"p\">,</span>\n  <span class=\"na\">title</span> <span class=\"p\">=</span> <span class=\"s\">{A Hierarchical Regression Chain Framework for Affective Vocal Burst Recognition}</span><span class=\"p\">,</span>\n  <span class=\"na\">author</span> <span class=\"p\">=</span> <span class=\"s\">{Li, Jinchao and Wu, Xixin and Song, Kaitao and Li, Dongsheng and Liu, Xunying and Meng, Helen}</span><span class=\"p\">,</span>\n  <span class=\"na\">booktitle</span> <span class=\"p\">=</span> <span class=\"s\">{ICASSP}</span><span class=\"p\">,</span>\n  <span class=\"na\">year</span> <span class=\"p\">=</span> <span class=\"s\">{2023}</span><span class=\"p\">,</span>\n  <span class=\"na\">organization</span> <span class=\"p\">=</span> <span class=\"s\">{IEEE}</span>\n<span class=\"p\">}</span></code></pre></figure></div>\n\n    </div>\n</div>\n</li></ol>\n",
          "url": "/publications/",
          
          "relUrl": "/publications/"
        },
        "13": {
          "doc": "Publications",
          "title": "2022",
          "content": "\n<ol class=\"bibliography\"><li><!-- _layouts/bib.html -->\n<div class=\"row\"><div class=\"col-sm-2\">\n      <div class=\"preview d-flex align-items-center\">\n\n\n<a href=\"/assets/papers/22/MER.png\" class=\"popup img-link\" title=\"papers/22/MER.png\">\n\n<figure>\n  <picture>\n    <!-- Auto scaling with imagemagick -->\n    \n      <source class=\"responsive-img-srcset\" srcset=\"\n          /assets/papers/22/MER-480.webp 480w,\n          /assets/papers/22/MER-800.webp 800w,\n          /assets/papers/22/MER-1280.webp 1280w,\n          \" sizes=\"200px\" type=\"image/webp\" loading=\"lazy\" data-proofer-ignore=\"\" />\n    \n\n    <!-- Fallback to the original file -->\n    <img src=\"/assets/papers/22/MER.png\" class=\"preview rounded\" width=\"100%\" height=\"auto\" alt=\"papers/22/MER.png\" data-zoomable=\"\" loading=\"lazy\" decoding=\"async\" onerror=\"this.onerror=null; $('.responsive-img-srcset').remove();\" />\n  </picture>\n</figure>\n\n  </a>\n\n</div>\n    </div><!-- Entry bib key -->\n  <div id=\"li2022context\" class=\"col-sm-10\">\n    <!-- Title -->\n    <div class=\"title\">Context-Aware Multimodal Fusion for Emotion Recognition</div>\n    <!-- Author -->\n    <div class=\"author\">\n      \n\n      \n\n      \n      <span class=\"highlight-author\">Jinchao\n            Li</span>,&nbsp;Shuai\n            Wang,&nbsp;Yang\n            Chao,&nbsp;Xunying\n            Liu, Helen\n      Meng\n    </div>\n\n    <!-- Journal/Book title and date -->\n    \n    \n    <div class=\"periodical\">\n      <em>In INTERSPEECH</em>, 2022\n    </div>\n    <div class=\"periodical\">\n      \n    </div>\n\n    <!-- Links/Buttons -->\n    <div class=\"links\">\n        <button class=\"abstract btn btn-sm z-depth-0\" title=\"Click to show/hide abstract\">\n          ABS\n          <i class=\"fas fa-sort open\"></i>\n          <i class=\"fas fa-times hidden\"></i>\n        </button>\n        <button class=\"bibtex btn btn-sm z-depth-0\" title=\"Click to show/hide bibtex\">\n          BIB\n          <i class=\"fas fa-sort open\"></i>\n          <i class=\"fas fa-times hidden\"></i>\n        </button>\n        <a href=\"/assets/papers/22/MER_paper_IS22.pdf\" class=\"btn btn-sm z-depth-0\" role=\"button\" target=\"_blank\" rel=\"noopener noreferrer\">PDF</a>\n        <a href=\"/assets/papers/22/MER_poster_IS22.png\" class=\"btn btn-sm z-depth-0\" role=\"button\" target=\"_blank\" rel=\"noopener noreferrer\">POSTER</a>\n    </div>\n    \n      \n      \n      \n\n    <!-- Hidden abstract block -->\n      <div class=\"abstract hidden\">\n        <p>Automatic emotion recognition (AER) is an inherently complex multimodal task that aims to automatically determine the emotional state of a given expression. Recent works have witnessed the benefits of upstream pretrained models in both audio and textual modalities for the AER task. However, efforts are still needed to effectively integrate features across multiple modalities, devoting due considerations to granularity mismatch and asynchrony in time steps. In this work, we first validate the effectiveness of the upstream models in a unimodal setup and empirically find that partial fine-tuning of the pretrained model in the feature space can significantly boost performance. Moreover, we take the context of the current sentence to model a more accurate emotional state. Based on the unimodal setups, we further propose several multimodal fusion methods to combine high-level features from the audio and text modalities. Experiments are carried out on the IEMOCAP dataset in a 4-category classification problem and compared with state-of-the-art methods in recent literature. Results show that the proposed models gave a superior performance of up to 84.45% and 80.36% weighted accuracy scores respectively in Session 5 and 5-fold cross-validation settings.</p>\n      </div><!-- Hidden bibtex block -->\n      <div class=\"bibtex hidden\"><figure class=\"highlight\"><pre><code class=\"language-bibtex\" data-lang=\"bibtex\"><span class=\"nc\">@inproceedings</span><span class=\"p\">{</span><span class=\"nl\">li2022context</span><span class=\"p\">,</span>\n  <span class=\"na\">url</span> <span class=\"p\">=</span> <span class=\"s\">{https://www.isca-speech.org/archive/interspeech_2022/li22v_interspeech.html}</span><span class=\"p\">,</span>\n  <span class=\"na\">title</span> <span class=\"p\">=</span> <span class=\"s\">{Context-Aware Multimodal Fusion for Emotion Recognition}</span><span class=\"p\">,</span>\n  <span class=\"na\">author</span> <span class=\"p\">=</span> <span class=\"s\">{Li, Jinchao and Wang, Shuai and Chao, Yang and Liu, Xunying and Meng, Helen}</span><span class=\"p\">,</span>\n  <span class=\"na\">booktitle</span> <span class=\"p\">=</span> <span class=\"s\">{INTERSPEECH}</span><span class=\"p\">,</span>\n  <span class=\"na\">year</span> <span class=\"p\">=</span> <span class=\"s\">{2022}</span><span class=\"p\">,</span>\n  <span class=\"na\">organization</span> <span class=\"p\">=</span> <span class=\"s\">{IEEE}</span>\n<span class=\"p\">}</span></code></pre></figure></div>\n\n    </div>\n</div>\n</li></ol>\n",
          "url": "/publications/",
          
          "relUrl": "/publications/"
        },
        "14": {
          "doc": "Publications",
          "title": "2021",
          "content": "\n<ol class=\"bibliography\"><li><!-- _layouts/bib.html -->\n<div class=\"row\"><div class=\"col-sm-2\">\n      <div class=\"preview d-flex align-items-center\">\n\n\n<a href=\"/assets/papers/21/Comp_AD.png\" class=\"popup img-link\" title=\"papers/21/Comp_AD.png\">\n\n<figure>\n  <picture>\n    <!-- Auto scaling with imagemagick -->\n    \n      <source class=\"responsive-img-srcset\" srcset=\"\n          /assets/papers/21/Comp_AD-480.webp 480w,\n          /assets/papers/21/Comp_AD-800.webp 800w,\n          /assets/papers/21/Comp_AD-1280.webp 1280w,\n          \" sizes=\"200px\" type=\"image/webp\" loading=\"lazy\" data-proofer-ignore=\"\" />\n    \n\n    <!-- Fallback to the original file -->\n    <img src=\"/assets/papers/21/Comp_AD.png\" class=\"preview rounded\" width=\"100%\" height=\"auto\" alt=\"papers/21/Comp_AD.png\" data-zoomable=\"\" loading=\"lazy\" decoding=\"async\" onerror=\"this.onerror=null; $('.responsive-img-srcset').remove();\" />\n  </picture>\n</figure>\n\n  </a>\n\n</div>\n    </div><!-- Entry bib key -->\n  <div id=\"li2021comparative\" class=\"col-sm-10\">\n    <!-- Title -->\n    <div class=\"title\">A Comparative Study of Acoustic and Linguistic Features Classification for Alzheimer‚Äôs Disease Detection</div>\n    <!-- Author -->\n    <div class=\"author\">\n      \n\n      \n\n      \n      <span class=\"highlight-author\">Jinchao\n            Li</span>,&nbsp;Jianwei\n            Yu,&nbsp;Zi\n            Ye,&nbsp;Simon\n            Wong,&nbsp;Manwai\n            Mak,&nbsp;Brian\n            Mak, Xunying\n          Liu, Helen\n      Meng\n    </div>\n\n    <!-- Journal/Book title and date -->\n    \n    \n    <div class=\"periodical\">\n      <em>In ICASSP</em>, 2021\n    </div>\n    <div class=\"periodical\">\n      \n    </div>\n\n    <!-- Links/Buttons -->\n    <div class=\"links\">\n        <button class=\"abstract btn btn-sm z-depth-0\" title=\"Click to show/hide abstract\">\n          ABS\n          <i class=\"fas fa-sort open\"></i>\n          <i class=\"fas fa-times hidden\"></i>\n        </button>\n        <button class=\"bibtex btn btn-sm z-depth-0\" title=\"Click to show/hide bibtex\">\n          BIB\n          <i class=\"fas fa-sort open\"></i>\n          <i class=\"fas fa-times hidden\"></i>\n        </button>\n        <a href=\"https://www1.se.cuhk.edu.hk/~hccl/publications/pub/ICASSP_jcli.pdf\" class=\"btn btn-sm z-depth-0\" role=\"button\">PDF</a>\n        <a href=\"/assets/papers/21/Comp_AD.pdf\" class=\"btn btn-sm z-depth-0\" role=\"button\" target=\"_blank\" rel=\"noopener noreferrer\">POSTER</a>\n        <a href=\"https://github.com/JinchaoLove/NCDdetection_ICASSP2021\" class=\"btn btn-sm z-depth-0\" role=\"button\">CODE</a>\n    </div>\n    \n      \n      \n      \n\n    <!-- Hidden abstract block -->\n      <div class=\"abstract hidden\">\n        <p>With the global population ageing rapidly, Alzheimer‚Äôs Disease (AD) is particularly prominent in older adults, which has an insidious onset followed by gradual, irreversible deterioration in cognitive domains (memory, communication, etc). Thus the detection of Alzheimer‚Äôs Disease is crucial for timely intervention to slow down disease progression. This paper presents a comparative study of different acoustic and linguistic features for the AD detection using various classifiers. Experimental results on ADReSS dataset reflect that the proposed models using ComParE, X-vector, Linguistics, TFIDF and BERT features are able to detect AD with high accuracy and sensitivity, and are comparable with the state-of-the-art results reported. While most previous work used manual transcripts, our results also indicate that similar or even better performance could be obtained using automatically recognized transcripts over manually collected ones. This work achieves accuracy scores at 0.67 for acoustic features and 0.88 for linguistic features on either manual or ASR transcripts on the ADReSS Challenge test set.</p>\n      </div><!-- Hidden bibtex block -->\n      <div class=\"bibtex hidden\"><figure class=\"highlight\"><pre><code class=\"language-bibtex\" data-lang=\"bibtex\"><span class=\"nc\">@inproceedings</span><span class=\"p\">{</span><span class=\"nl\">li2021comparative</span><span class=\"p\">,</span>\n  <span class=\"na\">url</span> <span class=\"p\">=</span> <span class=\"s\">{https://ieeexplore.ieee.org/document/9414147}</span><span class=\"p\">,</span>\n  <span class=\"na\">title</span> <span class=\"p\">=</span> <span class=\"s\">{A Comparative Study of Acoustic and Linguistic Features Classification for Alzheimer's Disease Detection}</span><span class=\"p\">,</span>\n  <span class=\"na\">author</span> <span class=\"p\">=</span> <span class=\"s\">{Li, Jinchao and Yu, Jianwei and Ye, Zi and Wong, Simon and Mak, Manwai and Mak, Brian and Liu, Xunying and Meng, Helen}</span><span class=\"p\">,</span>\n  <span class=\"na\">booktitle</span> <span class=\"p\">=</span> <span class=\"s\">{ICASSP}</span><span class=\"p\">,</span>\n  <span class=\"na\">year</span> <span class=\"p\">=</span> <span class=\"s\">{2021}</span><span class=\"p\">,</span>\n  <span class=\"na\">organization</span> <span class=\"p\">=</span> <span class=\"s\">{IEEE}</span>\n<span class=\"p\">}</span></code></pre></figure></div>\n\n    </div>\n</div>\n</li>\n<li><!-- _layouts/bib.html -->\n<div class=\"row\"><div class=\"col-sm-2\">\n      <div class=\"preview d-flex align-items-center\">\n\n\n<a href=\"/assets/papers/21/ASR_AD.png\" class=\"popup img-link\" title=\"papers/21/ASR_AD.png\">\n\n<figure>\n  <picture>\n    <!-- Auto scaling with imagemagick -->\n    \n      <source class=\"responsive-img-srcset\" srcset=\"\n          /assets/papers/21/ASR_AD-480.webp 480w,\n          /assets/papers/21/ASR_AD-800.webp 800w,\n          /assets/papers/21/ASR_AD-1280.webp 1280w,\n          \" sizes=\"200px\" type=\"image/webp\" loading=\"lazy\" data-proofer-ignore=\"\" />\n    \n\n    <!-- Fallback to the original file -->\n    <img src=\"/assets/papers/21/ASR_AD.png\" class=\"preview rounded\" width=\"100%\" height=\"auto\" alt=\"papers/21/ASR_AD.png\" data-zoomable=\"\" loading=\"lazy\" decoding=\"async\" onerror=\"this.onerror=null; $('.responsive-img-srcset').remove();\" />\n  </picture>\n</figure>\n\n  </a>\n\n</div>\n    </div><!-- Entry bib key -->\n  <div id=\"ye2021development\" class=\"col-sm-10\">\n    <!-- Title -->\n    <div class=\"title\">Development of the CUHK Elderly Speech Recognition System for Neurocognitive Disorder Detection Using the DementiaBank Corpus</div>\n    <!-- Author -->\n    <div class=\"author\">\n      \n\n      \n\n      \n      Zi\n            Ye,&nbsp;Shoukang\n            Hu,&nbsp;<span class=\"highlight-author\">Jinchao\n            Li</span>,&nbsp;Xurong\n            Xie,&nbsp;Mengzhe\n            Geng,&nbsp;Jianwei\n            Yu,\n          <span class=\"more-authors\" title=\"click to view 4 more authors\" onclick=\"\n                var element = $(this);\n                element.attr('title', '');\n                var more_authors_text = element.text() == '4 more authors' ? 'Junhao Xu, Boyang Xue, Shansong Liu, Xunying Liu' : '4 more authors';\n                var cursorPosition = 0;\n                var textAdder = setInterval(function(){\n                  element.text(more_authors_text.substring(0, cursorPosition + 1));\n                  if (++cursorPosition == more_authors_text.length){\n                    clearInterval(textAdder);\n                  }\n              }, '10');\n            \">4 more authors</span>, Helen\n      Meng\n    </div>\n\n    <!-- Journal/Book title and date -->\n    \n    \n    <div class=\"periodical\">\n      <em>In ICASSP</em>, 2021\n    </div>\n    <div class=\"periodical\">\n      \n    </div>\n\n    <!-- Links/Buttons -->\n    <div class=\"links\">\n        <button class=\"abstract btn btn-sm z-depth-0\" title=\"Click to show/hide abstract\">\n          ABS\n          <i class=\"fas fa-sort open\"></i>\n          <i class=\"fas fa-times hidden\"></i>\n        </button>\n        <button class=\"bibtex btn btn-sm z-depth-0\" title=\"Click to show/hide bibtex\">\n          BIB\n          <i class=\"fas fa-sort open\"></i>\n          <i class=\"fas fa-times hidden\"></i>\n        </button>\n        <a href=\"https://www1.se.cuhk.edu.hk/~hccl/publications/pub/zye_revised_v4.pdf\" class=\"btn btn-sm z-depth-0\" role=\"button\">PDF</a>\n        <a href=\"/assets/papers/21/ASR_AD.pdf\" class=\"btn btn-sm z-depth-0\" role=\"button\" target=\"_blank\" rel=\"noopener noreferrer\">POSTER</a>\n    </div>\n    \n      \n      \n      \n\n    <!-- Hidden abstract block -->\n      <div class=\"abstract hidden\">\n        <p>Early diagnosis of Neurocognitive Disorder (NCD) is crucial in facilitating preventive care and timely treatment to delay further progression. This paper presents the development of a state-of-the-art automatic speech recognition (ASR) system built on the Dementia-Bank Pitt corpus for automatic NCD detection. Speed perturbation based audio data augmentation expanded the limited elderly speech data by four times. Large quantities of out-of-domain, non-aged adult speech were exploited by cross-domain adapting a 1000-hour LibriSpeech corpus trained LF-MMI factored TDNN system to DementiaBank. The variability among elderly speakers was modeled using i-Vector and learning hidden unit contributions (LHUC) based speaker adaptive training. Robust Bayesian estimation of TDNN systems and LHUC transforms were used in both cross-domain and speaker adaptation. A Transformer language model was also built to improve the final system performance. A word error rate (WER) reduction of 11.72% absolute (26.11% relative) was obtained over the baseline i-Vector adapted LF-MMI TDNN system on the evaluation data of 48 elderly speakers. The best NCD detection accuracy of 88%, comparable to that using the ground truth speech transcripts, was obtained using the textual features extracted from the final ASR system outputs.</p>\n      </div><!-- Hidden bibtex block -->\n      <div class=\"bibtex hidden\"><figure class=\"highlight\"><pre><code class=\"language-bibtex\" data-lang=\"bibtex\"><span class=\"nc\">@inproceedings</span><span class=\"p\">{</span><span class=\"nl\">ye2021development</span><span class=\"p\">,</span>\n  <span class=\"na\">url</span> <span class=\"p\">=</span> <span class=\"s\">{https://ieeexplore.ieee.org/document/9413634}</span><span class=\"p\">,</span>\n  <span class=\"na\">title</span> <span class=\"p\">=</span> <span class=\"s\">{Development of the CUHK Elderly Speech Recognition System for Neurocognitive Disorder Detection Using the DementiaBank Corpus}</span><span class=\"p\">,</span>\n  <span class=\"na\">author</span> <span class=\"p\">=</span> <span class=\"s\">{Ye, Zi and Hu, Shoukang and Li, Jinchao and Xie, Xurong and Geng, Mengzhe and Yu, Jianwei and Xu, Junhao and Xue, Boyang and Liu, Shansong and Liu, Xunying and Meng, Helen}</span><span class=\"p\">,</span>\n  <span class=\"na\">booktitle</span> <span class=\"p\">=</span> <span class=\"s\">{ICASSP}</span><span class=\"p\">,</span>\n  <span class=\"na\">year</span> <span class=\"p\">=</span> <span class=\"s\">{2021}</span><span class=\"p\">,</span>\n  <span class=\"na\">organization</span> <span class=\"p\">=</span> <span class=\"s\">{IEEE}</span>\n<span class=\"p\">}</span></code></pre></figure></div>\n\n    </div>\n</div>\n</li></ol>\n",
          "url": "/publications/",
          
          "relUrl": "/publications/"
        },
        "15": {
          "doc": "Publications",
          "title": "2019",
          "content": "\n<ol class=\"bibliography\"><li><!-- _layouts/bib.html -->\n<div class=\"row\"><div class=\"col-sm-2\">\n      <div class=\"preview d-flex align-items-center\">\n\n\n<a href=\"/assets/papers/19/SC.png\" class=\"popup img-link\" title=\"papers/19/SC.png\">\n\n<figure>\n  <picture>\n    <!-- Auto scaling with imagemagick -->\n    \n      <source class=\"responsive-img-srcset\" srcset=\"\n          /assets/papers/19/SC-480.webp 480w,\n          /assets/papers/19/SC-800.webp 800w,\n          /assets/papers/19/SC-1280.webp 1280w,\n          \" sizes=\"200px\" type=\"image/webp\" loading=\"lazy\" data-proofer-ignore=\"\" />\n    \n\n    <!-- Fallback to the original file -->\n    <img src=\"/assets/papers/19/SC.png\" class=\"preview rounded\" width=\"100%\" height=\"auto\" alt=\"papers/19/SC.png\" data-zoomable=\"\" loading=\"lazy\" decoding=\"async\" onerror=\"this.onerror=null; $('.responsive-img-srcset').remove();\" />\n  </picture>\n</figure>\n\n  </a>\n\n</div>\n    </div><!-- Entry bib key -->\n  <div id=\"changbao2019method\" class=\"col-sm-10\">\n    <!-- Title -->\n    <div class=\"title\">Method, Device and Electronic Equipment for Determining Sound Source Information based on Microphone Array</div>\n    <!-- Author -->\n    <div class=\"author\">\n      \n\n      \n\n      \n      Changbao\n            Zhu, Jinchao\n      Li\n    </div>\n\n    <!-- Journal/Book title and date -->\n    \n    \n    <div class=\"periodical\">\n      <em>In Patent: CN110148422B</em>, 2019\n    </div>\n    <div class=\"periodical\">\n      \n    </div>\n\n    <!-- Links/Buttons -->\n    <div class=\"links\">\n        <button class=\"abstract btn btn-sm z-depth-0\" title=\"Click to show/hide abstract\">\n          ABS\n          <i class=\"fas fa-sort open\"></i>\n          <i class=\"fas fa-times hidden\"></i>\n        </button>\n        <a href=\"https://patent-image.qichacha.com/pdf/81a740a39373966f3730316bd32e08d5.pdf\" class=\"btn btn-sm z-depth-0\" role=\"button\">PDF</a>\n    </div>\n    \n      \n      \n      \n\n    <!-- Hidden abstract block -->\n      <div class=\"abstract hidden\">\n        <p>Êú¨ÂÖ¨ÂºÄÂÆûÊñΩ‰æãÂÖ¨ÂºÄ‰∫Ü‰∏ÄÁßçÂü∫‰∫é‰º†Â£∞Âô®ÈòµÂàóÁ°ÆÂÆöÂ£∞Ê∫ê‰ø°ÊÅØÁöÑÊñπÊ≥ï,ÂÖ∂‰∏≠,ÊñπÊ≥ïÂåÖÊã¨:Á°ÆÂÆö‰º†Â£∞Âô®ÈòµÂàóÈááÈõÜÁöÑÂ§öË∑ØÈü≥È¢ë‰ø°Âè∑;Á°ÆÂÆöÊâÄËø∞Â§öË∑ØÈü≥È¢ë‰ø°Âè∑ÁöÑÁõ∏‰ººÊÄßÂ∫¶Èáè‰ø°ÊÅØ;Á°ÆÂÆöÊâÄËø∞Â§öË∑ØÈü≥È¢ë‰ø°Âè∑ÁöÑÁõ∏ÂÖ≥ÊÄßÂ∫¶Èáè‰ø°ÊÅØ;Âü∫‰∫éÊâÄËø∞Áõ∏‰ººÊÄßÂ∫¶Èáè‰ø°ÊÅØÂíåÊâÄËø∞Áõ∏ÂÖ≥ÊÄßÂ∫¶Èáè‰ø°ÊÅØ,Á°ÆÂÆöÂ£∞Ê∫ê‰ø°ÊÅØ.ËøòÂÖ¨ÂºÄ‰∫Ü‰∏ÄÁßçÂü∫‰∫é‰º†Â£∞Âô®ÈòµÂàóÁ°ÆÂÆöÂ£∞Ê∫ê‰ø°ÊÅØÁöÑË£ÖÁΩÆ,ÂÖ∂‰∏≠,Ë£ÖÁΩÆÂåÖÊã¨:Èü≥È¢ë‰ø°Âè∑Á°ÆÂÆöÊ®°Âùó,Áõ∏‰ººÊÄßÂ∫¶Èáè‰ø°ÊÅØÁ°ÆÂÆöÊ®°Âùó,Áõ∏ÂÖ≥ÊÄßÂ∫¶Èáè‰ø°ÊÅØÁ°ÆÂÆöÊ®°ÂùóÂíåÂ£∞Ê∫ê‰ø°ÊÅØÁ°ÆÂÆöÊ®°Âùó.Êú¨ÂÖ¨ÂºÄÂÆûÊñΩ‰æãÂèØ‰ª•ÈÄöËøáÁ°ÆÂÆöÂπ∂Âü∫‰∫é‰º†Â£∞Âô®ÈòµÂàóÈááÈõÜÁöÑÂ§öË∑ØÈü≥È¢ë‰ø°Âè∑ÁöÑÁõ∏‰ººÊÄßÂ∫¶Èáè‰ø°ÊÅØÂíåÁõ∏ÂÖ≥ÊÄßÂ∫¶Èáè‰ø°ÊÅØ,Á°ÆÂÆöÂ£∞Ê∫ê‰ø°ÊÅØ,ÂèØ‰ª•ÈÄöËøá‰º†Â£∞Âô®ÈòµÂàóÂæóÂà∞Â£∞Ê∫ê‰ø°ÊÅØ,Êñπ‰æøÂêéÁª≠ËøõË°åË°åÂ£∞Ê∫êÊï∞Èáè‰º∞ËÆ°,ÂèØ‰ª•‰øùËØÅÂ£∞Ê∫êÊï∞Èáè‰º∞ËÆ°ÁöÑÂáÜÁ°ÆÊÄß.</p>\n      </div>\n\n    </div>\n</div>\n</li></ol>\n</div>\n\n",
          "url": "/publications/",
          
          "relUrl": "/publications/"
        },
        "16": {
          "doc": "Publications",
          "title": "Publications",
          "content": "<p>ü§óThanks to all the collaborators for their great work! Check out my <a href=\"https://scholar.google.com/citations?hl=en&amp;user=SB7xjMoAAAAJ\">Google Scholar</a> for more information.</p>\n\n<p><em>* indicates equal contributions.</em></p>\n\n<script src=\"/assets/js/bibsearch.js\" type=\"module\"></script>\n\n<p><input type=\"text\" id=\"bibsearch\" spellcheck=\"false\" autocomplete=\"off\" class=\"search bibsearch-form-input\" placeholder=\"Type to filter\" /></p>\n\n<div class=\"publications\">\n  ",
          "url": "/publications/",
          
          "relUrl": "/publications/"
        }
      ,
        "17": {
          "doc": "Space",
          "title": "ü§ó Let‚Äôs Connect!",
          "content": "\n\n<div class=\"schedule-time\">\nI'm always glad to discuss ideas or just chat! Feel free to email me <a href=\"javascript:location.href = 'mailto:' + ['jinchaolovefy','gmail.com'].join('@')\" link-attr-ignore=\"\"><i class=\"fas fa-envelope\"></i></a> or schedule a meeting <a href=\"https://calendar.notion.so/meet/jcli-7s60p1fdd/95tj3p70\"><i class=\"fas fa-calendar\" aria-hidden=\"true\"></i></a>.\n</div>\n\n<hr />\n\n",
          "url": "/space/#-lets-connect",
          
          "relUrl": "/space/#-lets-connect"
        },
        "18": {
          "doc": "Space",
          "title": "‚≠êÔ∏è Projects",
          "content": "\n\n<div class=\"row justify-content-center\">\n  \n    \n\n\n\n\n\n\n\n\n<div class=\"p-2 text-center mx-auto col-5\">\n  <a href=\"https://github.com/JinchaoLove/CUHK-PhD-Thesis-Template\">\n    <img class=\"light w-100\" alt=\"JinchaoLove/CUHK-PhD-Thesis-Template\" src=\"https://github-readme-stats.vercel.app/api/pin/?username=JinchaoLove&amp;repo=CUHK-PhD-Thesis-Template&amp;theme=default&amp;locale=en&amp;show_owner=false&amp;description_lines_count=2\" loading=\"lazy\" decoding=\"async\" />\n    <img class=\"dark w-100\" alt=\"JinchaoLove/CUHK-PhD-Thesis-Template\" src=\"https://github-readme-stats.vercel.app/api/pin/?username=JinchaoLove&amp;repo=CUHK-PhD-Thesis-Template&amp;theme=dark&amp;locale=en&amp;show_owner=false&amp;description_lines_count=2\" loading=\"lazy\" decoding=\"async\" />\n  </a>\n</div>\n\n  \n    \n\n\n\n\n\n\n\n\n<div class=\"p-2 text-center mx-auto col-5\">\n  <a href=\"https://github.com/JinchaoLove/AffectiveVocalBurstRecognition\">\n    <img class=\"light w-100\" alt=\"JinchaoLove/AffectiveVocalBurstRecognition\" src=\"https://github-readme-stats.vercel.app/api/pin/?username=JinchaoLove&amp;repo=AffectiveVocalBurstRecognition&amp;theme=default&amp;locale=en&amp;show_owner=false&amp;description_lines_count=2\" loading=\"lazy\" decoding=\"async\" />\n    <img class=\"dark w-100\" alt=\"JinchaoLove/AffectiveVocalBurstRecognition\" src=\"https://github-readme-stats.vercel.app/api/pin/?username=JinchaoLove&amp;repo=AffectiveVocalBurstRecognition&amp;theme=dark&amp;locale=en&amp;show_owner=false&amp;description_lines_count=2\" loading=\"lazy\" decoding=\"async\" />\n  </a>\n</div>\n\n  \n    \n\n\n\n\n\n\n\n\n<div class=\"p-2 text-center mx-auto col-5\">\n  <a href=\"https://github.com/JinchaoLove/NCDdetection_ICASSP2021\">\n    <img class=\"light w-100\" alt=\"JinchaoLove/NCDdetection_ICASSP2021\" src=\"https://github-readme-stats.vercel.app/api/pin/?username=JinchaoLove&amp;repo=NCDdetection_ICASSP2021&amp;theme=default&amp;locale=en&amp;show_owner=false&amp;description_lines_count=2\" loading=\"lazy\" decoding=\"async\" />\n    <img class=\"dark w-100\" alt=\"JinchaoLove/NCDdetection_ICASSP2021\" src=\"https://github-readme-stats.vercel.app/api/pin/?username=JinchaoLove&amp;repo=NCDdetection_ICASSP2021&amp;theme=dark&amp;locale=en&amp;show_owner=false&amp;description_lines_count=2\" loading=\"lazy\" decoding=\"async\" />\n  </a>\n</div>\n\n  \n    \n\n\n\n\n\n\n\n\n<div class=\"p-2 text-center mx-auto col-5\">\n  <a href=\"https://github.com/JinchaoLove/ChatGPT-Next-Web-Azure\">\n    <img class=\"light w-100\" alt=\"JinchaoLove/ChatGPT-Next-Web-Azure\" src=\"https://github-readme-stats.vercel.app/api/pin/?username=JinchaoLove&amp;repo=ChatGPT-Next-Web-Azure&amp;theme=default&amp;locale=en&amp;show_owner=false&amp;description_lines_count=2\" loading=\"lazy\" decoding=\"async\" />\n    <img class=\"dark w-100\" alt=\"JinchaoLove/ChatGPT-Next-Web-Azure\" src=\"https://github-readme-stats.vercel.app/api/pin/?username=JinchaoLove&amp;repo=ChatGPT-Next-Web-Azure&amp;theme=dark&amp;locale=en&amp;show_owner=false&amp;description_lines_count=2\" loading=\"lazy\" decoding=\"async\" />\n  </a>\n</div>\n\n  \n</div>\n\n<hr />\n\n",
          "url": "/space/#%EF%B8%8F-projects",
          
          "relUrl": "/space/#Ô∏è-projects"
        },
        "19": {
          "doc": "Space",
          "title": "üåè Journey Across the World",
          "content": "\n\n<script src=\"/assets/lib/d3/d3.min.js\"></script>\n\n<script src=\"/assets/lib/topojson/topojson.min.js\"></script>\n\n<script src=\"/assets/lib/datamaps/datamaps.world.min.js\"></script>\n\n<div id=\"world-map\"></div>\n\n<script>\n  var map = new Datamap({\n    element: document.getElementById('world-map'),\n    fills: {\n      defaultFill: 'rgb(169,212,154)',\n      visited: 'rgb(238,146,100)'\n    },\n    data: {\n      \n        \"CHN\": {'fillKey': 'visited', 'info': \"Motherland\"},\n      \n        \"HKG\": {'fillKey': 'visited', 'info': \"PhD journey\"},\n      \n        \"GBR\": {'fillKey': 'visited', 'info': \"Summer camp\"},\n      \n        \"GRC\": {'fillKey': 'visited', 'info': \"ICASSP conference\"},\n      \n        \"ARE\": {'fillKey': 'visited', 'info': \"Layover check-in at Dubai\"}\n      \n    },\n    geographyConfig: {\n      highlightFillColor: 'visited',\n      popupTemplate: function(geo, data) {\n        if (data) {\n            return '<div class=\"hoverinfo\"><b>' + geo.properties.name + ': ' + data.info + '</b></div>'\n        } else {\n          return '<div class=\"hoverinfo\"><b>' + geo.properties.name + '</b></div>';\n        }\n        }\n    }\n  });\n</script>\n\n<p>\nMy journey üó∫:<br />\n\n  üá®üá≥China, \n\n  üá≠üá∞Hong Kong, \n\n  üá¨üáßUnited Kingdom, \n\n  üá¨üá∑Greece, \n\n  üá¶üá™United Arab Emirates\n\n</p>\n",
          "url": "/space/#-journey-across-the-world",
          
          "relUrl": "/space/#-journey-across-the-world"
        },
        "20": {
          "doc": "Space",
          "title": "Space",
          "content": "<!-- Motto, Projects, Journey, Gallery, Hobby, Meeting -->\n\n<blockquote>\n  <p><b>Life is not a problem to be solved, but a reality to be experienced.</b></p>\n</blockquote>\n\n<blockquote>\n  <p><b>Keep it simple and straightforward (KISS).</b></p>\n</blockquote>\n\n<hr />\n\n",
          "url": "/space/",
          
          "relUrl": "/space/"
        }
      ,
        "21": {
          "doc": "Configure Waline with TiDB",
          "title": "Why Waline + TiDB?",
          "content": "\n\n<p><a href=\"https://waline.js.org/\">Waline</a> is a lightweight, open-source, privacy-focused, and self-hosted comment system with backend UI and rich features, and <a href=\"https://tidbcloud.com/\">TiDB Cloud</a> offers a scalable, MySQL-compatible database with a generous 5GB free tier ‚Äì making them a powerful pair for efficient and reliable comment system.</p>\n\n",
          "url": "/blog/set-waline-with-tidb/#why-waline--tidb",
          
          "relUrl": "/blog/set-waline-with-tidb/#why-waline--tidb"
        },
        "22": {
          "doc": "Configure Waline with TiDB",
          "title": "How to Set Up Waline with TiDB Cloud?",
          "content": "\n\n<p>Follow these steps to deploy and configure <a href=\"https://waline.js.org/\">Waline</a> using <a href=\"https://tidbcloud.com/\">TiDB Cloud</a> as the database backend.</p>\n\n<ol>\n  <li>\n    <p>Create a cluster in <a href=\"https://tidbcloud.com/\">TiDB Database</a> (any name is okay)</p>\n  </li>\n  <li>\n    <p>Connect with the above cluster in your local editor (e.g., VS Code)</p>\n\n    <p>To manage your database and initialize comment tables:</p>\n\n    <ol>\n      <li>\n        <p>Get Connection Details: In the TiDB Cloud console (Overview of the cluster), click the Connect button (top-right corner). You‚Äôll see the connection parameters <code class=\"language-plaintext highlighter-rouge\">HOST</code>, <code class=\"language-plaintext highlighter-rouge\">PORT</code>, <code class=\"language-plaintext highlighter-rouge\">USERNAME</code>, <code class=\"language-plaintext highlighter-rouge\">PASSWORD</code> (generate and copy it), <code class=\"language-plaintext highlighter-rouge\">DATABASE</code> and <code class=\"language-plaintext highlighter-rouge\">CA</code>.</p>\n      </li>\n      <li>\n        <p>Set Up VS Code: See details in <a href=\"https://docs.pingcap.com/tidbcloud/dev-guide-gui-vscode-sqltools/\">TiDB Developer Guide</a>.</p>\n      </li>\n      <li>\n        <p>Initialize the Waline Database Schema: Copy <a href=\"https://github.com/walinejs/waline/blob/main/assets/waline.tidb\">waline.tidb</a> and execute the entire script in VS Code. The script will create a database named <code class=\"language-plaintext highlighter-rouge\">waline</code> with <code class=\"language-plaintext highlighter-rouge\">wl_Comment</code>, <code class=\"language-plaintext highlighter-rouge\">wl_Counter</code> and <code class=\"language-plaintext highlighter-rouge\">wl_Users</code> in it.</p>\n      </li>\n    </ol>\n  </li>\n  <li>\n    <p>Configure <code class=\"language-plaintext highlighter-rouge\">Environment Variables</code> as listed in <a href=\"https://waline.js.org/guide/database.html#tidb\">Waline Guidebook</a></p>\n  </li>\n</ol>\n\n<blockquote class=\"prompt-warning\">\n  <p>When first connecting via VS Code, the default <code class=\"language-plaintext highlighter-rouge\">Database</code> is <code class=\"language-plaintext highlighter-rouge\">test</code>. After running <code class=\"language-plaintext highlighter-rouge\">waline.tidb</code>, the actual database used by Waline is <code class=\"language-plaintext highlighter-rouge\">waline</code>. So in <code class=\"language-plaintext highlighter-rouge\">Environment Variables</code>, set <code class=\"language-plaintext highlighter-rouge\">DATABASE = waline</code>.</p>\n</blockquote>\n",
          "url": "/blog/set-waline-with-tidb/#how-to-set-up-waline-with-tidb-cloud",
          
          "relUrl": "/blog/set-waline-with-tidb/#how-to-set-up-waline-with-tidb-cloud"
        },
        "23": {
          "doc": "Configure Waline with TiDB",
          "title": "Configure Waline with TiDB",
          "content": "",
          "url": "/blog/set-waline-with-tidb/",
          
          "relUrl": "/blog/set-waline-with-tidb/"
        }
      ,
        "24": {
          "doc": "Exchange Homepage and About",
          "title": "Exchange Homepage and About",
          "content": "<p>To change the default site homepage as <code class=\"language-plaintext highlighter-rouge\">About</code>, and the original paginator homepage as <code class=\"language-plaintext highlighter-rouge\">Blog</code>. It may be difficult since <a href=\"https://jekyllrb.com/docs/pagination/\">pagination</a> only works within <code class=\"language-plaintext highlighter-rouge\">index.html</code>, see similar <a href=\"https://github.com/cotes2020/jekyll-theme-chirpy/issues/711\">issue</a>.</p>\n\n<ol>\n  <li>\n    <p>Modify the front matters with permalink:</p>\n\n    <ul>\n      <li>Create a new folder, e.g. <code class=\"language-plaintext highlighter-rouge\">/blog/</code>, and move <code class=\"language-plaintext highlighter-rouge\">index.html</code> inside it.</li>\n      <li>Add <code class=\"language-plaintext highlighter-rouge\">permalink: /</code> in the front matter of <code class=\"language-plaintext highlighter-rouge\">/_tabs/about.md</code>.</li>\n    </ul>\n  </li>\n  <li>\n    <p>Add <code class=\"language-plaintext highlighter-rouge\">paginate_path</code> in <code class=\"language-plaintext highlighter-rouge\">/_config.yml</code> to point to the folder <code class=\"language-plaintext highlighter-rouge\">/blog/</code></p>\n\n    <div class=\"language-diff highlighter-rouge\"><div class=\"highlight\"><pre class=\"highlight\"><code><table class=\"rouge-table\"><tbody><tr><!-- <td class=\"rouge-gutter gl\"><pre class=\"lineno\">1\n2\n3\n4\n</pre></td> --><td class=\"rouge-code\"><pre> + paginate_path: \"/blog/page:num/\"\n ...\n - permalink: /posts/:title/\n + permalink: /blog/:title/\n</pre></td></tr></tbody></table></code></pre></div>    </div>\n  </li>\n  <li>\n    <p>Add <code class=\"language-plaintext highlighter-rouge\">BLOG</code> as a new tab or navigation item, refer to this <a href=\"https://github.com/cotes2020/jekyll-theme-chirpy/issues/855\">issue</a>.</p>\n\n    <ul>\n      <li>\n        <p>Modify <code class=\"language-plaintext highlighter-rouge\">/_includes/sidebar.html</code></p>\n\n        <div class=\"language-html highlighter-rouge\"><div class=\"highlight\"><pre class=\"highlight\"><code><table class=\"rouge-table\"><tbody><tr><!-- <td class=\"rouge-gutter gl\"><pre class=\"lineno\">1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n</pre></td> --><td class=\"rouge-code\"><pre>  <span class=\"c\">&lt;!-- home --&gt;</span>\n  - <span class=\"nt\">&lt;li</span> <span class=\"na\">class=</span><span class=\"s\">\"nav-item{% if page.layout == 'home' %}{{ \"</span> <span class=\"na\">active</span> <span class=\"err\">\"</span> <span class=\"err\">}}{%</span> <span class=\"na\">endif</span> <span class=\"err\">%}\"</span><span class=\"nt\">&gt;</span>\n  + <span class=\"nt\">&lt;li</span> <span class=\"na\">class=</span><span class=\"s\">\"nav-item{% if page.url == '/' | relative_url %}{{ \"</span> <span class=\"na\">active</span> <span class=\"err\">\"</span> <span class=\"err\">}}{%</span> <span class=\"na\">endif</span> <span class=\"err\">%}\"</span><span class=\"nt\">&gt;</span>\n  <span class=\"nt\">&lt;a</span> <span class=\"na\">href=</span><span class=\"s\">\"{{ '/' | relative_url }}\"</span> <span class=\"na\">class=</span><span class=\"s\">\"nav-link\"</span><span class=\"nt\">&gt;</span>\n      <span class=\"nt\">&lt;i</span> <span class=\"na\">class=</span><span class=\"s\">\"fa-fw fas fa-home\"</span><span class=\"nt\">&gt;&lt;/i&gt;</span>\n      <span class=\"nt\">&lt;span&gt;</span>{{ site.data.locales[include.lang].tabs.home | upcase }}<span class=\"nt\">&lt;/span&gt;</span>\n  <span class=\"nt\">&lt;/a&gt;</span>\n  <span class=\"nt\">&lt;/li&gt;</span>\n  + <span class=\"c\">&lt;!-- blog --&gt;</span>\n  + <span class=\"nt\">&lt;li</span> <span class=\"na\">class=</span><span class=\"s\">\"nav-item{% if page.url == '/blog/' | relative_url %}{{ \"</span> <span class=\"na\">active</span> <span class=\"err\">\"</span> <span class=\"err\">}}{%</span> <span class=\"na\">endif</span> <span class=\"err\">%}\"</span><span class=\"nt\">&gt;</span>\n  +   <span class=\"nt\">&lt;a</span> <span class=\"na\">href=</span><span class=\"s\">\"{{ '/blog/' | relative_url }}\"</span> <span class=\"na\">class=</span><span class=\"s\">\"nav-link\"</span><span class=\"nt\">&gt;</span>\n  +     <span class=\"nt\">&lt;i</span> <span class=\"na\">class=</span><span class=\"s\">\"fa-fw fas fa-pen\"</span><span class=\"nt\">&gt;&lt;/i&gt;</span> \n  +     <span class=\"nt\">&lt;span&gt;</span>BLOG<span class=\"nt\">&lt;/span&gt;</span>\n  +   <span class=\"nt\">&lt;/a&gt;</span>\n  + <span class=\"nt\">&lt;/li&gt;</span>\n  <span class=\"c\">&lt;!-- the real tabs --&gt;</span>\n  {% for tab in site.tabs %}\n  +     {% if tab.url == '/' | relative_url %}{% continue %}{% endif %}\n</pre></td></tr></tbody></table></code></pre></div>        </div>\n      </li>\n      <li>\n        <p>Modify <code class=\"language-plaintext highlighter-rouge\">/_includes/topbar.html</code></p>\n\n        <div class=\"language-html highlighter-rouge\"><div class=\"highlight\"><pre class=\"highlight\"><code><table class=\"rouge-table\"><tbody><tr><!-- <td class=\"rouge-gutter gl\"><pre class=\"lineno\">1\n2\n3\n4\n5\n6\n7\n</pre></td> --><td class=\"rouge-code\"><pre>  - {% if paths.size == 0 or page.layout == 'home' %}\n  -   <span class=\"c\">&lt;!-- index page --&gt;</span>\n  + {% if paths.size == 0 %}\n  +   <span class=\"c\">&lt;!-- home page --&gt;</span>\n  ...\n  -           {% elsif page.layout == 'category' or page.layout == 'tag' %}\n  +           {% elsif page.layout == 'category' or page.layout == 'tag' or page.layout == 'home' %}\n</pre></td></tr></tbody></table></code></pre></div>        </div>\n      </li>\n    </ul>\n  </li>\n  <li>\n    <p>Replace <code class=\"language-plaintext highlighter-rouge\">page.layout == 'home'</code> with <code class=\"language-plaintext highlighter-rouge\">page.url == '/' or page.url == site.baseurl</code> if needed, including <code class=\"language-plaintext highlighter-rouge\">_layouts/page.html</code>, <code class=\"language-plaintext highlighter-rouge\">_includes/head.html</code>, <code class=\"language-plaintext highlighter-rouge\">/_includes/sidebar.html</code>, and <code class=\"language-plaintext highlighter-rouge\">_includes/topbar.html</code>.</p>\n  </li>\n  <li>\n    <p>(Optional) Modify your style in <code class=\"language-plaintext highlighter-rouge\">/_tabs/about.md</code>. For example, this demo we disable <code class=\"language-plaintext highlighter-rouge\">title</code>, <code class=\"language-plaintext highlighter-rouge\">post-meta</code> and <code class=\"language-plaintext highlighter-rouge\">tails</code> in the homepage by assigning bool values <code class=\"language-plaintext highlighter-rouge\">has_title</code>, <code class=\"language-plaintext highlighter-rouge\">has_meta</code> and <code class=\"language-plaintext highlighter-rouge\">has_tail</code> to skip related part in <code class=\"language-plaintext highlighter-rouge\">_layouts/page.html</code> and <code class=\"language-plaintext highlighter-rouge\">_layouts/post.html</code>.</p>\n  </li>\n</ol>\n",
          "url": "/blog/exchange-homepage-and-about/",
          
          "relUrl": "/blog/exchange-homepage-and-about/"
        }
      ,
        "25": {
          "doc": "Customize Your Jekyll Chirpy Theme",
          "title": "Customize Your Jekyll Chirpy Theme",
          "content": "<p><a href=\"https://github.com/cotes2020/jekyll-theme-chirpy\">Jekyll Chirpy Theme</a> is a highly customizable theme with a large number of users and a very complete ecology. You can reproduce or adjust the configuration according to the official <a href=\"https://github.com/cotes2020/jekyll-theme-chirpy/wiki\">wiki</a> to realize your own magic modification.</p>\n\n<p>This blog introduces some basic tutorials to customize your own Chirpy theme, suitable for those friends who are not satisfied with the Chirpy configuration and want more blogging functions. Welcome to <a href=\"https://github.com/JinchaoLove/jekyll-chirpy-demo/issues\">issue</a> or <a href=\"https://github.com/JinchaoLove/jekyll-chirpy-demo/pulls\">join us</a> to help others build blogs.</p>\n\n<ul>\n  <li>\n    <p><a href=\"/blog/exchange-homepage-and-about/\">Exchange Homepage and About</a></p>\n  </li>\n  <li>\n    <p><a href=\"https://nihil.cc/posts/use_valine/\">Replace Disqus with Valine for Discussion</a></p>\n  </li>\n  <li>\n    <p><a href=\"https://blog.ixiaocai.net/posts/Astro-Blog-Customize-4-Waline-Enhancement/\">Use Waline for Discussion</a></p>\n  </li>\n</ul>\n\n",
          "url": "/blog/customize-jekyll-chirpy/",
          
          "relUrl": "/blog/customize-jekyll-chirpy/"
        }
      ,
        "26": {
          "doc": "Getting Started",
          "title": "Prerequisites",
          "content": "\n\n<p>Follow the instructions in the <a href=\"https://jekyllrb.com/docs/installation/\">Jekyll Docs</a> to complete the installation of the basic environment. <a href=\"https://git-scm.com/\">Git</a> also needs to be installed.</p>\n\n",
          "url": "/blog/getting-started/#prerequisites",
          
          "relUrl": "/blog/getting-started/#prerequisites"
        },
        "27": {
          "doc": "Getting Started",
          "title": "Installation",
          "content": "\n\n<h3 id=\"creating-a-new-site\">Creating a New Site</h3>\n\n<p>There are two ways to create a new repository for this theme:</p>\n\n<ul>\n  <li><a href=\"#option-1-using-the-chirpy-starter\"><strong>Using the Chirpy Starter</strong></a> - Easy to upgrade, isolates irrelevant project files so you can focus on writing.</li>\n  <li><a href=\"#option-2-github-fork\"><strong>GitHub Fork</strong></a> - Convenient for custom development, but difficult to upgrade. Unless you are familiar with Jekyll and are determined to tweak or contribute to this project, this approach is not recommended.</li>\n</ul>\n\n<h4 id=\"option-1-using-the-chirpy-starter\">Option 1. Using the Chirpy Starter</h4>\n\n<p>Sign in to GitHub and browse to <a href=\"https://github.com/cotes2020/chirpy-starter\"><strong>Chirpy Starter</strong></a>, click the button <kbd>Use this template</kbd> &gt; <kbd>Create a new repository</kbd>, and name the new repository <code class=\"language-plaintext highlighter-rouge\">USERNAME.github.io</code>, where <code class=\"language-plaintext highlighter-rouge\">USERNAME</code> represents your GitHub username.</p>\n\n<h4 id=\"option-2-github-fork\">Option 2. GitHub Fork</h4>\n\n<p>Sign in to GitHub to <a href=\"https://github.com/cotes2020/jekyll-theme-chirpy/fork\">fork <strong>Chirpy</strong></a>, and then rename it to <code class=\"language-plaintext highlighter-rouge\">USERNAME.github.io</code> (<code class=\"language-plaintext highlighter-rouge\">USERNAME</code> means your username).</p>\n\n<p>Next, clone your site to local machine. In order to build JavaScript files later, we need to install <a href=\"https://nodejs.org/\">Node.js</a>, and then run the tool:</p>\n\n<div class=\"language-console highlighter-rouge\"><div class=\"highlight\"><pre class=\"highlight\"><code><table class=\"rouge-table\"><tbody><tr><!-- <td class=\"rouge-gutter gl\"><pre class=\"lineno\">1\n</pre></td> --><td class=\"rouge-code\"><pre><span class=\"go\">bash tools/init\n</span></pre></td></tr></tbody></table></code></pre></div></div>\n\n<blockquote class=\"prompt-info\">\n  <p>If you don‚Äôt want to deploy your site on GitHub Pages, append option <code class=\"language-plaintext highlighter-rouge\">--no-gh</code> at the end of the above command.</p>\n</blockquote>\n\n<p>The above command will:</p>\n\n<ol>\n  <li>Check out the code to the <a href=\"https://github.com/cotes2020/jekyll-theme-chirpy/tags\">latest tag</a> (to ensure the stability of your site: as the code for the default branch is under development).</li>\n  <li>Remove non-essential sample files and take care of GitHub-related files.</li>\n  <li>Build JavaScript files and export to <code class=\"language-plaintext filepath highlighter-rouge\">assets/js/dist/</code>, then make them tracked by Git.</li>\n  <li>Automatically create a new commit to save the changes above.</li>\n</ol>\n\n<h3 id=\"installing-dependencies\">Installing Dependencies</h3>\n\n<p>Before running local server for the first time, go to the root directory of your site and run:</p>\n\n<div class=\"language-console highlighter-rouge\"><div class=\"highlight\"><pre class=\"highlight\"><code><table class=\"rouge-table\"><tbody><tr><!-- <td class=\"rouge-gutter gl\"><pre class=\"lineno\">1\n</pre></td> --><td class=\"rouge-code\"><pre><span class=\"go\">bundle\n</span></pre></td></tr></tbody></table></code></pre></div></div>\n\n",
          "url": "/blog/getting-started/#installation",
          
          "relUrl": "/blog/getting-started/#installation"
        },
        "28": {
          "doc": "Getting Started",
          "title": "Usage",
          "content": "\n\n<h3 id=\"configuration\">Configuration</h3>\n\n<p>Update the variables of <code class=\"language-plaintext filepath highlighter-rouge\">_config.yml</code> as needed. Some of them are typical options:</p>\n\n<ul>\n  <li><code class=\"language-plaintext highlighter-rouge\">url</code></li>\n  <li><code class=\"language-plaintext highlighter-rouge\">avatar</code></li>\n  <li><code class=\"language-plaintext highlighter-rouge\">timezone</code></li>\n  <li><code class=\"language-plaintext highlighter-rouge\">lang</code></li>\n</ul>\n\n<h3 id=\"customizing-stylesheet\">Customizing Stylesheet</h3>\n\n<p>If you need to customize the stylesheet, copy the theme‚Äôs <code class=\"language-plaintext filepath highlighter-rouge\">assets/css/style.scss</code> to the same path on your Jekyll site, and then add the custom style at the end of it.</p>\n\n<p>Starting with version <code class=\"language-plaintext highlighter-rouge\">4.1.0</code>, if you want to overwrite the SASS variables defined in <code class=\"language-plaintext filepath highlighter-rouge\">_sass/addon/variables.scss</code>, copy the main sass file <code class=\"language-plaintext filepath highlighter-rouge\">_sass/jekyll-theme-chirpy.scss</code> into the <code class=\"language-plaintext filepath highlighter-rouge\">_sass</code> directory in your site‚Äôs source, then create a new file <code class=\"language-plaintext filepath highlighter-rouge\">_sass/variables-hook.scss</code> and assign new value.</p>\n\n<h3 id=\"customing-static-assets\">Customing Static Assets</h3>\n\n<p>Static assets configuration was introduced in version <code class=\"language-plaintext highlighter-rouge\">5.1.0</code>. The CDN of the static assets is defined by file <code class=\"language-plaintext filepath highlighter-rouge\">_data/origin/cors.yml</code>, and you can replace some of them according to the network conditions in the region where your website is published.</p>\n\n<p>Also, if you‚Äôd like to self-host the static assets, please refer to the <a href=\"https://github.com/cotes2020/chirpy-static-assets#readme\"><em>chirpy-static-assets</em></a>.</p>\n\n<h3 id=\"running-local-server\">Running Local Server</h3>\n\n<p>You may want to preview the site contents before publishing, so just run it by:</p>\n\n<div class=\"language-console highlighter-rouge\"><div class=\"highlight\"><pre class=\"highlight\"><code><table class=\"rouge-table\"><tbody><tr><!-- <td class=\"rouge-gutter gl\"><pre class=\"lineno\">1\n</pre></td> --><td class=\"rouge-code\"><pre><span class=\"go\">bundle exec jekyll s\n</span></pre></td></tr></tbody></table></code></pre></div></div>\n\n<p>Or run the site on Docker with the following command:</p>\n\n<div class=\"language-console highlighter-rouge\"><div class=\"highlight\"><pre class=\"highlight\"><code><table class=\"rouge-table\"><tbody><tr><!-- <td class=\"rouge-gutter gl\"><pre class=\"lineno\">1\n2\n3\n4\n</pre></td> --><td class=\"rouge-code\"><pre><span class=\"go\">docker run -it --rm \\\n</span><span class=\"gp\">    --volume=\"$</span>PWD:/srv/jekyll<span class=\"s2\">\" </span><span class=\"se\">\\</span><span class=\"s2\">\n</span><span class=\"go\">    -p 4000:4000 jekyll/jekyll \\\n    jekyll serve\n</span></pre></td></tr></tbody></table></code></pre></div></div>\n\n<p>After a few seconds, the local service will be published at <em><a href=\"http://127.0.0.1:4000\">http://127.0.0.1:4000</a></em>.</p>\n\n",
          "url": "/blog/getting-started/#usage",
          
          "relUrl": "/blog/getting-started/#usage"
        },
        "29": {
          "doc": "Getting Started",
          "title": "Deployment",
          "content": "\n\n<p>Before the deployment begins, check out the file <code class=\"language-plaintext filepath highlighter-rouge\">_config.yml</code> and make sure the <code class=\"language-plaintext highlighter-rouge\">url</code> is configured correctly. Furthermore, if you prefer the <a href=\"https://help.github.com/en/github/working-with-github-pages/about-github-pages#types-of-github-pages-sites\"><strong>project site</strong></a> and don‚Äôt use a custom domain, or you want to visit your website with a base URL on a web server other than <strong>GitHub Pages</strong>, remember to change the <code class=\"language-plaintext highlighter-rouge\">baseurl</code> to your project name that starts with a slash, e.g, <code class=\"language-plaintext highlighter-rouge\">/project-name</code>.</p>\n\n<p>Now you can choose <em>ONE</em> of the following methods to deploy your Jekyll site.</p>\n\n<h3 id=\"deploy-by-using-github-actions\">Deploy by Using GitHub Actions</h3>\n\n<p>There are a few things to get ready for.</p>\n\n<ul>\n  <li>If you‚Äôre on the GitHub Free plan, keep your site repository public.</li>\n  <li>\n    <p>If you have committed <code class=\"language-plaintext filepath highlighter-rouge\">Gemfile.lock</code> to the repository, and your local machine is not running Linux, go the the root of your site and update the platform list of the lock-file:</p>\n\n    <div class=\"language-console highlighter-rouge\"><div class=\"highlight\"><pre class=\"highlight\"><code><table class=\"rouge-table\"><tbody><tr><!-- <td class=\"rouge-gutter gl\"><pre class=\"lineno\">1\n</pre></td> --><td class=\"rouge-code\"><pre><span class=\"go\">bundle lock --add-platform x86_64-linux\n</span></pre></td></tr></tbody></table></code></pre></div>    </div>\n  </li>\n</ul>\n\n<p>Next, configure the <em>Pages</em> service.</p>\n\n<ol>\n  <li>\n    <p>Browse to your repository on GitHub. Select the tab <em>Settings</em>, then click <em>Pages</em> in the left navigation bar. Then, in the <strong>Source</strong> section (under <em>Build and deployment</em>), select <a href=\"https://docs.github.com/en/pages/getting-started-with-github-pages/configuring-a-publishing-source-for-your-github-pages-site#publishing-with-a-custom-github-actions-workflow\"><strong>GitHub Actions</strong></a> from the dropdown menu.  <br />\n<img src=\"/assets/img/demos/pages-source-light.png\" alt=\"Build source\" class=\"light border normal\" w=\"375\" h=\"140\" /><br />\n<img src=\"/assets/img/demos/pages-source-dark.png\" alt=\"Build source\" class=\"dark normal\" w=\"375\" h=\"140\" /></p>\n  </li>\n  <li>\n    <p>Push any commits to GitHub to trigger the <em>Actions</em> workflow. In the <em>Actions</em> tab of your repository, you should see the workflow <em>Build and Deploy</em> running. Once the build is complete and successful, the site will be deployed automatically.</p>\n  </li>\n</ol>\n\n<p>At this point, you can go to the URL indicated by GitHub to access your site.</p>\n\n<h3 id=\"manually-build-and-deploy\">Manually Build and Deploy</h3>\n\n<p>On self-hosted servers, you cannot enjoy the convenience of <strong>GitHub Actions</strong>. Therefore, you should build the site on your local machine and then upload the site files to the server.</p>\n\n<p>Go to the root of the source project, and build your site as follows:</p>\n\n<div class=\"language-console highlighter-rouge\"><div class=\"highlight\"><pre class=\"highlight\"><code><table class=\"rouge-table\"><tbody><tr><!-- <td class=\"rouge-gutter gl\"><pre class=\"lineno\">1\n</pre></td> --><td class=\"rouge-code\"><pre><span class=\"go\">JEKYLL_ENV=production bundle exec jekyll b\n</span></pre></td></tr></tbody></table></code></pre></div></div>\n\n<p>Or build the site on Docker:</p>\n\n<div class=\"language-console highlighter-rouge\"><div class=\"highlight\"><pre class=\"highlight\"><code><table class=\"rouge-table\"><tbody><tr><!-- <td class=\"rouge-gutter gl\"><pre class=\"lineno\">1\n2\n3\n4\n5\n</pre></td> --><td class=\"rouge-code\"><pre><span class=\"go\">docker run -it --rm \\\n    --env JEKYLL_ENV=production \\\n</span><span class=\"gp\">    --volume=\"$</span>PWD:/srv/jekyll<span class=\"s2\">\" </span><span class=\"se\">\\</span><span class=\"s2\">\n</span><span class=\"go\">    jekyll/jekyll \\\n    jekyll build\n</span></pre></td></tr></tbody></table></code></pre></div></div>\n\n<p>Unless you specified the output path, the generated site files will be placed in folder <code class=\"language-plaintext filepath highlighter-rouge\">_site</code> of the project‚Äôs root directory. Now you should upload those files to the target server.</p>\n\n",
          "url": "/blog/getting-started/#deployment",
          
          "relUrl": "/blog/getting-started/#deployment"
        },
        "30": {
          "doc": "Getting Started",
          "title": "Getting Started",
          "content": "",
          "url": "/blog/getting-started/",
          
          "relUrl": "/blog/getting-started/"
        }
      ,
        "31": {
          "doc": "Writing a New Post",
          "title": "Naming and Path",
          "content": "\n\n<p>Create a new file named <code class=\"language-plaintext filepath highlighter-rouge\">YYYY-MM-DD-TITLE.EXTENSION</code> and put it in the <code class=\"language-plaintext filepath highlighter-rouge\">_posts</code> of the root directory. Please note that the <code class=\"language-plaintext filepath highlighter-rouge\">EXTENSION</code> must be one of <code class=\"language-plaintext filepath highlighter-rouge\">md</code> and <code class=\"language-plaintext filepath highlighter-rouge\">markdown</code>. If you want to save time of creating files, please consider using the plugin <a href=\"https://github.com/jekyll/jekyll-compose\"><code class=\"language-plaintext highlighter-rouge\">Jekyll-Compose</code></a> to accomplish this.</p>\n\n",
          "url": "/blog/write-a-new-post/#naming-and-path",
          
          "relUrl": "/blog/write-a-new-post/#naming-and-path"
        },
        "32": {
          "doc": "Writing a New Post",
          "title": "Front Matter",
          "content": "\n\n<p>Basically, you need to fill the <a href=\"https://jekyllrb.com/docs/front-matter/\">Front Matter</a> as below at the top of the post:</p>\n\n<div class=\"language-yaml highlighter-rouge\"><div class=\"highlight\"><pre class=\"highlight\"><code><table class=\"rouge-table\"><tbody><tr><!-- <td class=\"rouge-gutter gl\"><pre class=\"lineno\">1\n2\n3\n4\n5\n6\n</pre></td> --><td class=\"rouge-code\"><pre><span class=\"nn\">---</span>\n<span class=\"na\">title</span><span class=\"pi\">:</span> <span class=\"s\">TITLE</span>\n<span class=\"na\">date</span><span class=\"pi\">:</span> <span class=\"s\">YYYY-MM-DD HH:MM:SS +/-TTTT</span>\n<span class=\"na\">categories</span><span class=\"pi\">:</span> <span class=\"pi\">[</span><span class=\"nv\">TOP_CATEGORIE</span><span class=\"pi\">,</span> <span class=\"nv\">SUB_CATEGORIE</span><span class=\"pi\">]</span>\n<span class=\"na\">tags</span><span class=\"pi\">:</span> <span class=\"pi\">[</span><span class=\"nv\">TAG</span><span class=\"pi\">]</span>     <span class=\"c1\"># TAG names should always be lowercase</span>\n<span class=\"nn\">---</span>\n</pre></td></tr></tbody></table></code></pre></div></div>\n\n<blockquote class=\"prompt-tip\">\n  <p>The posts‚Äô <em>layout</em> has been set to <code class=\"language-plaintext highlighter-rouge\">post</code> by default, so there is no need to add the variable <em>layout</em> in the Front Matter block.</p>\n</blockquote>\n\n<h3 id=\"timezone-of-date\">Timezone of Date</h3>\n\n<p>In order to accurately record the release date of a post, you should not only set up the <code class=\"language-plaintext highlighter-rouge\">timezone</code> of <code class=\"language-plaintext filepath highlighter-rouge\">_config.yml</code> but also provide the post‚Äôs timezone in variable <code class=\"language-plaintext highlighter-rouge\">date</code> of its Front Matter block. Format: <code class=\"language-plaintext highlighter-rouge\">+/-TTTT</code>, e.g. <code class=\"language-plaintext highlighter-rouge\">+0800</code>.</p>\n\n<h3 id=\"categories-and-tags\">Categories and Tags</h3>\n\n<p>The <code class=\"language-plaintext highlighter-rouge\">categories</code> of each post are designed to contain up to two elements, and the number of elements in <code class=\"language-plaintext highlighter-rouge\">tags</code> can be zero to infinity. For instance:</p>\n\n<div class=\"language-yaml highlighter-rouge\"><div class=\"highlight\"><pre class=\"highlight\"><code><table class=\"rouge-table\"><tbody><tr><!-- <td class=\"rouge-gutter gl\"><pre class=\"lineno\">1\n2\n3\n4\n</pre></td> --><td class=\"rouge-code\"><pre><span class=\"nn\">---</span>\n<span class=\"na\">categories</span><span class=\"pi\">:</span> <span class=\"pi\">[</span><span class=\"nv\">Animal</span><span class=\"pi\">,</span> <span class=\"nv\">Insect</span><span class=\"pi\">]</span>\n<span class=\"na\">tags</span><span class=\"pi\">:</span> <span class=\"pi\">[</span><span class=\"nv\">bee</span><span class=\"pi\">]</span>\n<span class=\"nn\">---</span>\n</pre></td></tr></tbody></table></code></pre></div></div>\n\n<h3 id=\"author-information\">Author Information</h3>\n\n<p>The author information of the post usually does not need to be filled in the <em>Front Matter</em> , they will be obtained from variables <code class=\"language-plaintext highlighter-rouge\">social.name</code> and the first entry of <code class=\"language-plaintext highlighter-rouge\">social.links</code> of the configuration file by default. But you can also override it as follows:</p>\n\n<p>Adding author information in <code class=\"language-plaintext highlighter-rouge\">_data/authors.yml</code> (If your website doesn‚Äôt have this file, don‚Äôt hesitate to create one).</p>\n\n<div file=\"_data/authors.yml\" class=\"language-yaml highlighter-rouge\"><div class=\"highlight\"><pre class=\"highlight\"><code><table class=\"rouge-table\"><tbody><tr><!-- <td class=\"rouge-gutter gl\"><pre class=\"lineno\">1\n2\n3\n4\n</pre></td> --><td class=\"rouge-code\"><pre><span class=\"na\">&lt;author_id&gt;</span><span class=\"pi\">:</span>\n  <span class=\"na\">name</span><span class=\"pi\">:</span> <span class=\"s\">&lt;full name&gt;</span>\n  <span class=\"na\">twitter</span><span class=\"pi\">:</span> <span class=\"s\">&lt;twitter_of_author&gt;</span>\n  <span class=\"na\">url</span><span class=\"pi\">:</span> <span class=\"s\">&lt;homepage_of_author&gt;</span>\n</pre></td></tr></tbody></table></code></pre></div></div>\n\n<p>And then use <code class=\"language-plaintext highlighter-rouge\">author</code> to specify a single entry or <code class=\"language-plaintext highlighter-rouge\">authors</code> to specify multiple entries:</p>\n\n<div class=\"language-yaml highlighter-rouge\"><div class=\"highlight\"><pre class=\"highlight\"><code><table class=\"rouge-table\"><tbody><tr><!-- <td class=\"rouge-gutter gl\"><pre class=\"lineno\">1\n2\n3\n4\n5\n</pre></td> --><td class=\"rouge-code\"><pre><span class=\"nn\">---</span>\n<span class=\"na\">author</span><span class=\"pi\">:</span> <span class=\"s\">&lt;author_id&gt;</span>                     <span class=\"c1\"># for single entry</span>\n<span class=\"c1\"># or</span>\n<span class=\"na\">authors</span><span class=\"pi\">:</span> <span class=\"pi\">[</span><span class=\"nv\">&lt;author1_id&gt;</span><span class=\"pi\">,</span> <span class=\"nv\">&lt;author2_id&gt;</span><span class=\"pi\">]</span>   <span class=\"c1\"># for multiple entries</span>\n<span class=\"nn\">---</span>\n</pre></td></tr></tbody></table></code></pre></div></div>\n\n<p>Having said that, the key <code class=\"language-plaintext highlighter-rouge\">author</code> can also identify multiple entries.</p>\n\n<blockquote class=\"prompt-info\">\n  <p>The benefit of reading the author information from the file <code class=\"language-plaintext filepath highlighter-rouge\">_data/authors.yml</code> is that the page will have the meta tag <code class=\"language-plaintext highlighter-rouge\">twitter:creator</code>, which enriches the <a href=\"https://developer.twitter.com/en/docs/twitter-for-websites/cards/guides/getting-started#card-and-content-attribution\">Twitter Cards</a> and is good for SEO.</p>\n</blockquote>\n\n<h3 id=\"post-description\">Post Description</h3>\n\n<p>By default, the first words of the post are used to display on the home page for a list of posts, in the <em>Further Reading</em> section, and in the XML of the RSS feed. If you don‚Äôt want to display the auto-generated description for the post, you can customize it using the <code class=\"language-plaintext highlighter-rouge\">description</code> field in the <em>Front Matter</em> as follows:</p>\n\n<div class=\"language-yaml highlighter-rouge\"><div class=\"highlight\"><pre class=\"highlight\"><code><table class=\"rouge-table\"><tbody><tr><!-- <td class=\"rouge-gutter gl\"><pre class=\"lineno\">1\n2\n3\n</pre></td> --><td class=\"rouge-code\"><pre><span class=\"nn\">---</span>\n<span class=\"na\">description</span><span class=\"pi\">:</span> <span class=\"s\">Short summary of the post.</span>\n<span class=\"nn\">---</span>\n</pre></td></tr></tbody></table></code></pre></div></div>\n\n<p>Additionally, the <code class=\"language-plaintext highlighter-rouge\">description</code> text will also be displayed under the post title on the post‚Äôs page.</p>\n\n<blockquote class=\"prompt-tip\">\n  <p>The <code class=\"language-plaintext highlighter-rouge\">jekyll-feed</code> plugin will automatically process with the front-matters <code class=\"language-plaintext highlighter-rouge\">date</code>, <code class=\"language-plaintext highlighter-rouge\">title</code>, <code class=\"language-plaintext highlighter-rouge\">excerpt</code>, <code class=\"language-plaintext highlighter-rouge\">id</code>, <code class=\"language-plaintext highlighter-rouge\">category</code>, <code class=\"language-plaintext highlighter-rouge\">tags</code>, <code class=\"language-plaintext highlighter-rouge\">image</code>, <code class=\"language-plaintext highlighter-rouge\">author</code>, and <code class=\"language-plaintext highlighter-rouge\">description</code>.</p>\n</blockquote>\n\n",
          "url": "/blog/write-a-new-post/#front-matter",
          
          "relUrl": "/blog/write-a-new-post/#front-matter"
        },
        "33": {
          "doc": "Writing a New Post",
          "title": "Table of Contents",
          "content": "\n\n<p>By default, the <strong>T</strong>able <strong>o</strong>f <strong>C</strong>ontents (TOC) is displayed on the right panel of the post. If you want to turn it off globally, go to <code class=\"language-plaintext filepath highlighter-rouge\">_config.yml</code> and set the value of variable <code class=\"language-plaintext highlighter-rouge\">toc</code> to <code class=\"language-plaintext highlighter-rouge\">false</code>. If you want to turn off TOC for a specific post, add the following to the post‚Äôs <a href=\"https://jekyllrb.com/docs/front-matter/\">Front Matter</a>:</p>\n\n<div class=\"language-yaml highlighter-rouge\"><div class=\"highlight\"><pre class=\"highlight\"><code><table class=\"rouge-table\"><tbody><tr><!-- <td class=\"rouge-gutter gl\"><pre class=\"lineno\">1\n2\n3\n</pre></td> --><td class=\"rouge-code\"><pre><span class=\"nn\">---</span>\n<span class=\"na\">toc</span><span class=\"pi\">:</span> <span class=\"kc\">false</span>\n<span class=\"nn\">---</span>\n</pre></td></tr></tbody></table></code></pre></div></div>\n\n",
          "url": "/blog/write-a-new-post/#table-of-contents",
          
          "relUrl": "/blog/write-a-new-post/#table-of-contents"
        },
        "34": {
          "doc": "Writing a New Post",
          "title": "Comments",
          "content": "\n\n<p>The global switch of comments is defined by variable <code class=\"language-plaintext highlighter-rouge\">comments.provider</code> in the file <code class=\"language-plaintext filepath highlighter-rouge\">_config.yml</code>. After selecting a comment system for this variable, comments will be turned on for all posts.</p>\n\n<p>If you want to close the comment for a specific post, add the following to the <strong>Front Matter</strong> of the post:</p>\n\n<div class=\"language-yaml highlighter-rouge\"><div class=\"highlight\"><pre class=\"highlight\"><code><table class=\"rouge-table\"><tbody><tr><!-- <td class=\"rouge-gutter gl\"><pre class=\"lineno\">1\n2\n3\n</pre></td> --><td class=\"rouge-code\"><pre><span class=\"nn\">---</span>\n<span class=\"na\">comments</span><span class=\"pi\">:</span> <span class=\"kc\">false</span>\n<span class=\"nn\">---</span>\n</pre></td></tr></tbody></table></code></pre></div></div>\n\n",
          "url": "/blog/write-a-new-post/#comments",
          
          "relUrl": "/blog/write-a-new-post/#comments"
        },
        "35": {
          "doc": "Writing a New Post",
          "title": "Mathematics",
          "content": "\n\n<p>For website performance reasons, the mathematical feature won‚Äôt be loaded by default. But it can be enabled by:</p>\n\n<div class=\"language-yaml highlighter-rouge\"><div class=\"highlight\"><pre class=\"highlight\"><code><table class=\"rouge-table\"><tbody><tr><!-- <td class=\"rouge-gutter gl\"><pre class=\"lineno\">1\n2\n3\n</pre></td> --><td class=\"rouge-code\"><pre><span class=\"nn\">---</span>\n<span class=\"na\">math</span><span class=\"pi\">:</span> <span class=\"kc\">true</span>\n<span class=\"nn\">---</span>\n</pre></td></tr></tbody></table></code></pre></div></div>\n\n<p>After enabling the mathematical feature, you can add math equations with the following syntax:</p>\n\n<ul>\n  <li><strong>Block math</strong> should be added with <code class=\"language-plaintext highlighter-rouge\">$$ math $$</code> with <strong>mandatory</strong> blank lines before and after <code class=\"language-plaintext highlighter-rouge\">$$</code>\n    <ul>\n      <li><strong>Inserting equation numbering</strong> should be added with <code class=\"language-plaintext highlighter-rouge\">$$\\begin{equation} math \\end{equation}$$</code></li>\n      <li><strong>Referencing equation numbering</strong> should be done with <code class=\"language-plaintext highlighter-rouge\">\\label{eq:label_name}</code> in the equation block and <code class=\"language-plaintext highlighter-rouge\">\\eqref{eq:label_name}</code> inline with text (see example below)</li>\n    </ul>\n  </li>\n  <li><strong>Inline math</strong> (in lines) should be added with <code class=\"language-plaintext highlighter-rouge\">$$ math $$</code> without any blank line before or after <code class=\"language-plaintext highlighter-rouge\">$$</code></li>\n  <li><strong>Inline math</strong> (in lists) should be added with <code class=\"language-plaintext highlighter-rouge\">\\$$ math $$</code></li>\n</ul>\n\n<div class=\"language-markdown highlighter-rouge\"><div class=\"highlight\"><pre class=\"highlight\"><code><table class=\"rouge-table\"><tbody><tr><!-- <td class=\"rouge-gutter gl\"><pre class=\"lineno\">1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\n21\n22\n23\n24\n25\n26\n</pre></td> --><td class=\"rouge-code\"><pre><span class=\"c\">&lt;!-- Block math, keep all blank lines --&gt;</span>\n\n$$\nLaTeX_math_expression\n$$\n\n<span class=\"c\">&lt;!-- Equation numbering, keep all blank lines  --&gt;</span>\n\n$$\n<span class=\"se\">\\b</span>egin{equation}\n  LaTeX_math_expression\n  <span class=\"se\">\\l</span>abel{eq:label_name}\n<span class=\"se\">\\e</span>nd{equation}\n$$\n\nCan be referenced as <span class=\"se\">\\e</span>qref{eq:label_name}.\n\n<span class=\"c\">&lt;!-- Inline math in lines, NO blank lines --&gt;</span>\n\n\"Lorem ipsum dolor sit amet, $$ LaTeX_math_expression $$ consectetur adipiscing elit.\"\n\n<span class=\"c\">&lt;!-- Inline math in lists, escape the first `$` --&gt;</span>\n<span class=\"p\">\n1.</span> <span class=\"se\">\\$</span>$ LaTeX_math_expression $$\n<span class=\"p\">2.</span> <span class=\"se\">\\$</span>$ LaTeX_math_expression $$\n<span class=\"p\">3.</span> <span class=\"se\">\\$</span>$ LaTeX_math_expression $$\n</pre></td></tr></tbody></table></code></pre></div></div>\n\n",
          "url": "/blog/write-a-new-post/#mathematics",
          
          "relUrl": "/blog/write-a-new-post/#mathematics"
        },
        "36": {
          "doc": "Writing a New Post",
          "title": "Mermaid",
          "content": "\n\n<p><a href=\"https://github.com/mermaid-js/mermaid\"><strong>Mermaid</strong></a> is a great diagram generation tool. To enable it on your post, add the following to the YAML block:</p>\n\n<div class=\"language-yaml highlighter-rouge\"><div class=\"highlight\"><pre class=\"highlight\"><code><table class=\"rouge-table\"><tbody><tr><!-- <td class=\"rouge-gutter gl\"><pre class=\"lineno\">1\n2\n3\n</pre></td> --><td class=\"rouge-code\"><pre><span class=\"nn\">---</span>\n<span class=\"na\">mermaid</span><span class=\"pi\">:</span> <span class=\"kc\">true</span>\n<span class=\"nn\">---</span>\n</pre></td></tr></tbody></table></code></pre></div></div>\n\n<p>Then you can use it like other markdown languages: surround the graph code with <code class=\"language-plaintext highlighter-rouge\">```mermaid</code> and <code class=\"language-plaintext highlighter-rouge\">```</code>.</p>\n\n",
          "url": "/blog/write-a-new-post/#mermaid",
          
          "relUrl": "/blog/write-a-new-post/#mermaid"
        },
        "37": {
          "doc": "Writing a New Post",
          "title": "Images",
          "content": "\n\n<h3 id=\"caption\">Caption</h3>\n\n<p>Add italics to the next line of an image, then it will become the caption and appear at the bottom of the image:</p>\n\n<div class=\"language-markdown nolineno highlighter-rouge\"><div class=\"highlight\"><pre class=\"highlight\"><code><table class=\"rouge-table\"><tbody><tr><!-- <td class=\"rouge-gutter gl\"><pre class=\"lineno\">1\n2\n</pre></td> --><td class=\"rouge-code\"><pre><span class=\"p\">![</span><span class=\"nv\">img-description</span><span class=\"p\">](</span><span class=\"sx\">/path/to/image</span><span class=\"p\">)</span>\n<span class=\"ge\">_Image Caption_</span>\n</pre></td></tr></tbody></table></code></pre></div></div>\n\n<h3 id=\"size\">Size</h3>\n\n<p>In order to prevent the page content layout from shifting when the image is loaded, we should set the width and height for each image.</p>\n\n<div class=\"language-markdown nolineno highlighter-rouge\"><div class=\"highlight\"><pre class=\"highlight\"><code><table class=\"rouge-table\"><tbody><tr><!-- <td class=\"rouge-gutter gl\"><pre class=\"lineno\">1\n</pre></td> --><td class=\"rouge-code\"><pre><span class=\"p\">![</span><span class=\"nv\">Desktop View</span><span class=\"p\">](</span><span class=\"sx\">/assets/img/sample/mockup.png</span><span class=\"p\">)</span>{: width=\"700\" height=\"400\" }\n</pre></td></tr></tbody></table></code></pre></div></div>\n\n<blockquote class=\"prompt-info\">\n  <p>For an SVG, you have to at least specify its <em>width</em>, otherwise it won‚Äôt be rendered.</p>\n</blockquote>\n\n<p>Starting from <em>Chirpy v5.0.0</em>, <code class=\"language-plaintext highlighter-rouge\">height</code> and <code class=\"language-plaintext highlighter-rouge\">width</code> support abbreviations (<code class=\"language-plaintext highlighter-rouge\">height</code> ‚Üí <code class=\"language-plaintext highlighter-rouge\">h</code>, <code class=\"language-plaintext highlighter-rouge\">width</code> ‚Üí <code class=\"language-plaintext highlighter-rouge\">w</code>). The following example has the same effect as the above:</p>\n\n<div class=\"language-markdown nolineno highlighter-rouge\"><div class=\"highlight\"><pre class=\"highlight\"><code><table class=\"rouge-table\"><tbody><tr><!-- <td class=\"rouge-gutter gl\"><pre class=\"lineno\">1\n</pre></td> --><td class=\"rouge-code\"><pre><span class=\"p\">![</span><span class=\"nv\">Desktop View</span><span class=\"p\">](</span><span class=\"sx\">/assets/img/sample/mockup.png</span><span class=\"p\">)</span>{: w=\"700\" h=\"400\" }\n</pre></td></tr></tbody></table></code></pre></div></div>\n\n<h3 id=\"position\">Position</h3>\n\n<p>By default, the image is centered, but you can specify the position by using one of the classes <code class=\"language-plaintext highlighter-rouge\">normal</code>, <code class=\"language-plaintext highlighter-rouge\">left</code>, and <code class=\"language-plaintext highlighter-rouge\">right</code>.</p>\n\n<blockquote class=\"prompt-warning\">\n  <p>Once the position is specified, the image caption should not be added.</p>\n</blockquote>\n\n<ul>\n  <li>\n    <p><strong>Normal position</strong></p>\n\n    <p>Image will be left aligned in below sample:</p>\n\n    <div class=\"language-markdown nolineno highlighter-rouge\"><div class=\"highlight\"><pre class=\"highlight\"><code><table class=\"rouge-table\"><tbody><tr><!-- <td class=\"rouge-gutter gl\"><pre class=\"lineno\">1\n</pre></td> --><td class=\"rouge-code\"><pre><span class=\"p\">![</span><span class=\"nv\">Desktop View</span><span class=\"p\">](</span><span class=\"sx\">/assets/img/sample/mockup.png</span><span class=\"p\">)</span>{: .normal }\n</pre></td></tr></tbody></table></code></pre></div>    </div>\n  </li>\n  <li>\n    <p><strong>Float to the left</strong></p>\n\n    <div class=\"language-markdown nolineno highlighter-rouge\"><div class=\"highlight\"><pre class=\"highlight\"><code><table class=\"rouge-table\"><tbody><tr><!-- <td class=\"rouge-gutter gl\"><pre class=\"lineno\">1\n</pre></td> --><td class=\"rouge-code\"><pre><span class=\"p\">![</span><span class=\"nv\">Desktop View</span><span class=\"p\">](</span><span class=\"sx\">/assets/img/sample/mockup.png</span><span class=\"p\">)</span>{: .left }\n</pre></td></tr></tbody></table></code></pre></div>    </div>\n  </li>\n  <li>\n    <p><strong>Float to the right</strong></p>\n\n    <div class=\"language-markdown nolineno highlighter-rouge\"><div class=\"highlight\"><pre class=\"highlight\"><code><table class=\"rouge-table\"><tbody><tr><!-- <td class=\"rouge-gutter gl\"><pre class=\"lineno\">1\n</pre></td> --><td class=\"rouge-code\"><pre><span class=\"p\">![</span><span class=\"nv\">Desktop View</span><span class=\"p\">](</span><span class=\"sx\">/assets/img/sample/mockup.png</span><span class=\"p\">)</span>{: .right }\n</pre></td></tr></tbody></table></code></pre></div>    </div>\n  </li>\n</ul>\n\n<h3 id=\"darklight-mode\">Dark/Light mode</h3>\n\n<p>You can make images follow theme preferences in dark/light mode. This requires you to prepare two images, one for dark mode and one for light mode, and then assign them a specific class (<code class=\"language-plaintext highlighter-rouge\">dark</code> or <code class=\"language-plaintext highlighter-rouge\">light</code>):</p>\n\n<div class=\"language-markdown highlighter-rouge\"><div class=\"highlight\"><pre class=\"highlight\"><code><table class=\"rouge-table\"><tbody><tr><!-- <td class=\"rouge-gutter gl\"><pre class=\"lineno\">1\n2\n</pre></td> --><td class=\"rouge-code\"><pre><span class=\"p\">![</span><span class=\"nv\">Light mode only</span><span class=\"p\">](</span><span class=\"sx\">/path/to/light-mode.png</span><span class=\"p\">)</span>{: .light }\n<span class=\"p\">![</span><span class=\"nv\">Dark mode only</span><span class=\"p\">](</span><span class=\"sx\">/path/to/dark-mode.png</span><span class=\"p\">)</span>{: .dark }\n</pre></td></tr></tbody></table></code></pre></div></div>\n\n<h3 id=\"shadow\">Shadow</h3>\n\n<p>The screenshots of the program window can be considered to show the shadow effect:</p>\n\n<div class=\"language-markdown nolineno highlighter-rouge\"><div class=\"highlight\"><pre class=\"highlight\"><code><table class=\"rouge-table\"><tbody><tr><!-- <td class=\"rouge-gutter gl\"><pre class=\"lineno\">1\n</pre></td> --><td class=\"rouge-code\"><pre><span class=\"p\">![</span><span class=\"nv\">Desktop View</span><span class=\"p\">](</span><span class=\"sx\">/assets/img/sample/mockup.png</span><span class=\"p\">)</span>{: .shadow }\n</pre></td></tr></tbody></table></code></pre></div></div>\n\n<h3 id=\"image-path\">Image Path</h3>\n\n<p>When a post contains many images, it will be a time-consuming task to repeatedly define the path of the images. To solve this, we can define this path in the YAML block of the post:</p>\n\n<div class=\"language-yml highlighter-rouge\"><div class=\"highlight\"><pre class=\"highlight\"><code><table class=\"rouge-table\"><tbody><tr><!-- <td class=\"rouge-gutter gl\"><pre class=\"lineno\">1\n2\n3\n</pre></td> --><td class=\"rouge-code\"><pre><span class=\"nn\">---</span>\n<span class=\"na\">img_path</span><span class=\"pi\">:</span> <span class=\"s\">/img/path/</span>\n<span class=\"nn\">---</span>\n</pre></td></tr></tbody></table></code></pre></div></div>\n\n<p>And then, the image source of Markdown can write the file name directly:</p>\n\n<div class=\"language-md nolineno highlighter-rouge\"><div class=\"highlight\"><pre class=\"highlight\"><code><table class=\"rouge-table\"><tbody><tr><!-- <td class=\"rouge-gutter gl\"><pre class=\"lineno\">1\n</pre></td> --><td class=\"rouge-code\"><pre><span class=\"p\">![</span><span class=\"nv\">The flower</span><span class=\"p\">](</span><span class=\"sx\">flower.png</span><span class=\"p\">)</span>\n</pre></td></tr></tbody></table></code></pre></div></div>\n\n<p>The output will be:</p>\n\n<div class=\"language-html nolineno highlighter-rouge\"><div class=\"highlight\"><pre class=\"highlight\"><code><table class=\"rouge-table\"><tbody><tr><!-- <td class=\"rouge-gutter gl\"><pre class=\"lineno\">1\n</pre></td> --><td class=\"rouge-code\"><pre><span class=\"nt\">&lt;img</span> <span class=\"na\">src=</span><span class=\"s\">\"/img/path/flower.png\"</span> <span class=\"na\">alt=</span><span class=\"s\">\"The flower\"</span> <span class=\"nt\">/&gt;</span>\n</pre></td></tr></tbody></table></code></pre></div></div>\n\n<h3 id=\"preview-image\">Preview Image</h3>\n\n<p>If you want to add an image at the top of the post, please provide an image with a resolution of <code class=\"language-plaintext highlighter-rouge\">1200 x 630</code>. Please note that if the image aspect ratio does not meet <code class=\"language-plaintext highlighter-rouge\">1.91 : 1</code>, the image will be scaled and cropped.</p>\n\n<p>Knowing these prerequisites, you can start setting the image‚Äôs attribute:</p>\n\n<div class=\"language-yaml highlighter-rouge\"><div class=\"highlight\"><pre class=\"highlight\"><code><table class=\"rouge-table\"><tbody><tr><!-- <td class=\"rouge-gutter gl\"><pre class=\"lineno\">1\n2\n3\n4\n5\n</pre></td> --><td class=\"rouge-code\"><pre><span class=\"nn\">---</span>\n<span class=\"na\">image</span><span class=\"pi\">:</span>\n  <span class=\"na\">path</span><span class=\"pi\">:</span> <span class=\"s\">/path/to/image</span>\n  <span class=\"na\">alt</span><span class=\"pi\">:</span> <span class=\"s\">image alternative text</span>\n<span class=\"nn\">---</span>\n</pre></td></tr></tbody></table></code></pre></div></div>\n\n<p>Note that the <a href=\"#image-path\"><code class=\"language-plaintext highlighter-rouge\">img_path</code></a> can also be passed to the preview image, that is, when it has been set, the attribute <code class=\"language-plaintext highlighter-rouge\">path</code> only needs the image file name.</p>\n\n<p>For simple use, you can also just use <code class=\"language-plaintext highlighter-rouge\">image</code> to define the path.</p>\n\n<div class=\"language-yml highlighter-rouge\"><div class=\"highlight\"><pre class=\"highlight\"><code><table class=\"rouge-table\"><tbody><tr><!-- <td class=\"rouge-gutter gl\"><pre class=\"lineno\">1\n2\n3\n</pre></td> --><td class=\"rouge-code\"><pre><span class=\"nn\">---</span>\n<span class=\"na\">image</span><span class=\"pi\">:</span> <span class=\"s\">/path/to/image</span>\n<span class=\"nn\">---</span>\n</pre></td></tr></tbody></table></code></pre></div></div>\n\n<h3 id=\"lqip\">LQIP</h3>\n\n<p>For preview images:</p>\n\n<div class=\"language-yaml highlighter-rouge\"><div class=\"highlight\"><pre class=\"highlight\"><code><table class=\"rouge-table\"><tbody><tr><!-- <td class=\"rouge-gutter gl\"><pre class=\"lineno\">1\n2\n3\n4\n</pre></td> --><td class=\"rouge-code\"><pre><span class=\"nn\">---</span>\n<span class=\"na\">image</span><span class=\"pi\">:</span>\n  <span class=\"na\">lqip</span><span class=\"pi\">:</span> <span class=\"s\">/path/to/lqip-file</span> <span class=\"c1\"># or base64 URI</span>\n<span class=\"nn\">---</span>\n</pre></td></tr></tbody></table></code></pre></div></div>\n\n<blockquote>\n  <p>You can observe LQIP in the preview image of post <a href=\"https://chirpy-img.netlify.app/posts/text-and-typography/\"><em>Text and Typography</em></a>.</p>\n</blockquote>\n\n<p>For normal images:</p>\n\n<div class=\"language-markdown nolineno highlighter-rouge\"><div class=\"highlight\"><pre class=\"highlight\"><code><table class=\"rouge-table\"><tbody><tr><!-- <td class=\"rouge-gutter gl\"><pre class=\"lineno\">1\n</pre></td> --><td class=\"rouge-code\"><pre><span class=\"p\">![</span><span class=\"nv\">Image description</span><span class=\"p\">](</span><span class=\"sx\">/path/to/image</span><span class=\"p\">)</span>{: lqip=\"/path/to/lqip-file\" }\n</pre></td></tr></tbody></table></code></pre></div></div>\n\n",
          "url": "/blog/write-a-new-post/#images",
          
          "relUrl": "/blog/write-a-new-post/#images"
        },
        "38": {
          "doc": "Writing a New Post",
          "title": "Pinned Posts",
          "content": "\n\n<p>You can pin one or more posts to the top of the home page, and the fixed posts are sorted in reverse order according to their release date. Enable by:</p>\n\n<div class=\"language-yaml highlighter-rouge\"><div class=\"highlight\"><pre class=\"highlight\"><code><table class=\"rouge-table\"><tbody><tr><!-- <td class=\"rouge-gutter gl\"><pre class=\"lineno\">1\n2\n3\n</pre></td> --><td class=\"rouge-code\"><pre><span class=\"nn\">---</span>\n<span class=\"na\">pin</span><span class=\"pi\">:</span> <span class=\"kc\">true</span>\n<span class=\"nn\">---</span>\n</pre></td></tr></tbody></table></code></pre></div></div>\n\n",
          "url": "/blog/write-a-new-post/#pinned-posts",
          
          "relUrl": "/blog/write-a-new-post/#pinned-posts"
        },
        "39": {
          "doc": "Writing a New Post",
          "title": "Prompts",
          "content": "\n\n<p>There are several types of prompts: <code class=\"language-plaintext highlighter-rouge\">tip</code>, <code class=\"language-plaintext highlighter-rouge\">info</code>, <code class=\"language-plaintext highlighter-rouge\">warning</code>, and <code class=\"language-plaintext highlighter-rouge\">danger</code>. They can be generated by adding the class <code class=\"language-plaintext highlighter-rouge\">prompt-{type}</code> to the blockquote. For example, define a prompt of type <code class=\"language-plaintext highlighter-rouge\">info</code> as follows:</p>\n\n<div class=\"language-md nolineno highlighter-rouge\"><div class=\"highlight\"><pre class=\"highlight\"><code><table class=\"rouge-table\"><tbody><tr><!-- <td class=\"rouge-gutter gl\"><pre class=\"lineno\">1\n2\n</pre></td> --><td class=\"rouge-code\"><pre><span class=\"gt\">&gt; Example line for prompt.</span>\n{: .prompt-info }\n</pre></td></tr></tbody></table></code></pre></div></div>\n\n",
          "url": "/blog/write-a-new-post/#prompts",
          
          "relUrl": "/blog/write-a-new-post/#prompts"
        },
        "40": {
          "doc": "Writing a New Post",
          "title": "Syntax",
          "content": "\n\n<h3 id=\"inline-code\">Inline Code</h3>\n\n<div class=\"language-md nolineno highlighter-rouge\"><div class=\"highlight\"><pre class=\"highlight\"><code><table class=\"rouge-table\"><tbody><tr><!-- <td class=\"rouge-gutter gl\"><pre class=\"lineno\">1\n</pre></td> --><td class=\"rouge-code\"><pre><span class=\"sb\">`inline code part`</span>\n</pre></td></tr></tbody></table></code></pre></div></div>\n\n<h3 id=\"filepath-hightlight\">Filepath Hightlight</h3>\n\n<div class=\"language-md nolineno highlighter-rouge\"><div class=\"highlight\"><pre class=\"highlight\"><code><table class=\"rouge-table\"><tbody><tr><!-- <td class=\"rouge-gutter gl\"><pre class=\"lineno\">1\n</pre></td> --><td class=\"rouge-code\"><pre><span class=\"sb\">`/path/to/a/file.extend`</span>{: .filepath}\n</pre></td></tr></tbody></table></code></pre></div></div>\n\n<h3 id=\"code-block\">Code Block</h3>\n\n<p>Markdown symbols <code class=\"language-plaintext highlighter-rouge\">```</code> can easily create a code block as follows:</p>\n\n<div class=\"language-md highlighter-rouge\"><div class=\"highlight\"><pre class=\"highlight\"><code><table class=\"rouge-table\"><tbody><tr><!-- <td class=\"rouge-gutter gl\"><pre class=\"lineno\">1\n2\n3\n</pre></td> --><td class=\"rouge-code\"><pre><span class=\"p\">```</span><span class=\"nl\">\n</span>This is a plaintext code snippet.\n<span class=\"p\">```</span>\n</pre></td></tr></tbody></table></code></pre></div></div>\n\n<h4 id=\"specifying-language\">Specifying Language</h4>\n\n<p>Using <code class=\"language-plaintext highlighter-rouge\">```{language}</code> you will get a code block with syntax highlight:</p>\n\n<div class=\"language-markdown highlighter-rouge\"><div class=\"highlight\"><pre class=\"highlight\"><code><table class=\"rouge-table\"><tbody><tr><!-- <td class=\"rouge-gutter gl\"><pre class=\"lineno\">1\n2\n3\n</pre></td> --><td class=\"rouge-code\"><pre><span class=\"p\">```</span><span class=\"nl\">yaml\n</span><span class=\"na\">key</span><span class=\"pi\">:</span> <span class=\"s\">value</span>\n<span class=\"p\">```</span>\n</pre></td></tr></tbody></table></code></pre></div></div>\n\n<blockquote class=\"prompt-danger\">\n  <p>The Jekyll tag <code class=\"language-plaintext highlighter-rouge\">{% highlight %}</code> is not compatible with this theme.</p>\n</blockquote>\n\n<h4 id=\"line-number\">Line Number</h4>\n\n<p>By default, all languages except <code class=\"language-plaintext highlighter-rouge\">plaintext</code>, <code class=\"language-plaintext highlighter-rouge\">console</code>, and <code class=\"language-plaintext highlighter-rouge\">terminal</code> will display line numbers. When you want to hide the line number of a code block, add the class <code class=\"language-plaintext highlighter-rouge\">nolineno</code> to it:</p>\n\n<div class=\"language-markdown highlighter-rouge\"><div class=\"highlight\"><pre class=\"highlight\"><code><table class=\"rouge-table\"><tbody><tr><!-- <td class=\"rouge-gutter gl\"><pre class=\"lineno\">1\n2\n3\n4\n</pre></td> --><td class=\"rouge-code\"><pre><span class=\"p\">```</span><span class=\"nl\">shell\n</span><span class=\"nb\">echo</span> <span class=\"s1\">'No more line numbers!'</span>\n<span class=\"p\">```</span>\n{: .nolineno }\n</pre></td></tr></tbody></table></code></pre></div></div>\n\n<h4 id=\"specifying-the-filename\">Specifying the Filename</h4>\n\n<p>You may have noticed that the code language will be displayed at the top of the code block. If you want to replace it with the file name, you can add the attribute <code class=\"language-plaintext highlighter-rouge\">file</code> to achieve this:</p>\n\n<div class=\"language-markdown highlighter-rouge\"><div class=\"highlight\"><pre class=\"highlight\"><code><table class=\"rouge-table\"><tbody><tr><!-- <td class=\"rouge-gutter gl\"><pre class=\"lineno\">1\n2\n3\n4\n</pre></td> --><td class=\"rouge-code\"><pre><span class=\"p\">```</span><span class=\"nl\">shell\n</span><span class=\"c\"># content</span>\n<span class=\"p\">```</span>\n{: file=\"path/to/file\" }\n</pre></td></tr></tbody></table></code></pre></div></div>\n\n<h4 id=\"liquid-codes\">Liquid Codes</h4>\n\n<p>If you want to display the <strong>Liquid</strong> snippet, surround the liquid code with <code class=\"language-plaintext highlighter-rouge\">{% raw %}</code> and <code class=\"language-plaintext highlighter-rouge\">{% endraw %}</code>:</p>\n\n<div class=\"language-markdown highlighter-rouge\"><div class=\"highlight\"><pre class=\"highlight\"><code><table class=\"rouge-table\"><tbody><tr><!-- <td class=\"rouge-gutter gl\"><pre class=\"lineno\">1\n2\n3\n4\n5\n6\n7\n</pre></td> --><td class=\"rouge-code\"><pre>{% raw %}\n<span class=\"p\">```</span><span class=\"nl\">liquid\n</span><span class=\"cp\">{%</span><span class=\"w\"> </span><span class=\"nt\">if</span><span class=\"w\"> </span><span class=\"nv\">product</span><span class=\"p\">.</span><span class=\"nv\">title</span><span class=\"w\"> </span><span class=\"ow\">contains</span><span class=\"w\"> </span><span class=\"s1\">'Pack'</span><span class=\"w\"> </span><span class=\"cp\">%}</span>\n  This product's title contains the word Pack.\n<span class=\"cp\">{%</span><span class=\"w\"> </span><span class=\"nt\">endif</span><span class=\"w\"> </span><span class=\"cp\">%}</span>\n<span class=\"p\">```</span>\n{% endraw %}\n</pre></td></tr></tbody></table></code></pre></div></div>\n\n<p>Or adding <code class=\"language-plaintext highlighter-rouge\">render_with_liquid: false</code> (Requires Jekyll 4.0 or higher) to the post‚Äôs YAML block.</p>\n\n",
          "url": "/blog/write-a-new-post/#syntax",
          
          "relUrl": "/blog/write-a-new-post/#syntax"
        },
        "41": {
          "doc": "Writing a New Post",
          "title": "Videos",
          "content": "\n\n<h3 id=\"video-sharing-platform\">Video Sharing Platform</h3>\n\n<p>You can embed a video with the following syntax:</p>\n\n<div class=\"language-liquid highlighter-rouge\"><div class=\"highlight\"><pre class=\"highlight\"><code><table class=\"rouge-table\"><tbody><tr><!-- <td class=\"rouge-gutter gl\"><pre class=\"lineno\">1\n</pre></td> --><td class=\"rouge-code\"><pre><span class=\"cp\">{%</span><span class=\"w\"> </span><span class=\"nt\">include</span><span class=\"w\"> </span>embed/{Platform}.html<span class=\"w\"> </span><span class=\"na\">id</span><span class=\"o\">=</span><span class=\"s1\">'{ID}'</span><span class=\"w\"> </span><span class=\"cp\">%}</span>\n</pre></td></tr></tbody></table></code></pre></div></div>\n\n<p>Where <code class=\"language-plaintext highlighter-rouge\">Platform</code> is the lowercase of the platform name, and <code class=\"language-plaintext highlighter-rouge\">ID</code> is the video ID.</p>\n\n<p>The following table shows how to get the two parameters we need in a given video URL, and you can also know the currently supported video platforms.</p>\n\n<table>\n  <thead>\n    <tr>\n      <th>Video URL</th>\n      <th>Platform</th>\n      <th style=\"text-align: left\">ID</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <td><a href=\"https://www.youtube.com/watch?v=H-B46URT4mg\">https://www.<strong>youtube</strong>.com/watch?v=<strong>H-B46URT4mg</strong></a></td>\n      <td><code class=\"language-plaintext highlighter-rouge\">youtube</code></td>\n      <td style=\"text-align: left\"><code class=\"language-plaintext highlighter-rouge\">H-B46URT4mg</code></td>\n    </tr>\n    <tr>\n      <td><a href=\"https://www.twitch.tv/videos/1634779211\">https://www.<strong>twitch</strong>.tv/videos/<strong>1634779211</strong></a></td>\n      <td><code class=\"language-plaintext highlighter-rouge\">twitch</code></td>\n      <td style=\"text-align: left\"><code class=\"language-plaintext highlighter-rouge\">1634779211</code></td>\n    </tr>\n    <tr>\n      <td><a href=\"https://www.bilibili.com/video/BV1Q44y1B7Wf\">https://www.<strong>bilibili</strong>.com/video/<strong>BV1Q44y1B7Wf</strong></a></td>\n      <td><code class=\"language-plaintext highlighter-rouge\">bilibili</code></td>\n      <td style=\"text-align: left\"><code class=\"language-plaintext highlighter-rouge\">BV1Q44y1B7Wf</code></td>\n    </tr>\n  </tbody>\n</table>\n\n<h3 id=\"video-file\">Video File</h3>\n\n<p>If you want to embed a video file directly, use the following syntax:</p>\n\n<div class=\"language-liquid highlighter-rouge\"><div class=\"highlight\"><pre class=\"highlight\"><code><table class=\"rouge-table\"><tbody><tr><!-- <td class=\"rouge-gutter gl\"><pre class=\"lineno\">1\n</pre></td> --><td class=\"rouge-code\"><pre><span class=\"cp\">{%</span><span class=\"w\"> </span><span class=\"nt\">include</span><span class=\"w\"> </span>embed/video.html<span class=\"w\"> </span><span class=\"na\">src</span><span class=\"o\">=</span><span class=\"s1\">'{URL}'</span><span class=\"w\"> </span><span class=\"cp\">%}</span>\n</pre></td></tr></tbody></table></code></pre></div></div>\n\n<p>Where <code class=\"language-plaintext highlighter-rouge\">URL</code> is an URL to a video file e.g. <code class=\"language-plaintext highlighter-rouge\">/assets/img/sample/video.mp4</code>.</p>\n\n<p>You can also specify additional attributes for the embedded video file. Here is a full list of attributes allowed.</p>\n\n<ul>\n  <li><code class=\"language-plaintext highlighter-rouge\">poster='/path/to/poster.png'</code> - poster image for a video that is shown while video is downloading</li>\n  <li><code class=\"language-plaintext highlighter-rouge\">title='Text'</code> - title for a video that appears below the video and looks same as for images</li>\n  <li><code class=\"language-plaintext highlighter-rouge\">autoplay=true</code> - video automatically begins to play back as soon as it can</li>\n  <li><code class=\"language-plaintext highlighter-rouge\">loop=true</code> - automatically seek back to the start upon reaching the end of the video</li>\n  <li><code class=\"language-plaintext highlighter-rouge\">muted=true</code> - audio will be initially silenced</li>\n</ul>\n\n<p>Consider an example utilizing all of the above:</p>\n\n<div class=\"language-liquid highlighter-rouge\"><div class=\"highlight\"><pre class=\"highlight\"><code><table class=\"rouge-table\"><tbody><tr><!-- <td class=\"rouge-gutter gl\"><pre class=\"lineno\">1\n2\n</pre></td> --><td class=\"rouge-code\"><pre><span class=\"cp\">{%</span><span class=\"w\"> </span><span class=\"nt\">include</span><span class=\"w\"> </span>embed/video.html<span class=\"w\"> </span><span class=\"na\">src</span><span class=\"o\">=</span><span class=\"s1\">'video.mp4'</span><span class=\"w\"> </span><span class=\"na\">poster</span><span class=\"o\">=</span><span class=\"s1\">'poster.png'</span><span class=\"w\"> </span><span class=\"na\">title</span><span class=\"o\">=</span><span class=\"s1\">'Demo video'</span><span class=\"w\">\n   </span><span class=\"na\">autoplay</span><span class=\"o\">=</span><span class=\"kc\">true</span><span class=\"w\"> </span><span class=\"na\">loop</span><span class=\"o\">=</span><span class=\"kc\">true</span><span class=\"w\"> </span><span class=\"na\">muted</span><span class=\"o\">=</span><span class=\"kc\">true</span><span class=\"w\"> </span><span class=\"cp\">%}</span>\n</pre></td></tr></tbody></table></code></pre></div></div>\n\n",
          "url": "/blog/write-a-new-post/#videos",
          
          "relUrl": "/blog/write-a-new-post/#videos"
        },
        "42": {
          "doc": "Writing a New Post",
          "title": "Learn More",
          "content": "\n\n<p>For more knowledge about Jekyll posts, visit the <a href=\"https://jekyllrb.com/docs/posts/\">Jekyll Docs: Posts</a>.</p>\n",
          "url": "/blog/write-a-new-post/#learn-more",
          
          "relUrl": "/blog/write-a-new-post/#learn-more"
        },
        "43": {
          "doc": "Writing a New Post",
          "title": "Writing a New Post",
          "content": "<p>This tutorial will guide you how to write a post in the <em>Chirpy</em> template, and it‚Äôs worth reading even if you‚Äôve used Jekyll before, as many features require specific variables to be set.</p>\n\n",
          "url": "/blog/write-a-new-post/",
          
          "relUrl": "/blog/write-a-new-post/"
        }
      ,
        "44": {
          "doc": "Text and Typography",
          "title": "Headings",
          "content": "\n\n<!-- markdownlint-disable MD022 MD025 -->\n",
          "url": "/blog/text-and-typography/#headings",
          
          "relUrl": "/blog/text-and-typography/#headings"
        },
        "45": {
          "doc": "Text and Typography",
          "title": "H1 - heading",
          "content": "\n\n",
          "url": "/blog/text-and-typography/#h1---heading",
          
          "relUrl": "/blog/text-and-typography/#h1---heading"
        },
        "46": {
          "doc": "Text and Typography",
          "title": "H2 - heading",
          "content": "\n\n<h3 data-toc-skip=\"\" class=\"mt-4 mb-0\" id=\"h3---heading\">H3 - heading</h3>\n\n<h4 data-toc-skip=\"\" class=\"mt-4\" id=\"h4---heading\">H4 - heading</h4>\n<!-- markdownlint-enable MD022 MD025 -->\n\n",
          "url": "/blog/text-and-typography/#h2---heading",
          
          "relUrl": "/blog/text-and-typography/#h2---heading"
        },
        "47": {
          "doc": "Text and Typography",
          "title": "Paragraph",
          "content": "\n\n<p>Quisque egestas convallis ipsum, ut sollicitudin risus tincidunt a. Maecenas interdum malesuada egestas. Duis consectetur porta risus, sit amet vulputate urna facilisis ac. Phasellus semper dui non purus ultrices sodales. Aliquam ante lorem, ornare a feugiat ac, finibus nec mauris. Vivamus ut tristique nisi. Sed vel leo vulputate, efficitur risus non, posuere mi. Nullam tincidunt bibendum rutrum. Proin commodo ornare sapien. Vivamus interdum diam sed sapien blandit, sit amet aliquam risus mattis. Nullam arcu turpis, mollis quis laoreet at, placerat id nibh. Suspendisse venenatis eros eros.</p>\n\n",
          "url": "/blog/text-and-typography/#paragraph",
          
          "relUrl": "/blog/text-and-typography/#paragraph"
        },
        "48": {
          "doc": "Text and Typography",
          "title": "Lists",
          "content": "\n\n<h3 id=\"ordered-list\">Ordered list</h3>\n\n<ol>\n  <li>Firstly</li>\n  <li>Secondly</li>\n  <li>Thirdly</li>\n</ol>\n\n<h3 id=\"unordered-list\">Unordered list</h3>\n\n<ul>\n  <li>Chapter\n    <ul>\n      <li>Section\n        <ul>\n          <li>Paragraph</li>\n        </ul>\n      </li>\n    </ul>\n  </li>\n</ul>\n\n<h3 id=\"todo-list\">ToDo list</h3>\n\n<ul class=\"task-list\">\n  <li class=\"task-list-item\"><input type=\"checkbox\" class=\"task-list-item-checkbox\" disabled=\"disabled\" />Job\n    <ul class=\"task-list\">\n      <li class=\"task-list-item\"><input type=\"checkbox\" class=\"task-list-item-checkbox\" disabled=\"disabled\" checked=\"checked\" />Step 1</li>\n      <li class=\"task-list-item\"><input type=\"checkbox\" class=\"task-list-item-checkbox\" disabled=\"disabled\" checked=\"checked\" />Step 2</li>\n      <li class=\"task-list-item\"><input type=\"checkbox\" class=\"task-list-item-checkbox\" disabled=\"disabled\" />Step 3</li>\n    </ul>\n  </li>\n</ul>\n\n<h3 id=\"description-list\">Description list</h3>\n\n<dl>\n  <dt>Sun</dt>\n  <dd>the star around which the earth orbits</dd>\n  <dt>Moon</dt>\n  <dd>the natural satellite of the earth, visible by reflected light from the sun</dd>\n</dl>\n\n",
          "url": "/blog/text-and-typography/#lists",
          
          "relUrl": "/blog/text-and-typography/#lists"
        },
        "49": {
          "doc": "Text and Typography",
          "title": "Block Quote",
          "content": "\n\n<blockquote>\n  <p>This line shows the <em>block quote</em>.</p>\n</blockquote>\n\n",
          "url": "/blog/text-and-typography/#block-quote",
          
          "relUrl": "/blog/text-and-typography/#block-quote"
        },
        "50": {
          "doc": "Text and Typography",
          "title": "Prompts",
          "content": "\n\n<blockquote class=\"prompt-tip\">\n  <p>An example showing the <code class=\"language-plaintext highlighter-rouge\">tip</code> type prompt.</p>\n</blockquote>\n\n<blockquote class=\"prompt-info\">\n  <p>An example showing the <code class=\"language-plaintext highlighter-rouge\">info</code> type prompt.</p>\n</blockquote>\n\n<blockquote class=\"prompt-warning\">\n  <p>An example showing the <code class=\"language-plaintext highlighter-rouge\">warning</code> type prompt.</p>\n</blockquote>\n\n<blockquote class=\"prompt-danger\">\n  <p>An example showing the <code class=\"language-plaintext highlighter-rouge\">danger</code> type prompt.</p>\n</blockquote>\n\n",
          "url": "/blog/text-and-typography/#prompts",
          
          "relUrl": "/blog/text-and-typography/#prompts"
        },
        "51": {
          "doc": "Text and Typography",
          "title": "Tables",
          "content": "\n\n<table>\n  <thead>\n    <tr>\n      <th style=\"text-align: left\">Company</th>\n      <th style=\"text-align: left\">Contact</th>\n      <th style=\"text-align: right\">Country</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <td style=\"text-align: left\">Alfreds Futterkiste</td>\n      <td style=\"text-align: left\">Maria Anders</td>\n      <td style=\"text-align: right\">Germany</td>\n    </tr>\n    <tr>\n      <td style=\"text-align: left\">Island Trading</td>\n      <td style=\"text-align: left\">Helen Bennett</td>\n      <td style=\"text-align: right\">UK</td>\n    </tr>\n    <tr>\n      <td style=\"text-align: left\">Magazzini Alimentari Riuniti</td>\n      <td style=\"text-align: left\">Giovanni Rovelli</td>\n      <td style=\"text-align: right\">Italy</td>\n    </tr>\n  </tbody>\n</table>\n\n",
          "url": "/blog/text-and-typography/#tables",
          
          "relUrl": "/blog/text-and-typography/#tables"
        },
        "52": {
          "doc": "Text and Typography",
          "title": "Links",
          "content": "\n\n<p><a href=\"http://127.0.0.1:4000\">http://127.0.0.1:4000</a></p>\n\n",
          "url": "/blog/text-and-typography/#links",
          
          "relUrl": "/blog/text-and-typography/#links"
        },
        "53": {
          "doc": "Text and Typography",
          "title": "Footnote",
          "content": "\n\n<p>Click the hook will locate the footnote<sup id=\"fnref:footnote\" role=\"doc-noteref\"><a href=\"#fn:footnote\" class=\"footnote\" rel=\"footnote\">1</a></sup>, and here is another footnote<sup id=\"fnref:fn-nth-2\" role=\"doc-noteref\"><a href=\"#fn:fn-nth-2\" class=\"footnote\" rel=\"footnote\">2</a></sup>.</p>\n\n",
          "url": "/blog/text-and-typography/#footnote",
          
          "relUrl": "/blog/text-and-typography/#footnote"
        },
        "54": {
          "doc": "Text and Typography",
          "title": "Inline code",
          "content": "\n\n<p>This is an example of <code class=\"language-plaintext highlighter-rouge\">Inline Code</code>.</p>\n\n",
          "url": "/blog/text-and-typography/#inline-code",
          
          "relUrl": "/blog/text-and-typography/#inline-code"
        },
        "55": {
          "doc": "Text and Typography",
          "title": "Filepath",
          "content": "\n\n<p>Here is the <code class=\"language-plaintext filepath highlighter-rouge\">/path/to/the/file.extend</code>.</p>\n\n",
          "url": "/blog/text-and-typography/#filepath",
          
          "relUrl": "/blog/text-and-typography/#filepath"
        },
        "56": {
          "doc": "Text and Typography",
          "title": "Code blocks",
          "content": "\n\n<h3 id=\"common\">Common</h3>\n\n<div class=\"language-plaintext highlighter-rouge\"><div class=\"highlight\"><pre class=\"highlight\"><code><table class=\"rouge-table\"><tbody><tr><!-- <td class=\"rouge-gutter gl\"><pre class=\"lineno\">1\n</pre></td> --><td class=\"rouge-code\"><pre>This is a common code snippet, without syntax highlight and line number.\n</pre></td></tr></tbody></table></code></pre></div></div>\n\n<h3 id=\"specific-language\">Specific Language</h3>\n\n<div class=\"language-bash highlighter-rouge\"><div class=\"highlight\"><pre class=\"highlight\"><code><table class=\"rouge-table\"><tbody><tr><!-- <td class=\"rouge-gutter gl\"><pre class=\"lineno\">1\n2\n3\n4\n</pre></td> --><td class=\"rouge-code\"><pre><span class=\"k\">if</span> <span class=\"o\">[</span> <span class=\"nv\">$?</span> <span class=\"nt\">-ne</span> 0 <span class=\"o\">]</span><span class=\"p\">;</span> <span class=\"k\">then\n  </span><span class=\"nb\">echo</span> <span class=\"s2\">\"The command was not successful.\"</span><span class=\"p\">;</span>\n  <span class=\"c\">#do the needful / exit</span>\n<span class=\"k\">fi</span><span class=\"p\">;</span>\n</pre></td></tr></tbody></table></code></pre></div></div>\n\n<h3 id=\"specific-filename\">Specific filename</h3>\n\n<div file=\"_sass/jekyll-theme-chirpy.scss\" class=\"language-sass highlighter-rouge\"><div class=\"highlight\"><pre class=\"highlight\"><code><table class=\"rouge-table\"><tbody><tr><!-- <td class=\"rouge-gutter gl\"><pre class=\"lineno\">1\n2\n3\n</pre></td> --><td class=\"rouge-code\"><pre><span class=\"k\">@import</span>\n  <span class=\"s2\">\"colors/light-typography\"</span><span class=\"o\">,</span>\n  <span class=\"s2\">\"colors/dark-typography\"</span><span class=\"o\">;</span>\n</pre></td></tr></tbody></table></code></pre></div></div>\n\n<h3 id=\"jupyter-notebook\">Jupyter notebook</h3>\n\n<div\n  class=\"jupyter-notebook\"\n  style=\"position: relative; width: 100%; margin: 0 auto;\">\n  <div class=\"jupyter-notebook-iframe-container\">\n    <iframe\n      src=\"/assets/jupyter/blog.ipynb.html\"\n      style=\"position: absolute; top: 0; left: 0; border-style: none;\"\n      width=\"100%\"\n      height=\"100%\"\n      onload=\"this.parentElement.style.paddingBottom = (this.contentWindow.document.documentElement.scrollHeight + 10) + 'px'\"></iframe>\n  </div>\n</div>\n\n\n",
          "url": "/blog/text-and-typography/#code-blocks",
          
          "relUrl": "/blog/text-and-typography/#code-blocks"
        },
        "57": {
          "doc": "Text and Typography",
          "title": "Mathematics",
          "content": "\n\n<p>The mathematics powered by <a href=\"https://www.mathjax.org/\"><strong>MathJax</strong></a>:</p>\n\n\\[\\begin{equation}\n  \\sum_{n=1}^\\infty 1/n^2 = \\frac{\\pi^2}{6}\n  \\label{eq:series}\n\\end{equation}\\]\n\n<p>We can reference the equation as \\eqref{eq:series}.</p>\n\n<p>When $a \\ne 0$, there are two solutions to $ax^2 + bx + c = 0$ and they are</p>\n\n\\[x = {-b \\pm \\sqrt{b^2-4ac} \\over 2a}\\]\n\n",
          "url": "/blog/text-and-typography/#mathematics",
          
          "relUrl": "/blog/text-and-typography/#mathematics"
        },
        "58": {
          "doc": "Text and Typography",
          "title": "Mermaid SVG",
          "content": "\n\n<pre><code class=\"language-mermaid\"> gantt\n  title  Adding GANTT diagram functionality to mermaid\n  apple :a, 2017-07-20, 1w\n  banana :crit, b, 2017-07-23, 1d\n  cherry :active, c, after b a, 1d\n</code></pre>\n\n",
          "url": "/blog/text-and-typography/#mermaid-svg",
          
          "relUrl": "/blog/text-and-typography/#mermaid-svg"
        },
        "59": {
          "doc": "Text and Typography",
          "title": "Images",
          "content": "\n\n<h3 id=\"default-with-caption\">Default (with caption)</h3>\n\n<p><img src=\"https://chirpy-img.netlify.app/posts/20190808/mockup.png\" alt=\"Desktop View\" width=\"972\" height=\"589\" /><br />\n<em>Full screen width and center alignment</em></p>\n\n<h3 id=\"left-aligned\">Left aligned</h3>\n\n<p><img src=\"https://chirpy-img.netlify.app/posts/20190808/mockup.png\" alt=\"Desktop View\" width=\"972\" height=\"589\" class=\"w-75 normal\" /></p>\n\n<h3 id=\"float-to-left\">Float to left</h3>\n\n<p><img src=\"https://chirpy-img.netlify.app/posts/20190808/mockup.png\" alt=\"Desktop View\" width=\"972\" height=\"589\" class=\"w-50 left\" /><br />\nPraesent maximus aliquam sapien. Sed vel neque in dolor pulvinar auctor. Maecenas pharetra, sem sit amet interdum posuere, tellus lacus eleifend magna, ac lobortis felis ipsum id sapien. Proin ornare rutrum metus, ac convallis diam volutpat sit amet. Phasellus volutpat, elit sit amet tincidunt mollis, felis mi scelerisque mauris, ut facilisis leo magna accumsan sapien. In rutrum vehicula nisl eget tempor. Nullam maximus ullamcorper libero non maximus. Integer ultricies velit id convallis varius. Praesent eu nisl eu urna finibus ultrices id nec ex. Mauris ac mattis quam. Fusce aliquam est nec sapien bibendum, vitae malesuada ligula condimentum.</p>\n\n<h3 id=\"float-to-right\">Float to right</h3>\n\n<p><img src=\"https://chirpy-img.netlify.app/posts/20190808/mockup.png\" alt=\"Desktop View\" width=\"972\" height=\"589\" class=\"w-50 right\" /><br />\nPraesent maximus aliquam sapien. Sed vel neque in dolor pulvinar auctor. Maecenas pharetra, sem sit amet interdum posuere, tellus lacus eleifend magna, ac lobortis felis ipsum id sapien. Proin ornare rutrum metus, ac convallis diam volutpat sit amet. Phasellus volutpat, elit sit amet tincidunt mollis, felis mi scelerisque mauris, ut facilisis leo magna accumsan sapien. In rutrum vehicula nisl eget tempor. Nullam maximus ullamcorper libero non maximus. Integer ultricies velit id convallis varius. Praesent eu nisl eu urna finibus ultrices id nec ex. Mauris ac mattis quam. Fusce aliquam est nec sapien bibendum, vitae malesuada ligula condimentum.</p>\n\n<h3 id=\"darklight-mode--shadow\">Dark/Light mode &amp; Shadow</h3>\n\n<p>The image below will toggle dark/light mode based on theme preference, notice it has shadows.</p>\n\n<p><img src=\"https://chirpy-img.netlify.app/posts/20190808/devtools-light.png\" alt=\"light mode only\" class=\"light w-75 shadow rounded-10\" w=\"1212\" h=\"668\" /><br />\n<img src=\"https://chirpy-img.netlify.app/posts/20190808/devtools-dark.png\" alt=\"dark mode only\" class=\"dark w-75 shadow rounded-10\" w=\"1212\" h=\"668\" /></p>\n\n",
          "url": "/blog/text-and-typography/#images",
          
          "relUrl": "/blog/text-and-typography/#images"
        },
        "60": {
          "doc": "Text and Typography",
          "title": "Video",
          "content": "\n\n<iframe class=\"embed-video\" loading=\"lazy\" src=\"https://www.youtube.com/embed/Balreaj8Yqs\" title=\"YouTube video player\" frameborder=\"0\" allow=\"accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture\" allowfullscreen=\"\"></iframe>\n\n",
          "url": "/blog/text-and-typography/#video",
          
          "relUrl": "/blog/text-and-typography/#video"
        },
        "61": {
          "doc": "Text and Typography",
          "title": "Reverse Footnote",
          "content": "\n\n<div class=\"footnotes\" role=\"doc-endnotes\">\n  <ol>\n    <li id=\"fn:footnote\" role=\"doc-endnote\">\n      <p>The footnote source¬†<a href=\"#fnref:footnote\" class=\"reversefootnote\" role=\"doc-backlink\">&#8617;&#xfe0e;</a></p>\n    </li>\n    <li id=\"fn:fn-nth-2\" role=\"doc-endnote\">\n      <p>The 2nd footnote source¬†<a href=\"#fnref:fn-nth-2\" class=\"reversefootnote\" role=\"doc-backlink\">&#8617;&#xfe0e;</a></p>\n    </li>\n  </ol>\n</div>\n",
          "url": "/blog/text-and-typography/#reverse-footnote",
          
          "relUrl": "/blog/text-and-typography/#reverse-footnote"
        },
        "62": {
          "doc": "Text and Typography",
          "title": "Text and Typography",
          "content": "",
          "url": "/blog/text-and-typography/",
          
          "relUrl": "/blog/text-and-typography/"
        }
      ,
        "63": {
          "doc": "Blog",
          "title": "Blog",
          "content": null,
          "url": "/blog/",
          
          "relUrl": "/blog/"
        }
      ,
        "64": {
          "doc": "typography",
          "title": "typography",
          "content": null,
          "url": "/blog/tags/typography/",
          
          "relUrl": "/blog/tags/typography/"
        }
      ,
        "65": {
          "doc": "writing",
          "title": "writing",
          "content": null,
          "url": "/blog/tags/writing/",
          
          "relUrl": "/blog/tags/writing/"
        }
      ,
        "66": {
          "doc": "getting started",
          "title": "getting started",
          "content": null,
          "url": "/blog/tags/getting-started/",
          
          "relUrl": "/blog/tags/getting-started/"
        }
      ,
        "67": {
          "doc": "theme",
          "title": "theme",
          "content": null,
          "url": "/blog/tags/theme/",
          
          "relUrl": "/blog/tags/theme/"
        }
      ,
        "68": {
          "doc": "homepage",
          "title": "homepage",
          "content": null,
          "url": "/blog/tags/homepage/",
          
          "relUrl": "/blog/tags/homepage/"
        }
      ,
        "69": {
          "doc": "Blogging",
          "title": "Blogging",
          "content": null,
          "url": "/blog/categories/blogging/",
          
          "relUrl": "/blog/categories/blogging/"
        }
      ,
        "70": {
          "doc": "Demo",
          "title": "Demo",
          "content": null,
          "url": "/blog/categories/demo/",
          
          "relUrl": "/blog/categories/demo/"
        }
      ,
        "71": {
          "doc": "Tutorial",
          "title": "Tutorial",
          "content": null,
          "url": "/blog/categories/tutorial/",
          
          "relUrl": "/blog/categories/tutorial/"
        }
}
