<!doctype html><html lang="en" ><head><link rel="canonical" href="https://jinchaoli.com/publications/"><meta charset="utf-8"><meta http-equiv="Content-Type" content="text/html; charset=UTF-8"><meta name="theme-color" media="(prefers-color-scheme: light)" content="#f7f7f7"><meta name="theme-color" media="(prefers-color-scheme: dark)" content="#1b1b1e"><meta name="apple-mobile-web-app-capable" content="yes"><meta name="apple-mobile-web-app-status-bar-style" content="black-translucent"><meta http-equiv="X-UA-Compatible" content="IE=edge"><meta name="viewport" content="width=device-width, user-scalable=no initial-scale=1, shrink-to-fit=no, viewport-fit=cover" ><link rel="shortcut icon" href="data:image/svg+xml,<svg xmlns=%22http://www.w3.org/2000/svg%22 viewBox=%220 0 100 100%22><text y=%22.9em%22 font-size=%2290%22>ğŸ“</text></svg>" ><title>Publications</title><meta name="author" content="Jinchao Li"><meta name="description" content="Jinchao Li's Homepage" ><meta name="keywords" content="jinchao li, æé”¦è¶…, cuhk, é¦™æ¸¯ä¸­æ–‡å¤§å­¦, research, speech and natural language processing"><meta property="og:site_name" content="Publications"><meta property="og:type" content="website"><meta property="og:title" content="Publications | Publications"><meta property="og:url" content="https://jinchaoli.com/publications/" ><meta property="og:description" content="Jinchao Li's Homepage" ><meta property="og:locale" content="en"><meta name="twitter:card" content="summary"><meta name="twitter:title" content="Publications"><meta name="twitter:description" content="Jinchao Li's Homepage" ><meta name="twitter:site" content="@JinchaoLove"><meta name="twitter:creator" content="@JinchaoLove"> <script type="application/ld+json"> { "author": { "@type": "Person", "name": "Jinchao Li" }, "url": "https://jinchaoli.com/publications/", "@type": "WebSite", "description": "Jinchao Li's Homepage", "headline": "Publications", "sameAs": ["https://github.com/JinchaoLove", "https://twitter.com/JinchaoLove", "https://scholar.google.com/citations?hl=en&user=0LWE_dkAAAAJ", "https://www.researchgate.net/profile/Jinchao-Li-3", "https://zhihu.com/people/jinchaoli"], "name": "Jinchao Li", "@context": "https://schema.org" } </script><link href="/assets/lib/fonts/main.css" rel="stylesheet"><link rel="stylesheet" href="/assets/lib/bootstrap/bootstrap.min.css"><link rel="stylesheet" href="/assets/lib/fontawesome-free/css/all.min.css"><link rel="stylesheet" href="/assets/css/theme.css"><link rel="stylesheet" href="/assets/lib/tocbot/tocbot.css"><link defer rel="stylesheet" href="/assets/lib/loading-attribute-polyfill/loading-attribute-polyfill.min.css"><link defer rel="stylesheet" href="/assets/lib/magnific-popup/magnific-popup.css"> <script type="text/javascript" defer> class ModeToggle { static get MODE_KEY() { return 'mode'; } static get MODE_ATTR() { return 'data-mode'; } static get DARK_MODE() { return 'dark'; } static get LIGHT_MODE() { return 'light'; } static get ID() { return 'mode-toggle'; } constructor() { if (this.hasMode) { if (this.isDarkMode) { if (!this.isSysDarkPrefer) { this.setDark(); } } else { if (this.isSysDarkPrefer) { this.setLight(); } } } let self = this; /* always follow the system prefers */ this.sysDarkPrefers.addEventListener('change', () => { if (self.hasMode) { if (self.isDarkMode) { if (!self.isSysDarkPrefer) { self.setDark(); } } else { if (self.isSysDarkPrefer) { self.setLight(); } } self.clearMode(); } self.notify(); }); } /* constructor() */ get sysDarkPrefers() { return window.matchMedia('(prefers-color-scheme: dark)'); } get isSysDarkPrefer() { return this.sysDarkPrefers.matches; } get isDarkMode() { return this.mode === ModeToggle.DARK_MODE; } get isLightMode() { return this.mode === ModeToggle.LIGHT_MODE; } get hasMode() { return this.mode != null; } get mode() { return sessionStorage.getItem(ModeToggle.MODE_KEY); } /* get the current mode on screen */ get modeStatus() { if (this.isDarkMode || (!this.hasMode && this.isSysDarkPrefer)) { return ModeToggle.DARK_MODE; } else { return ModeToggle.LIGHT_MODE; } } setDark() { document.documentElement.setAttribute(ModeToggle.MODE_ATTR, ModeToggle.DARK_MODE); sessionStorage.setItem(ModeToggle.MODE_KEY, ModeToggle.DARK_MODE); } setLight() { document.documentElement.setAttribute(ModeToggle.MODE_ATTR, ModeToggle.LIGHT_MODE); sessionStorage.setItem(ModeToggle.MODE_KEY, ModeToggle.LIGHT_MODE); } clearMode() { document.documentElement.removeAttribute(ModeToggle.MODE_ATTR); sessionStorage.removeItem(ModeToggle.MODE_KEY); } /* Notify another plugins that the theme mode has changed */ notify() { window.postMessage( { direction: ModeToggle.ID, message: this.modeStatus }, '*' ); } flipMode() { if (this.hasMode) { if (this.isSysDarkPrefer) { if (this.isLightMode) { this.clearMode(); } else { this.setLight(); } } else { if (this.isDarkMode) { this.clearMode(); } else { this.setDark(); } } } else { if (this.isSysDarkPrefer) { this.setLight(); } else { this.setDark(); } } this.notify(); } /* flipMode() */ } /* ModeToggle */ const modeToggle = new ModeToggle(); </script><body><aside aria-label="Sidebar" id="sidebar" class="d-flex flex-column align-items-end"><header class="profile-wrapper"> <a href="/" id="avatar" class="rounded-circle"><img src="/assets/img/avatar/Jinchao.png" width="112" height="112" alt="avatar" onerror="this.style.display='none'"></a><h1 class="site-title"> <a href="/">Jinchao Li</a></h1><p class="site-subtitle fst-italic mb-0">Jinchao Li's Homepage</p><div class="social"> <a href="javascript:location.href = 'mailto:' + ['jinchaolovefy','gmail.com'].join('@')" data-bs-toggle="tooltip" data-bs-placement="top" aria-label="email" data-bs-original-title="email" link-attr-ignore style="color: #0072c5;" > <i class="fas fa-envelope"></i> </a> <a href="https://scholar.google.com/citations?hl=en&user=SB7xjMoAAAAJ" data-bs-toggle="tooltip" data-bs-placement="top" aria-label="google-scholar" data-bs-original-title="google-scholar" link-attr-ignore target="_blank" rel="noopener noreferrer" style="color: #5c92f6;" > <i class="fa-brands fa-google-scholar"></i> </a> <a href="https://github.com/JinchaoLove" data-bs-toggle="tooltip" data-bs-placement="top" aria-label="github" data-bs-original-title="github" link-attr-ignore target="_blank" rel="noopener noreferrer" > <i class="fab fa-github"></i> </a> <a href="/feed.xml" data-bs-toggle="tooltip" data-bs-placement="top" aria-label="rss" data-bs-original-title="rss" link-attr-ignore style="color: #ee802f;" > <i class="fas fa-rss"></i> </a></div></header><nav class="flex-column flex-grow-1 w-100 ps-0"><ul class="nav"><li class="nav-item"> <a href="/" class="nav-link"> <i class="fa-fw fas fa-home"></i> <span>HOME</span> </a><li class="nav-item active"> <a href="/publications/" class="nav-link"> <i class="fa-fw fas fa-book"></i> <span>PUBLICATIONS</span> </a></ul><div class="blog-wrapper d-flex flex-wrap align-items-center w-100"><div class="nav-buttons"> <button id="nav-blog" class="nav-icon" title="" data-bs-toggle="tooltip" data-bs-placement="bottom" aria-label="Blog" data-bs-original-title="Blog" > <i class="fa-fw fas fa-edit"></i> </button> <button id="nav-categories" class="nav-icon" title="" data-bs-toggle="tooltip" data-bs-placement="bottom" aria-label="Categories" data-bs-original-title="Categories" > <i class="fa-fw fas fa-layer-group"></i> </button> <button id="nav-tags" class="nav-icon" title="" data-bs-toggle="tooltip" data-bs-placement="bottom" aria-label="Tags" data-bs-original-title="Tags" > <i class="fa-fw fas fa-tags"></i> </button> <button id="nav-archives" class="nav-icon" title="" data-bs-toggle="tooltip" data-bs-placement="bottom" aria-label="Archives" data-bs-original-title="Archives" > <i class="fa-fw fas fa-archive"></i> </button></div><div class="nav-contents"><div class="nav-blog"> <a href="/blog/" class="nav-title"> <i class="fa-fw fas fa-edit"></i> <span>BLOG</span> </a></div><div class="nav-categories"> <a href="/blog/categories/" class="nav-title"> <i class="fa-fw fas fa-layer-group"></i> <span>CATEGORIES</span> </a> <a href="/blog/categories/blogging/" class="tag">Blogging</a> <a href="/blog/categories/demo/" class="tag">Demo</a> <a href="/blog/categories/tutorial/" class="tag">Tutorial</a></div><div class="nav-tags"> <a href="/blog/tags/" class="nav-title"> <i class="fa-fw fas fa-tags"></i> <span>TAGS</span> </a><div class="tag"> <a class="tag-name" href="/blog/tags/getting-started/">getting started<span class="tag-size text-muted">1</span> </a></div><div class="tag"> <a class="tag-name" href="/blog/tags/homepage/">homepage<span class="tag-size text-muted">1</span> </a></div><div class="tag"> <a class="tag-name" href="/blog/tags/theme/">theme<span class="tag-size text-muted">1</span> </a></div><div class="tag"> <a class="tag-name" href="/blog/tags/typography/">typography<span class="tag-size text-muted">1</span> </a></div><div class="tag"> <a class="tag-name" href="/blog/tags/writing/">writing<span class="tag-size text-muted">1</span> </a></div></div><div class="nav-archives"> <a href="/blog/archives/" class="nav-title"> <i class="fa-fw fas fa-archive"></i> <span>ARCHIVES</span> </a><ul><li> <a href="/blog/exchange-homepage-and-about/">Exchange Homepage and About</a><li> <a href="/blog/customize-jekyll-chirpy/">Customize Your Jekyll Chirpy Theme</a><li> <a href="/blog/getting-started/">Getting Started</a><li> <a href="/blog/write-a-new-post/">Writing a New Post</a><li> <a href="/blog/text-and-typography/">Text and Typography</a></ul></div></div></div></nav></aside><header id="topbar-wrapper" aria-label="Top Bar"><div id="topbar" class="d-flex align-items-center justify-content-between px-lg-3 h-100" ><div class="d-flex align-items-center"> <button type="button" id="sidebar-trigger" class="btn btn-link" data-bs-toggle="tooltip" data-bs-placement="right" aria-label="Hide/show sidebar" data-bs-original-title="Hide/show sidebar" > <i class="fas fa-bars fa-fw"></i> </button><nav id="breadcrumb" aria-label="Breadcrumb"> <span> <a href="/">Home</a> </span> <span>Publications</span></nav><div id="topbar-title"> Publications</div></div><div class="d-flex align-items-center"> <button type="button" id="search-trigger" class="btn btn-link" data-bs-toggle="tooltip" data-bs-placement="left" aria-label="Search (âŒ˜/âŒƒ + k)" data-bs-original-title="Search (âŒ˜/âŒƒ + k)" > <i class="fas fa-search fa-fw"></i> </button><div id="search-box"> <search class="align-items-center"> <i class="fas fa-search fa-fw"></i> <input class="form-control" id="search-input" type="search" aria-label="search" placeholder="Search (âŒ˜/âŒƒ + k)" tabindex="0" autocomplete="true" list="suggestions" > <datalist id="suggestions"></datalist> </search></div><button type="button" id="search-cancel" class="btn btn-link text-decoration-none" data-bs-toggle="tooltip" data-bs-placement="bottom" aria-label="Cancel (â‹esc)" data-bs-original-title="Cancel (â‹esc)" > <i class="fa fa-times" aria-hidden="true"></i> </button> <span class="icon-border"></span> <button type="button" class="mode-toggle btn light" data-bs-toggle="tooltip" data-bs-placement="top" aria-label="Light/dark theme" data-bs-original-title="Light/dark theme" > <i class="fas fa-star-and-crescent"></i> </button> <button type="button" class="mode-toggle btn dark" data-bs-toggle="tooltip" data-bs-placement="top" aria-label="Light/dark theme" data-bs-original-title="Light/dark theme" > <i class="fas fa-sun"></i> </button></div></div></header><div id="main-wrapper" class="d-flex justify-content-center"><div class="container d-flex flex-column px-xxl-3"><div class="row flex-grow-1"><main aria-label="Main Content" class="col-12 px-xl-1" ><article class="px-1"><h1 class="dynamic-title"> Publications</h1><div class="content"><p>ğŸ¤—Thanks to all the collaborators for their great work! Check out my <a href="https://scholar.google.com/citations?hl=en&amp;user=SB7xjMoAAAAJ">Google Scholar</a> for more information.</p><div class="publications"><h2 class="bibliography">2023</h2><ol class="bibliography"><li><div class="row"><div class="col-sm-2"><div class="preview d-flex align-items-center"><figure><picture> <source class="responsive-img-srcset" srcset=" /assets/papers/23/emotion2vec-480.webp 480w, /assets/papers/23/emotion2vec-800.webp 800w, /assets/papers/23/emotion2vec-1280.webp 1280w, " sizes="200px" type="image/webp" loading="lazy" data-proofer-ignore="" /> <a href="/assets/papers/23/emotion2vec.png" class="popup img-link preview rounded shimmer"><img src="/assets/papers/23/emotion2vec.png" width="100%" height="auto" alt="papers/23/emotion2vec.png" data-zoomable="" onerror="this.onerror=null; $('.responsive-img-srcset').remove();" loading="lazy" loading="lazy"></a> </picture></figure></div></div><div id="ma2023emotion2vec" class="col-sm-10"><div class="title">emotion2vec: Self-Supervised Pre-Training for Speech Emotion Representation</div><div class="author"> Ziyang Ma,&nbsp;Zhisheng Zheng,&nbsp;Jiaxin Ye,&nbsp;<span class="highlight-author">Jinchao Li</span>,&nbsp;Zhifu Gao,&nbsp;Shiliang Zhang,&nbsp;and&nbsp;Xie Chen</div><div class="periodical"> <em>arXiv:2312.15185</em>, 2023</div><div class="periodical"></div><div class="links"> <button class="abstract btn btn-sm z-depth-0" title="Click to show/hide abstract">ABS</button> <button class="bibtex btn btn-sm z-depth-0" title="Click to show/hide bibtex">BIB</button> <a href="https://arxiv.org/pdf/2312.15185.pdf" class="btn btn-sm z-depth-0" role="button">PDF</a> <a href="https://www.modelscope.cn/models/iic/emotion2vec_base_finetuned/summary" class="btn btn-sm z-depth-0" role="button">DEMO</a> <a href="https://github.com/ddlBoJack/emotion2vec" class="btn btn-sm z-depth-0" role="button">CODE</a></div><div class="abstract hidden"><p>We propose emotion2vec, a universal speech emotion representation model. emotion2vec is pre-trained on open-source unlabeled emotion data through self-supervised online distillation, combining utterance-level loss and frame-level loss during pre-training. emotion2vec outperforms state-of-the-art pre-trained universal models and emotion specialist models by only training linear layers for the speech emotion recognition task on the mainstream IEMOCAP dataset. In addition, emotion2vec shows consistent improvements among 10 different languages of speech emotion recognition datasets. emotion2vec also shows excellent results on other emotion tasks, such as song emotion recognition, emotion prediction in conversation, and sentiment analysis. Comparison experiments, ablation experiments, and visualization comprehensively demonstrate the universal capability of the proposed emotion2vec. To the best of our knowledge, emotion2vec is the first universal representation model in various emotion-related tasks, filling a gap in the field.</p></div><div class="bibtex hidden"><figure class="highlight"><pre><code class="language-bibtex" data-lang="bibtex"><span class="nc">@article</span><span class="p">{</span><span class="nl">ma2023emotion2vec</span><span class="p">,</span>
  <span class="na">url</span> <span class="p">=</span> <span class="s">{https://arxiv.org/abs/2312.15185}</span><span class="p">,</span>
  <span class="na">title</span> <span class="p">=</span> <span class="s">{emotion2vec: Self-Supervised Pre-Training for Speech Emotion Representation}</span><span class="p">,</span>
  <span class="na">author</span> <span class="p">=</span> <span class="s">{Ma, Ziyang and Zheng, Zhisheng and Ye, Jiaxin and Li, Jinchao and Gao, Zhifu and Zhang, Shiliang and Chen, Xie}</span><span class="p">,</span>
  <span class="na">journal</span> <span class="p">=</span> <span class="s">{arXiv:2312.15185}</span><span class="p">,</span>
  <span class="na">year</span> <span class="p">=</span> <span class="s">{2023}</span>
<span class="p">}</span></code></pre></figure></div></div></div><li><div class="row"><div class="col-sm-2"><div class="preview d-flex align-items-center"><figure><picture> <source class="responsive-img-srcset" srcset=" /assets/papers/23/MultiAD_ICASSP23-480.webp 480w, /assets/papers/23/MultiAD_ICASSP23-800.webp 800w, /assets/papers/23/MultiAD_ICASSP23-1280.webp 1280w, " sizes="200px" type="image/webp" loading="lazy" data-proofer-ignore="" /> <a href="/assets/papers/23/MultiAD_ICASSP23.png" class="popup img-link preview rounded shimmer"><img src="/assets/papers/23/MultiAD_ICASSP23.png" width="100%" height="auto" alt="papers/23/MultiAD_ICASSP23.png" data-zoomable="" onerror="this.onerror=null; $('.responsive-img-srcset').remove();" loading="lazy" loading="lazy"></a> </picture></figure></div></div><div id="li2023leveraging" class="col-sm-10"><div class="title">Leveraging Pretrained Representations With Task-Related Keywords for Alzheimerâ€™s Disease Detection</div><div class="author"> <span class="highlight-author">Jinchao Li</span>,&nbsp;Kaitao Song,&nbsp;Junan Li,&nbsp;Bo Zheng,&nbsp;Dongsheng Li,&nbsp;Xixin Wu,&nbsp;Xunying Liu, and Helen Meng</div><div class="periodical"> <em>In ICASSP</em>, 2023</div><div class="periodical"></div><div class="links"> <button class="abstract btn btn-sm z-depth-0" title="Click to show/hide abstract">ABS</button> <button class="bibtex btn btn-sm z-depth-0" title="Click to show/hide bibtex">BIB</button> <a href="https://arxiv.org/pdf/2303.08019.pdf" class="btn btn-sm z-depth-0" role="button">PDF</a> <a href="/assets/papers/23/AD_poster.pdf" class="btn btn-sm z-depth-0" role="button" target="_blank" rel="noopener noreferrer">POSTER</a></div><div class="abstract hidden"><p>With the global population aging rapidly, Alzheimerâ€™s disease (AD) is particularly prominent in older adults, which has an insidious onset and leads to a gradual, irreversible deterioration in cognitive domains (memory, communication, etc.). Speech-based AD detection opens up the possibility of widespread screening and timely disease intervention. Recent advances in pre-trained models motivate AD detection modeling to shift from low-level features to high-level representations. This paper presents several efficient methods to extract better AD-related cues from high-level acoustic and linguistic features. Based on these features, the paper also proposes a novel task-oriented approach by modeling the relationship between the participantsâ€™ description and the cognitive task. Experiments are carried out on the ADReSS dataset in a binary classification setup, and models are evaluated on the unseen test set. Results and comparison with recent literature demonstrate the efficiency and superior performance of proposed acoustic, linguistic and task-oriented methods. The findings also show the importance of semantic and syntactic information, and feasibility of automation and generalization with the promising audio-only and task-oriented methods for the AD detection task.</p></div><div class="bibtex hidden"><figure class="highlight"><pre><code class="language-bibtex" data-lang="bibtex"><span class="nc">@inproceedings</span><span class="p">{</span><span class="nl">li2023leveraging</span><span class="p">,</span>
  <span class="na">url</span> <span class="p">=</span> <span class="s">{https://ieeexplore.ieee.org/document/10096205}</span><span class="p">,</span>
  <span class="na">title</span> <span class="p">=</span> <span class="s">{Leveraging Pretrained Representations With Task-Related Keywords for Alzheimerâ€™s Disease Detection}</span><span class="p">,</span>
  <span class="na">author</span> <span class="p">=</span> <span class="s">{Li, Jinchao and Song, Kaitao and Li, Junan and Zheng, Bo and Li, Dongsheng and Wu, Xixin and Liu, Xunying and Meng, Helen}</span><span class="p">,</span>
  <span class="na">booktitle</span> <span class="p">=</span> <span class="s">{ICASSP}</span><span class="p">,</span>
  <span class="na">year</span> <span class="p">=</span> <span class="s">{2023}</span><span class="p">,</span>
  <span class="na">organization</span> <span class="p">=</span> <span class="s">{IEEE}</span>
<span class="p">}</span></code></pre></figure></div></div></div><li><div class="row"><div class="col-sm-2"><div class="preview d-flex align-items-center"><figure><picture> <source class="responsive-img-srcset" srcset=" /assets/papers/23/AVB_ICASSP23-480.webp 480w, /assets/papers/23/AVB_ICASSP23-800.webp 800w, /assets/papers/23/AVB_ICASSP23-1280.webp 1280w, " sizes="200px" type="image/webp" loading="lazy" data-proofer-ignore="" /> <a href="/assets/papers/23/AVB_ICASSP23.png" class="popup img-link preview rounded shimmer"><img src="/assets/papers/23/AVB_ICASSP23.png" width="100%" height="auto" alt="papers/23/AVB_ICASSP23.png" data-zoomable="" onerror="this.onerror=null; $('.responsive-img-srcset').remove();" loading="lazy" loading="lazy"></a> </picture></figure></div></div><div id="li2023hierarchical" class="col-sm-10"><div class="title">A Hierarchical Regression Chain Framework for Affective Vocal Burst Recognition</div><div class="author"> <span class="highlight-author">Jinchao Li</span>,&nbsp;Xixin Wu,&nbsp;Kaitao Song,&nbsp;Dongsheng Li,&nbsp;Xunying Liu,&nbsp;and&nbsp;Helen Meng</div><div class="periodical"> <em>In ICASSP</em>, 2023</div><div class="periodical"></div><div class="links"> <button class="abstract btn btn-sm z-depth-0" title="Click to show/hide abstract">ABS</button> <button class="bibtex btn btn-sm z-depth-0" title="Click to show/hide bibtex">BIB</button> <a href="https://arxiv.org/pdf/2303.08027.pdf" class="btn btn-sm z-depth-0" role="button">PDF</a> <a href="/assets/papers/23/AVB_poster.pdf" class="btn btn-sm z-depth-0" role="button" target="_blank" rel="noopener noreferrer">POSTER</a> <a href="https://github.com/JinchaoLove/AffectiveVocalBurstRecognition" class="btn btn-sm z-depth-0" role="button">CODE</a></div><div class="abstract hidden"><p>As a common way of emotion signaling via non-linguistic vocalizations, vocal burst (VB) plays an important role in daily social interaction. Understanding and modeling human vocal bursts are indispensable for developing robust and general artificial intelligence. Exploring computational approaches for understanding vocal bursts is attracting increasing research attention. In this work, we propose a hierarchical framework, based on chain regression models, for affective recognition from VBs, that explicitly considers multiple relationships: (i) between emotional states and diverse cultures; (ii) between low-dimensional (arousal &amp; valence) and high-dimensional (10 emotion classes) emotion spaces; and (iii) between various emotion classes within the high-dimensional space. To address the challenge of data sparsity, we also use self-supervised learning (SSL) representations with layer-wise and temporal aggregation modules. The proposed systems participated in the ACII Affective Vocal Burst (A-VB) Challenge 2022 and ranked first in the "TWOâ€ and "CULTUREâ€ tasks. Experimental results based on the ACII Challenge 2022 dataset demonstrate the superior performance of the proposed system and the effectiveness of considering multiple relationships using hierarchical regression chain models.</p></div><div class="bibtex hidden"><figure class="highlight"><pre><code class="language-bibtex" data-lang="bibtex"><span class="nc">@inproceedings</span><span class="p">{</span><span class="nl">li2023hierarchical</span><span class="p">,</span>
  <span class="na">url</span> <span class="p">=</span> <span class="s">{https://ieeexplore.ieee.org/document/10096395/}</span><span class="p">,</span>
  <span class="na">title</span> <span class="p">=</span> <span class="s">{A Hierarchical Regression Chain Framework for Affective Vocal Burst Recognition}</span><span class="p">,</span>
  <span class="na">author</span> <span class="p">=</span> <span class="s">{Li, Jinchao and Wu, Xixin and Song, Kaitao and Li, Dongsheng and Liu, Xunying and Meng, Helen}</span><span class="p">,</span>
  <span class="na">booktitle</span> <span class="p">=</span> <span class="s">{ICASSP}</span><span class="p">,</span>
  <span class="na">year</span> <span class="p">=</span> <span class="s">{2023}</span><span class="p">,</span>
  <span class="na">organization</span> <span class="p">=</span> <span class="s">{IEEE}</span>
<span class="p">}</span></code></pre></figure></div></div></div></ol><h2 class="bibliography">2022</h2><ol class="bibliography"><li><div class="row"><div class="col-sm-2"><div class="preview d-flex align-items-center"><figure><picture> <source class="responsive-img-srcset" srcset=" /assets/papers/22/MER-480.webp 480w, /assets/papers/22/MER-800.webp 800w, /assets/papers/22/MER-1280.webp 1280w, " sizes="200px" type="image/webp" loading="lazy" data-proofer-ignore="" /> <a href="/assets/papers/22/MER.png" class="popup img-link preview rounded shimmer"><img src="/assets/papers/22/MER.png" width="100%" height="auto" alt="papers/22/MER.png" data-zoomable="" onerror="this.onerror=null; $('.responsive-img-srcset').remove();" loading="lazy" loading="lazy"></a> </picture></figure></div></div><div id="li2022context" class="col-sm-10"><div class="title">Context-Aware Multimodal Fusion for Emotion Recognition</div><div class="author"> <span class="highlight-author">Jinchao Li</span>,&nbsp;Shuai Wang,&nbsp;Yang Chao,&nbsp;Xunying Liu,&nbsp;and&nbsp;Helen Meng</div><div class="periodical"> <em>In INTERSPEECH</em>, 2022</div><div class="periodical"></div><div class="links"> <button class="abstract btn btn-sm z-depth-0" title="Click to show/hide abstract">ABS</button> <button class="bibtex btn btn-sm z-depth-0" title="Click to show/hide bibtex">BIB</button> <a href="/assets/papers/22/MER_paper_IS22.pdf" class="btn btn-sm z-depth-0" role="button" target="_blank" rel="noopener noreferrer">PDF</a> <a href="/assets/papers/22/MER_poster_IS22.png" class="btn btn-sm z-depth-0" role="button" target="_blank" rel="noopener noreferrer">POSTER</a></div><div class="abstract hidden"><p>Automatic emotion recognition (AER) is an inherently complex multimodal task that aims to automatically determine the emotional state of a given expression. Recent works have witnessed the benefits of upstream pretrained models in both audio and textual modalities for the AER task. However, efforts are still needed to effectively integrate features across multiple modalities, devoting due considerations to granularity mismatch and asynchrony in time steps. In this work, we first validate the effectiveness of the upstream models in a unimodal setup and empirically find that partial fine-tuning of the pretrained model in the feature space can significantly boost performance. Moreover, we take the context of the current sentence to model a more accurate emotional state. Based on the unimodal setups, we further propose several multimodal fusion methods to combine high-level features from the audio and text modalities. Experiments are carried out on the IEMOCAP dataset in a 4-category classification problem and compared with state-of-the-art methods in recent literature. Results show that the proposed models gave a superior performance of up to 84.45% and 80.36% weighted accuracy scores respectively in Session 5 and 5-fold cross-validation settings.</p></div><div class="bibtex hidden"><figure class="highlight"><pre><code class="language-bibtex" data-lang="bibtex"><span class="nc">@inproceedings</span><span class="p">{</span><span class="nl">li2022context</span><span class="p">,</span>
  <span class="na">url</span> <span class="p">=</span> <span class="s">{https://www.isca-speech.org/archive/interspeech_2022/li22v_interspeech.html}</span><span class="p">,</span>
  <span class="na">title</span> <span class="p">=</span> <span class="s">{Context-Aware Multimodal Fusion for Emotion Recognition}</span><span class="p">,</span>
  <span class="na">author</span> <span class="p">=</span> <span class="s">{Li, Jinchao and Wang, Shuai and Chao, Yang and Liu, Xunying and Meng, Helen}</span><span class="p">,</span>
  <span class="na">booktitle</span> <span class="p">=</span> <span class="s">{INTERSPEECH}</span><span class="p">,</span>
  <span class="na">year</span> <span class="p">=</span> <span class="s">{2022}</span><span class="p">,</span>
  <span class="na">organization</span> <span class="p">=</span> <span class="s">{IEEE}</span>
<span class="p">}</span></code></pre></figure></div></div></div></ol><h2 class="bibliography">2021</h2><ol class="bibliography"><li><div class="row"><div class="col-sm-2"><div class="preview d-flex align-items-center"><figure><picture> <source class="responsive-img-srcset" srcset=" /assets/papers/21/Comp_AD-480.webp 480w, /assets/papers/21/Comp_AD-800.webp 800w, /assets/papers/21/Comp_AD-1280.webp 1280w, " sizes="200px" type="image/webp" loading="lazy" data-proofer-ignore="" /> <a href="/assets/papers/21/Comp_AD.png" class="popup img-link preview rounded shimmer"><img src="/assets/papers/21/Comp_AD.png" width="100%" height="auto" alt="papers/21/Comp_AD.png" data-zoomable="" onerror="this.onerror=null; $('.responsive-img-srcset').remove();" loading="lazy" loading="lazy"></a> </picture></figure></div></div><div id="li2021comparative" class="col-sm-10"><div class="title">A Comparative Study of Acoustic and Linguistic Features Classification for Alzheimerâ€™s Disease Detection</div><div class="author"> <span class="highlight-author">Jinchao Li</span>,&nbsp;Jianwei Yu,&nbsp;Zi Ye,&nbsp;Simon Wong,&nbsp;Manwai Mak,&nbsp;Brian Mak,&nbsp;Xunying Liu, and Helen Meng</div><div class="periodical"> <em>In ICASSP</em>, 2021</div><div class="periodical"></div><div class="links"> <button class="abstract btn btn-sm z-depth-0" title="Click to show/hide abstract">ABS</button> <button class="bibtex btn btn-sm z-depth-0" title="Click to show/hide bibtex">BIB</button> <a href="https://www1.se.cuhk.edu.hk/~hccl/publications/pub/ICASSP_jcli.pdf" class="btn btn-sm z-depth-0" role="button">PDF</a> <a href="/assets/papers/21/Comp_AD.pdf" class="btn btn-sm z-depth-0" role="button" target="_blank" rel="noopener noreferrer">POSTER</a> <a href="https://github.com/JinchaoLove/NCDdetection_ICASSP2021" class="btn btn-sm z-depth-0" role="button">CODE</a></div><div class="abstract hidden"><p>With the global population ageing rapidly, Alzheimerâ€™s Disease (AD) is particularly prominent in older adults, which has an insidious onset followed by gradual, irreversible deterioration in cognitive domains (memory, communication, etc). Thus the detection of Alzheimerâ€™s Disease is crucial for timely intervention to slow down disease progression. This paper presents a comparative study of different acoustic and linguistic features for the AD detection using various classifiers. Experimental results on ADReSS dataset reflect that the proposed models using ComParE, X-vector, Linguistics, TFIDF and BERT features are able to detect AD with high accuracy and sensitivity, and are comparable with the state-of-the-art results reported. While most previous work used manual transcripts, our results also indicate that similar or even better performance could be obtained using automatically recognized transcripts over manually collected ones. This work achieves accuracy scores at 0.67 for acoustic features and 0.88 for linguistic features on either manual or ASR transcripts on the ADReSS Challenge test set.</p></div><div class="bibtex hidden"><figure class="highlight"><pre><code class="language-bibtex" data-lang="bibtex"><span class="nc">@inproceedings</span><span class="p">{</span><span class="nl">li2021comparative</span><span class="p">,</span>
  <span class="na">url</span> <span class="p">=</span> <span class="s">{https://ieeexplore.ieee.org/document/9414147}</span><span class="p">,</span>
  <span class="na">title</span> <span class="p">=</span> <span class="s">{A Comparative Study of Acoustic and Linguistic Features Classification for Alzheimer's Disease Detection}</span><span class="p">,</span>
  <span class="na">author</span> <span class="p">=</span> <span class="s">{Li, Jinchao and Yu, Jianwei and Ye, Zi and Wong, Simon and Mak, Manwai and Mak, Brian and Liu, Xunying and Meng, Helen}</span><span class="p">,</span>
  <span class="na">booktitle</span> <span class="p">=</span> <span class="s">{ICASSP}</span><span class="p">,</span>
  <span class="na">year</span> <span class="p">=</span> <span class="s">{2021}</span><span class="p">,</span>
  <span class="na">organization</span> <span class="p">=</span> <span class="s">{IEEE}</span>
<span class="p">}</span></code></pre></figure></div></div></div><li><div class="row"><div class="col-sm-2"><div class="preview d-flex align-items-center"><figure><picture> <source class="responsive-img-srcset" srcset=" /assets/papers/21/ASR_AD-480.webp 480w, /assets/papers/21/ASR_AD-800.webp 800w, /assets/papers/21/ASR_AD-1280.webp 1280w, " sizes="200px" type="image/webp" loading="lazy" data-proofer-ignore="" /> <a href="/assets/papers/21/ASR_AD.png" class="popup img-link preview rounded shimmer"><img src="/assets/papers/21/ASR_AD.png" width="100%" height="auto" alt="papers/21/ASR_AD.png" data-zoomable="" onerror="this.onerror=null; $('.responsive-img-srcset').remove();" loading="lazy" loading="lazy"></a> </picture></figure></div></div><div id="ye2021development" class="col-sm-10"><div class="title">Development of the CUHK Elderly Speech Recognition System for Neurocognitive Disorder Detection Using the DementiaBank Corpus</div><div class="author"> Zi Ye,&nbsp;Shoukang Hu,&nbsp;<span class="highlight-author">Jinchao Li</span>,&nbsp;Xurong Xie,&nbsp;Mengzhe Geng,&nbsp;Jianwei Yu,&nbsp;Junhao Xu, and <span class="more-authors" title="click to view 4 more authors" onclick=" var element = $(this); element.attr('title', ''); var more_authors_text = element.text() == '4 more authors' ? 'Boyang Xue, Shansong Liu, Xunying Liu, Helen Meng' : '4 more authors'; var cursorPosition = 0; var textAdder = setInterval(function(){ element.text(more_authors_text.substring(0, cursorPosition + 1)); if (++cursorPosition == more_authors_text.length){ clearInterval(textAdder); } }, '10'); ">4 more authors</span></div><div class="periodical"> <em>In ICASSP</em>, 2021</div><div class="periodical"></div><div class="links"> <button class="abstract btn btn-sm z-depth-0" title="Click to show/hide abstract">ABS</button> <button class="bibtex btn btn-sm z-depth-0" title="Click to show/hide bibtex">BIB</button> <a href="https://www1.se.cuhk.edu.hk/~hccl/publications/pub/zye_revised_v4.pdf" class="btn btn-sm z-depth-0" role="button">PDF</a> <a href="/assets/papers/21/ASR_AD.pdf" class="btn btn-sm z-depth-0" role="button" target="_blank" rel="noopener noreferrer">POSTER</a></div><div class="abstract hidden"><p>Early diagnosis of Neurocognitive Disorder (NCD) is crucial in facilitating preventive care and timely treatment to delay further progression. This paper presents the development of a state-of-the-art automatic speech recognition (ASR) system built on the Dementia-Bank Pitt corpus for automatic NCD detection. Speed perturbation based audio data augmentation expanded the limited elderly speech data by four times. Large quantities of out-of-domain, non-aged adult speech were exploited by cross-domain adapting a 1000-hour LibriSpeech corpus trained LF-MMI factored TDNN system to DementiaBank. The variability among elderly speakers was modeled using i-Vector and learning hidden unit contributions (LHUC) based speaker adaptive training. Robust Bayesian estimation of TDNN systems and LHUC transforms were used in both cross-domain and speaker adaptation. A Transformer language model was also built to improve the final system performance. A word error rate (WER) reduction of 11.72% absolute (26.11% relative) was obtained over the baseline i-Vector adapted LF-MMI TDNN system on the evaluation data of 48 elderly speakers. The best NCD detection accuracy of 88%, comparable to that using the ground truth speech transcripts, was obtained using the textual features extracted from the final ASR system outputs.</p></div><div class="bibtex hidden"><figure class="highlight"><pre><code class="language-bibtex" data-lang="bibtex"><span class="nc">@inproceedings</span><span class="p">{</span><span class="nl">ye2021development</span><span class="p">,</span>
  <span class="na">url</span> <span class="p">=</span> <span class="s">{https://ieeexplore.ieee.org/document/9413634}</span><span class="p">,</span>
  <span class="na">title</span> <span class="p">=</span> <span class="s">{Development of the CUHK Elderly Speech Recognition System for Neurocognitive Disorder Detection Using the DementiaBank Corpus}</span><span class="p">,</span>
  <span class="na">author</span> <span class="p">=</span> <span class="s">{Ye, Zi and Hu, Shoukang and Li, Jinchao and Xie, Xurong and Geng, Mengzhe and Yu, Jianwei and Xu, Junhao and Xue, Boyang and Liu, Shansong and Liu, Xunying and Meng, Helen}</span><span class="p">,</span>
  <span class="na">booktitle</span> <span class="p">=</span> <span class="s">{ICASSP}</span><span class="p">,</span>
  <span class="na">year</span> <span class="p">=</span> <span class="s">{2021}</span><span class="p">,</span>
  <span class="na">organization</span> <span class="p">=</span> <span class="s">{IEEE}</span>
<span class="p">}</span></code></pre></figure></div></div></div></ol><h2 class="bibliography">2019</h2><ol class="bibliography"><li><div class="row"><div class="col-sm-2"><div class="preview d-flex align-items-center"><figure><picture> <source class="responsive-img-srcset" srcset=" /assets/papers/19/SC-480.webp 480w, /assets/papers/19/SC-800.webp 800w, /assets/papers/19/SC-1280.webp 1280w, " sizes="200px" type="image/webp" loading="lazy" data-proofer-ignore="" /> <a href="/assets/papers/19/SC.png" class="popup img-link preview rounded shimmer"><img src="/assets/papers/19/SC.png" width="100%" height="auto" alt="papers/19/SC.png" data-zoomable="" onerror="this.onerror=null; $('.responsive-img-srcset').remove();" loading="lazy" loading="lazy"></a> </picture></figure></div></div><div id="changbao2019method" class="col-sm-10"><div class="title">Method, Device and Electronic Equipment for Determining Sound Source Information based on Microphone Array</div><div class="author"> Changbao Zhu,&nbsp;and&nbsp;<span class="highlight-author">Jinchao Li</span></div><div class="periodical"> <em>In Patent: CN110148422B</em>, 2019</div><div class="periodical"></div><div class="links"> <button class="abstract btn btn-sm z-depth-0" title="Click to show/hide abstract">ABS</button> <a href="https://patent-image.qichacha.com/pdf/81a740a39373966f3730316bd32e08d5.pdf" class="btn btn-sm z-depth-0" role="button">PDF</a></div><div class="abstract hidden"><p>æœ¬å…¬å¼€å®æ–½ä¾‹å…¬å¼€äº†ä¸€ç§åŸºäºä¼ å£°å™¨é˜µåˆ—ç¡®å®šå£°æºä¿¡æ¯çš„æ–¹æ³•,å…¶ä¸­,æ–¹æ³•åŒ…æ‹¬:ç¡®å®šä¼ å£°å™¨é˜µåˆ—é‡‡é›†çš„å¤šè·¯éŸ³é¢‘ä¿¡å·;ç¡®å®šæ‰€è¿°å¤šè·¯éŸ³é¢‘ä¿¡å·çš„ç›¸ä¼¼æ€§åº¦é‡ä¿¡æ¯;ç¡®å®šæ‰€è¿°å¤šè·¯éŸ³é¢‘ä¿¡å·çš„ç›¸å…³æ€§åº¦é‡ä¿¡æ¯;åŸºäºæ‰€è¿°ç›¸ä¼¼æ€§åº¦é‡ä¿¡æ¯å’Œæ‰€è¿°ç›¸å…³æ€§åº¦é‡ä¿¡æ¯,ç¡®å®šå£°æºä¿¡æ¯.è¿˜å…¬å¼€äº†ä¸€ç§åŸºäºä¼ å£°å™¨é˜µåˆ—ç¡®å®šå£°æºä¿¡æ¯çš„è£…ç½®,å…¶ä¸­,è£…ç½®åŒ…æ‹¬:éŸ³é¢‘ä¿¡å·ç¡®å®šæ¨¡å—,ç›¸ä¼¼æ€§åº¦é‡ä¿¡æ¯ç¡®å®šæ¨¡å—,ç›¸å…³æ€§åº¦é‡ä¿¡æ¯ç¡®å®šæ¨¡å—å’Œå£°æºä¿¡æ¯ç¡®å®šæ¨¡å—.æœ¬å…¬å¼€å®æ–½ä¾‹å¯ä»¥é€šè¿‡ç¡®å®šå¹¶åŸºäºä¼ å£°å™¨é˜µåˆ—é‡‡é›†çš„å¤šè·¯éŸ³é¢‘ä¿¡å·çš„ç›¸ä¼¼æ€§åº¦é‡ä¿¡æ¯å’Œç›¸å…³æ€§åº¦é‡ä¿¡æ¯,ç¡®å®šå£°æºä¿¡æ¯,å¯ä»¥é€šè¿‡ä¼ å£°å™¨é˜µåˆ—å¾—åˆ°å£°æºä¿¡æ¯,æ–¹ä¾¿åç»­è¿›è¡Œè¡Œå£°æºæ•°é‡ä¼°è®¡,å¯ä»¥ä¿è¯å£°æºæ•°é‡ä¼°è®¡çš„å‡†ç¡®æ€§.</p></div></div></div></ol></div></div></article></main></div><div class="row"><div id="tail-wrapper" class="col-12 px-xl-1" ><footer aria-label="Site Info" class=" d-flex flex-column justify-content-center text-muted flex-lg-row justify-content-lg-between align-items-lg-center pb-lg-3 " ><p>Â© <time>2024</time> <a href="https://github.com/JinchaoLove">Jinchao Li</a>. <span data-bs-toggle="tooltip" data-bs-placement="top" title="Except where otherwise noted, the blog posts on this site are licensed under the Creative Commons Attribution 4.0 International (CC BY 4.0) License by the author." >Some rights reserved.</span></p><p>Using the <a href="https://jekyllrb.com">Jekyll</a> theme <a href="https://github.com/JinchaoLove/AcademicChirpy">AcademicChirpy</a>.</p></footer></div></div><div id="search-result-wrapper" class="d-flex justify-content-center unloaded"><div class="col-11 content"><div id="search-results" class="d-flex flex-wrap justify-content-center text-muted mt-3"></div></div></div></div><aside aria-label="Scroll to Top"> <button id="back-to-top" type="button" class="btn btn-lg btn-box-shadow"> <i class="fas fa-angle-up"></i> </button></aside></div><div id="mask"></div><aside id="notification" class="toast" role="alert" aria-live="assertive" aria-atomic="true" data-bs-animation="true" data-bs-autohide="false" ><div class="toast-header"> <button type="button" class="btn-close ms-auto" data-bs-dismiss="toast" aria-label="Close" ></button></div><div class="toast-body text-center pt-0"><p class="px-2 mb-3">A new version of content is available.</p><button type="button" class="btn btn-primary" aria-label="Update"> Update </button></div></aside><script src="/assets/lib/jquery/jquery.min.js"></script> <script src="/assets/lib/bootstrap/bootstrap.bundle.min.js"></script> <script src="/assets/lib/lunr/lunr.min.js"></script> <script src="/assets/lib/loading-attribute-polyfill/loading-attribute-polyfill.umd.min.js"></script> <script src="/assets/lib/magnific-popup/jquery.magnific-popup.min.js"></script> <script src="/assets/lib/clipboard/clipboard.min.js"></script> <script src="/assets/lib/dayjs/dayjs.min.js"></script> <script src="/assets/lib/dayjs/locale/en.min.js"></script> <script src="/assets/lib/dayjs/plugin/relativeTime.min.js"></script> <script src="/assets/lib/dayjs/plugin/localizedFormat.min.js"></script> <script src="/assets/lib/tocbot/tocbot.min.js"></script> <script defer src="/assets/js/dist/page.min.js"></script> <script async src="/assets/js/just-the-docs.js"></script> <script defer src="/assets/js/dist/app.min.js"></script> <script async src="https://d1bxh8uas1mnw7.cloudfront.net/assets/embed.js"></script> <script async src="https://badge.dimensions.ai/badge.js"></script> <script type="text/javascript"> $(document).ready(function () { $('.mode-toggle').click(function () { let theme = localStorage.getItem('theme'); if (modeToggle.isDarkMode || (!modeToggle.hasMode && modeToggle.isSysDarkPrefer)) { theme = 'dark'; } else { theme = 'light'; } console.log('theme:' + theme); // Set jupyter notebooks themes. let jupyterNotebooks = document.getElementsByClassName('jupyter-notebook-iframe-container'); for (let i = 0; i < jupyterNotebooks.length; i++) { let iframeDocument = jupyterNotebooks[i].getElementsByTagName('iframe')[0].contentWindow.document; let bodyElement = iframeDocument.body; let headElement = iframeDocument.head; // Remove existing theme link elements let existingLinks = headElement.querySelectorAll('link[data-jp-theme-link]'); for (let link of existingLinks) { headElement.removeChild(link); } // Get original style-sheets let styleElements = headElement.querySelectorAll('style'); let variablesElement; for (let style of styleElements) { if (style.innerHTML.includes('The following CSS variables define the main, public API for styling JupyterLab.')) { variablesElement = style; break; } } // console.log(styleElements); // New style-sheets const cssLink = document.createElement('link'); cssLink.setAttribute('data-jp-theme-link', ''); cssLink.rel = 'stylesheet'; cssLink.type = 'text/css'; if (theme == 'dark') { cssLink.href = '/assets/css/jupyter-dark.css'; bodyElement.setAttribute('data-jp-theme-light', 'false'); bodyElement.setAttribute('data-jp-theme-name', 'JupyterLab Dark'); } else { cssLink.href = '/assets/css/jupyter-light.css'; bodyElement.setAttribute('data-jp-theme-light', 'true'); bodyElement.setAttribute('data-jp-theme-name', 'JupyterLab Light'); } // Import the cssLink if (variablesElement) { // variablesElement.insertAdjacentElement('beforebegin', cssLink); variablesElement.insertAdjacentElement('afterend', cssLink); } else { headElement.appendChild(cssLink); } } }); }); </script>
